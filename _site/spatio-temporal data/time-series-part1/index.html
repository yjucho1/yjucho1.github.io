<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.13.0 by Michael Rose
  Copyright 2013-2018 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE.txt
-->
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>시계열 분석 - part1 - yjucho’s blog</title>
<meta name="description" content="시계열 데이터는 일정 시간동안 수집된 일련의 데이터로, 시간에 따라서 샘플링되었기 때문에 인접한 시간에 수집된 데이터는 서로 상관관계가 존재하게 됩니다. 따라서 시계열 데이터를 추론하는데, 전통적인 통계적 추론이 적합하지 않을수 있습니다. 현실 세계에서는 날씨, 주가, 마케팅 데이터 등등 다양한 시계열 데이터가 존재하고, 시계열 분석을 통해 이러한 데이터가 생성되는 메카니즘을 이해하여 설명가능한 모델로서 데이터를 표현하거나, 미래 값을 예측하는 것, 의미있는 신호값만만 분리하는 일에 활용할 수 있습니다.">



<meta property="og:type" content="article">
<meta property="og:locale" content="en_EN">
<meta property="og:site_name" content="yjucho's blog">
<meta property="og:title" content="시계열 분석 - part1">
<meta property="og:url" content="http://localhost:4000/spatio-temporal%20data/time-series-part1/">


  <meta property="og:description" content="시계열 데이터는 일정 시간동안 수집된 일련의 데이터로, 시간에 따라서 샘플링되었기 때문에 인접한 시간에 수집된 데이터는 서로 상관관계가 존재하게 됩니다. 따라서 시계열 데이터를 추론하는데, 전통적인 통계적 추론이 적합하지 않을수 있습니다. 현실 세계에서는 날씨, 주가, 마케팅 데이터 등등 다양한 시계열 데이터가 존재하고, 시계열 분석을 통해 이러한 데이터가 생성되는 메카니즘을 이해하여 설명가능한 모델로서 데이터를 표현하거나, 미래 값을 예측하는 것, 의미있는 신호값만만 분리하는 일에 활용할 수 있습니다.">







  <meta property="article:published_time" content="2018-12-18T00:00:00+09:00">





  

  


<link rel="canonical" href="http://localhost:4000/spatio-temporal%20data/time-series-part1/">







  <script type="application/ld+json">
    {
      "@context": "http://schema.org",
      "@type": "Person",
      "name": "yjucho",
      "url": "http://localhost:4000",
      "sameAs": null
    }
  </script>







<!-- end _includes/seo.html -->


<link href="/feed.xml" type="application/atom+xml" rel="alternate" title="yjucho's blog Feed">

<!-- http://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">

<!--[if lte IE 9]>
  <style>
    /* old IE unsupported flexbox fixes */
    .greedy-nav .site-title {
      padding-right: 3em;
    }
    .greedy-nav button {
      position: absolute;
      top: 0;
      right: 0;
      height: 100%;
    }
  </style>
<![endif]-->



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single">

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    <div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        <a class="site-title" href="/">yjucho's blog</a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/about/" >About</a>
            </li><li class="masthead__menu-item">
              <a href="/category/" >Category</a>
            </li></ul>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle Menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      



<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="http://schema.org/Person">

  
    <div class="author__avatar">
      

      
        <img src="/assets/img/bio-photo.jpg" alt="yjucho" itemprop="image">
      
    </div>
  

  <div class="author__content">
    
      <h3 class="author__name" itemprop="name">yjucho</h3>
    
    
      <p class="author__bio" itemprop="description">
        Data science, Deep Learning, AI
      </p>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      
        <li itemprop="homeLocation" itemscope itemtype="http://schema.org/Place">
          <i class="fas fa-fw fa-map-marker-alt" aria-hidden="true"></i> <span itemprop="name">Seoul, Korea</span>
        </li>
      

      
        
          
            <li><a href="mailto:jyj0729@gmail.com"><i class="fas fa-fw fa-envelope-square" aria-hidden="true"></i> Email</a></li>
          
        
          
        
          
        
          
            <li><a href="https://www.facebook.com/yjucho"><i class="fab fa-fw fa-facebook-square" aria-hidden="true"></i> Facebook</a></li>
          
        
          
            <li><a href="https://github.com/yjucho1"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
          
        
          
        
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>

  
  </div>


  <article class="page" itemscope itemtype="http://schema.org/CreativeWork">
    <meta itemprop="headline" content="시계열 분석 - part1">
    <meta itemprop="description" content="시계열 데이터는 일정 시간동안 수집된 일련의 데이터로, 시간에 따라서 샘플링되었기 때문에 인접한 시간에 수집된 데이터는 서로 상관관계가 존재하게 됩니다. 따라서 시계열 데이터를 추론하는데, 전통적인 통계적 추론이 적합하지 않을수 있습니다. 현실 세계에서는 날씨, 주가, 마케팅 데이터 등등 다양한 시계열 데이터가 존재하고, 시계열 분석을 통해 이러한 데이터가 생성되는 메카니즘을 이해하여 설명가능한 모델로서 데이터를 표현하거나, 미래 값을 예측하는 것, 의미있는 신호값만만 분리하는 일에 활용할 수 있습니다.">
    <meta itemprop="datePublished" content="December 18, 2018">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">시계열 분석 - part1
</h1>
          
        </header>
      

      <section class="page__content" itemprop="text">
        
        <p>시계열 데이터는 일정 시간동안 수집된 일련의 데이터로, 시간에 따라서 샘플링되었기 때문에 인접한 시간에 수집된 데이터는 서로 상관관계가 존재하게 됩니다. 따라서 시계열 데이터를 추론하는데, 전통적인 통계적 추론이 적합하지 않을수 있습니다. 현실 세계에서는 날씨, 주가, 마케팅 데이터 등등 다양한 시계열 데이터가 존재하고, 시계열 분석을 통해 이러한 데이터가 생성되는 메카니즘을 이해하여 설명가능한 모델로서 데이터를 표현하거나, 미래 값을 예측하는 것, 의미있는 신호값만만 분리하는 일에 활용할 수 있습니다.</p>

<p>일반적인 시계열분석의 과정은 아래와 같습니다.</p>

<ul>
  <li>Plot the series and examine the main features of the graph, checking in particular whether there is
    <ul>
      <li>A trend</li>
      <li>A seasonal component</li>
      <li>Any apparent sharp changes in behavior</li>
      <li>Any outlying observations</li>
    </ul>
  </li>
  <li>Remove the trend and seasonal components to get stationary residuals
    <ul>
      <li>To achieve goal, you may need to apply a preliminary transformation like taking logarithms</li>
      <li>Estimate the trend and seasonal components using classical decomposition model</li>
      <li>Or Eliminate the trend and seasonal components using differencing</li>
      <li>Anyway, the goal is to get stationary residuals</li>
    </ul>
  </li>
  <li>Choose a model to fit the residuals</li>
  <li>Forecasting the residuals and then inverting the transformations for original series</li>
  <li>Alternative approach is to express the series in terms of its Fourier components. But it will not be discussed in here.</li>
</ul>

<p>시계열 분석은 시간의 경과에 따라 변하지 않는 어떤 특성을 가진 프로세스를 분석하는 것입니다. 만약 시계열 데이터를 예측하고자 한다면, 우리는 시간에 따라 변하지 않는 무언가가 있다는 것을 반드시 가정해야합니다. 예를 들면 평균이 변하지 않는것(no trend), 분산이 변하지 않고, 주기적인 패턴이 없는 데이터를 가정합니다. 이러한 속성을 “stationary”하다고 합니다. 어떤 시계열 데이터가 stationary하다는 가정을 하면, 우리는 다양한 기법들을 활용할수 있습니다. 앞으로의 포스팅에서는 이러한 기법들이 무엇인지 소개하려고 합니다.</p>

<h3 id="stationary-auto-covariance-function-and-auto-correlation-function">Stationary, Auto-Covariance Function and Auto-Correlation Function</h3>

<p>시계열 <script type="math/tex">\{X_t, t=0, \pm 1, ...\}</script> 가 stationary하다는 것은 h lag만큼 time-shifted 된 시계열 <script type="math/tex">\{X_{t+h}, t=0, \pm 1, ...\}</script>와 통계적인 속성이 유사하다는 것을 의미합니다. 이 때 통계적인 속성을 first and second order moments(mean and covariance)로만 제한하면, 아래와 같이 수식적 정의를 할 수 있습니다.</p>

<p><b>Definition<b></b></b></p>

<p>Let <script type="math/tex">{X_t}</script> be a time series with <script type="math/tex">% <![CDATA[
E(X_t^2) < \infty %]]></script>.</p>

<p>The mean function of <script type="math/tex">{X_t}</script> is <script type="math/tex">\mu_X(t) = E(X_t)</script>.</p>

<p>The covariance function of <script type="math/tex">{X_t}</script> is <script type="math/tex">\rho_X(r, s) = Cov(X_r, X_s) = E[(X_r - \mu_X(r))(X_s - \mu_X(s))]</script> for all integers r and s.</p>

<p><b>Definition</b></p>

<p><script type="math/tex">{X_t}</script> is (weakly) stationary if</p>

<p>1) <script type="math/tex">\mu_X(t)</script> is independent of t.</p>

<p>2) <script type="math/tex">\gamma_X(t+h, t)</script> is independent of t for each h.</p>

<p>Strictly stationary is also</p>

<p>3) <script type="math/tex">(X_1, ..., Xn)</script> and <script type="math/tex">(X_{1+h}, ..., X_{n+h})</script> have the same joint distributions for all h and n &gt;0</p>

<p>2)에서의 정의를 이용해 stationary한 타임시리즈의 <script type="math/tex">\gamma_X(t+h, t)</script>는 t에 대해서 무관하기 때문에 <script type="math/tex">\gamma(\cdot)</script>는 “autocovariance function”으로 부르며, <script type="math/tex">\gamma_X(h)</script>는 h lag에서의 값을 지칭하는 것으로 하겠습니다. 또한 covariance를 normalization하여 correlation을 함께 정의할수 있습니다.</p>

<p><b>Definition</b></p>

<p>Let <script type="math/tex">{X_t}</script> be a stationary time series.</p>

<p>The autocovariance function (ACVF) of <script type="math/tex">{X_t}</script> at lag h is <script type="math/tex">\gamma_X(h) = Cov(X_{t+h}, X_t)</script>.</p>

<p>The autocorrelation function (ACF) of <script type="math/tex">{X_t}</script> at lag h is <script type="math/tex">\rho_X(h) \equiv \frac{\gamma_X(h)}{\gamma_X(0)} = Cor(X_{t+h}, X_t)</script></p>

<p>stationary time series의 대표적인 예는 iid noise와 white noise가 있습니다. iid noise는 <script type="math/tex">\{X_t\}</script>가 동일하고(identically) 서로 독립적인(independent) 분포를 따르며, 평균이 0인 확률변수로 정의됩니다. t에 상관없이 평균이 0이고, <script type="math/tex">\gamma_X(\cdot)</script>도 0이기때문에 stationary 조건을 만족합니다.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
\gamma_X(t+h, t) & = \sigma^2, \ & if \ h=0 \\ 
                 & = 0, \ & if \ h \ne 0
\end{align} %]]></script>

<p>마찬가지로 white noise with zero mean and variance <script type="math/tex">\sigma^2</script>도 (weak) staionary 조건을 만족합니다. 참고로 iid noise와 white noise는 다릅니다. 모든 iid noise는 white noise이지만, 그 역은 성립하지 않습니다. <a href="https://www.researchgate.net/post/What_is_the_difference_between_white_noise_and_iid_noise">참고</a></p>

<p>시계열 데이터(realization)가 주어졌을때, 이 프로세스의 mean과 covariance를 추정하기 위해 sample mean과 sample covariance function, sample autocorrelation function를 사용합니다.</p>

<p><b>Definition</b></p>

<p>Let <script type="math/tex">x_1, ..., x_n</script> be observations of a time series.</p>

<p>The sample mean of <script type="math/tex">x_1, ..., x_n</script> is
<script type="math/tex">\bar{x} = \frac{1}{n} \sum_{t=1}^n x_t</script></p>

<p>The sample autocovariance function is
<script type="math/tex">% <![CDATA[
\hat{\gamma} := n^{-1} \sum_{t=1}^{n-|h|}(x_{t+|h|} - \bar{x})(x_{t} - \bar{x}), -n < h < n %]]></script></p>

<p>The sample autocorrelation function is
<script type="math/tex">% <![CDATA[
\hat{\rho} := \frac{\hat{\gamma}(h)}{\hat{\gamma}(0)}, -n < h < n %]]></script></p>

<h3 id="moving-average-and-auto-regressive-process">Moving Average and Auto Regressive process</h3>

<p>시계열 분석의 문제는 stocastic process의 realization인 시계열 데이터 <script type="math/tex">\{X_t\}</script>가 주어졌을때(그리고 <script type="math/tex">\{X_t\}</script>가 stationary할때), 우리는 이 데이터가 생성된 본래의 stocastic process를 모델링하고 싶다는 겁니다. 데이터가 주어져있기때문에 우리는 (sample) mean과 lag에 따른 covariance과 correlation은 쉽게 구할수 있는 상황입니다.(auto covariance function과 auto correlation function)</p>

<p>만약 ACF가 주어졌을때 이에 대응되는 stationary stochatic process가 unique하게 결정된다면 아주 쉬운 문제가 됩니다. analytic한 솔루션이 1개 존재하는 것이고, 그건 공식에 맞춰 계산하면 되는 문제가 되니까요. 그러한 특징과 관련된 2가지 모델을 살펴보도록 하겠습니다.</p>

<p>The MA(q) process:<br />
<script type="math/tex">\{X_t\}</script> is a moving-average process of order q if
<script type="math/tex">X_t = Z_t + \theta_1 Z_{t-1} + ... + \theta_q Z_{t-q}</script> <br />
where <script type="math/tex">\{Z_t\} \sim WN(0, \sigma^2)</script> and <script type="math/tex">\theta_1, ..., \theta_q</script> are constants.</p>

<p><script type="math/tex">\{X_t\}</script>가 t 이전 시점의 white noise <script type="math/tex">\{Z_x\}</script>(s <script type="math/tex">\le</script> t)로 표현되는 프로세스를 Moving Average process라고 합니다.  앞서 본 “stationary” 정의에 따라, MA(q) process는 항상 (weakly) stationary합니다.</p>

<p>invertibility :</p>

<p>흥미롭게도 MA(1)은 AR(<script type="math/tex">\infty</script>)로 변환될수 있습니다. 아래 수식에서 볼수 있듯이 <script type="math/tex">Z_t</script>는 <script type="math/tex">X_t</script>의 과거값들의 linear combination로 표현될수 있습니다.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
X_t & = Z_t + \theta Z_{t-1} \\
Z_t & = X_t - \theta Z_{t-1} \\
    & = X_t - \theta(X_{t-1} - \theta Z_{t-2}) \\
    & = X_t - \theta X_{t-1} + \theta^2 (X_{t-2} - \theta Z_{t-3}) \\
    & = X_t - \theta X_{t-1} + \theta^2 X_{t-2} - \theta^3 X_{t-3} + ... + (-\theta)^n Z_{t-n}, &
when \left\vert \theta \right\vert \lt 1, (-\theta)^n Z_{t-n} \approx 0 \\
    & = \sum_{n=0}^{\infty} (-\theta)^nX_{t-n} \\
\end{align} %]]></script>

<p>이러한 성질을 일반화하여 MA(q)에 대해 이야기할 수 있습니다. white noise인 <script type="math/tex">Z_t</script>를 <script type="math/tex">X_t</script>의 무한등비급수의 형태로 표현할수 있다면, 주어진 <script type="math/tex">\{X_t\}</script>는 invertible하다고 정의합니다. 이 때 수렴 조건(MA(1)에서의 <script type="math/tex">\left\vert \theta \right\vert \lt 1</script>)을 invertibility condition이라고 하고 합니다.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
Z_t & = \theta(B)X_t, \ & where \ \theta(\cdot) \ are \ the \ q-th \ degree \ polynomials \\
\theta(z) & = 1 + \theta z + ... + \theta_q z^q
\end{align} %]]></script>

<p>자세한 증명은 여기서 다루지 않지만, 결론적으로는 <script type="math/tex">\theta(z)</script>의 해가 unit circle 밖에 있는 경우 invertible 조건을 만족하게 됩니다.</p>

<p>Invertibility is equivalent to the condition</p>

<script type="math/tex; mode=display">\theta(z) = 1+ \theta_1 z + ... + \theta_q z^q \ne 0 \ for \ all \left\vert z \right\vert \le 1</script>

<p>invertible이 중요한 이유는 ACF가 주어질 때, 이 ACF를 만족하는 MA process가 unique하게 결정되기 때문입니다.</p>

<p>두번재 모델은 Auto-Regressive Process입니다. <script type="math/tex">\{X_t\}</script>가 이전 시점의 자기 자신 값 <script type="math/tex">\{X_s\}</script>(s <script type="math/tex">\le</script> t)와 t 시점의 white noise <script type="math/tex">\{Z_x\}</script>로 표현되는 프로세스를 Auto-Regress process라고 합니다. 기억해야할 점은 MA process와 달리 AR process는 항상 stationary한 것은 아니라는 점입니다.</p>

<p>The AR(q) process:<br />
<script type="math/tex">\{X_t\}</script> is a auto-regressive process of order q if
<script type="math/tex">X_t = Z_t + \phi_1 X_{t-1} + ... + \phi_q X_{t-q}</script> <br />
where <script type="math/tex">\{Z_t\} \sim WN(0, \sigma^2)</script> and <script type="math/tex">\phi_1, ..., \phi_q</script> are constants</p>

<p>MA process의 invertibility 와 유사한 개념으로 AR process에 casuality 개념을 도입할수 있습니다. 아래는 AR(1) process를 MA(<script type="math/tex">\infty</script>)로 변환하는 예시입니다.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
X_t & = \phi X_{t-1} + Z_t \\
    & = \phi (\phi X_{t-2} + Z_{t-1}) + Z_t \\
    & = \phi^2 X_{t-2} + \phi Z_{t-1} + Z_t  \\
    & = \phi^2 (\phi X_{t-3} + Z_{t-2}) + \phi Z_{t-1} + Z_t \\
    & = Z_t + \phi Z_{t-1} + \phi^2 Z_{t-3} + ... + \phi^n X_{t-n}, &
when \left\vert \phi \right\vert \lt 1, \phi^n X_{t-n} \approx 0 \\
    & = \sum_{n=0}^{\infty} (\phi)^n Z_{t-n} \\
\end{align} %]]></script>

<p><script type="math/tex">X_t</script>를 white noise인 <script type="math/tex">Z_x</script>(단, s <script type="math/tex">\le</script> t)의 linear combination 형태로 표현된다면, 주어진 <script type="math/tex">\{X_t\}</script>는 causal하다고 정의합니다. 위의 예시인 AR(1) process <script type="math/tex">\{X_t\}</script>가 causal하기 위해서는 <script type="math/tex">\left\vert \phi \right\vert \lt 1</script> 입니다.</p>

<p>이러한 성질을 일반적인 AR(p) process에 대해서도 이야기할수 있습니다.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
X_t & = \phi(B)X_t, \ & where \ \phi(\cdot) \ are \ the \ p-th \ degree \ polynomials \\
\phi(z) & = 1+ \phi z + ... + \phi_p z^p
\end{align} %]]></script>

<p>자세한 증명은 여기서 다루지 않지만, 결론적으로는 <script type="math/tex">\phi(z)</script>의 해가 unit circle 밖에 있는 경우 causality 조건을 만족하게 됩니다.</p>

<p>Causality is equivalent to the condition</p>

<script type="math/tex; mode=display">\phi(z) = 1 + \phi_1 z + ... + \phi_p z^p \ne 0 \ for \ all \left\vert z \right\vert \le 1</script>

<p>중요한 것은 causality를 만족하는 AR(p) process는 항상 stationary하고, stationary AR(p) process는 항상 causality를 만족합니다.</p>

<h3 id="armap-q">ARMA(p, q)</h3>

<p>AR(p)와 MA(q)가 합쳐진 process를 ARMA(p, q)로 표기하고 ARMA(p,q) process가 유일한 stationary solution <script type="math/tex">\{X_t\}</script>를 갖는 조건은 causality condition을 만족할 때이며, 그 역도 성립합니다.</p>

<p><b>Definition</b></p>

<p><script type="math/tex">\{X_t\}</script> is an ARMA(p, q) process if <script type="math/tex">\{X_t\}</script> is stationary and if for every t,</p>

<script type="math/tex; mode=display">X_t - \phi_1 X_{t-1} - ... - \phi_p X_{t-p} = Z_t + \theta_1 Z_{t-1} + ... + \theta_q Z_{t-q}</script>

<p>where <script type="math/tex">\{Z_t\} \sim WN(0, \sigma^2)</script> and the polynomials <script type="math/tex">( 1 - \phi z - ... - \phi_p z^p)</script> and <script type="math/tex">(1+\theta_1 z + ... + \theta_q z^q )</script> have no common factors.</p>

<p>Existence and Uniqueness :</p>

<p>A stationary solution <script type="math/tex">\{X_t\}</script> of equation ARMA(p, q) exists (and is also the unique stationary solution) if and only if</p>

<script type="math/tex; mode=display">\phi(z) = 1 + \phi_1 z + ... + \phi_p z^p \ne 0 \ for \ all \left\vert z \right\vert \le 1</script>

<p><b>Example</b></p>

<p>아래와 같은 조건을 만족하는 ARMA(1, 1) process <script type="math/tex">\{X_t\}</script>를 생각해보도록 하겠습니다.</p>

<script type="math/tex; mode=display">X_t - 0.5 X_{t-1} = Z_t + 0.4 Z_{t-1}, \ \ \ {Z_t} \sim WN(0, \sigma^2)</script>

<p><script type="math/tex">\phi(z) = 1 - 0.5 z</script>는 z=2 일때 0이 되고, 이는 unit circle밖에 위치하기 때문에 이는 causality 조건을 만족합니다. 즉, 유니크한 솔루션이 존재합니다. causal하기때문에 <script type="math/tex">X_t = \sum_{j=0}^\infty \psi_j Z_{t-j}</script>로 표현되는 constant <script type="math/tex">\{\psi\}</script>가 존재합니다.</p>

<p><script type="math/tex">X_t = \sum_{j=0}^\infty \psi_j Z_{t-j}</script>를 <script type="math/tex">X_t - \phi X_{t-1} = Z_t + \theta Z_{t-1}</script>에 대입하면 아래와 같은 식이 성립해야합니다.</p>

<script type="math/tex; mode=display">(1-\phi z - ... - \phi_p z^p)(\psi_0 +\psi_1 z + ...) = 1 +\theta_1z + ... + \theta_qz^q</script>

<p>양변의 <script type="math/tex">z^j, j=0,1,...</script> 계수가 동일해야하므로,</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
1 & = \psi_0 \\
\theta_1 & = \psi_1 - \psi_0 \phi_1 \\
\theta_2 & = \psi_2 - \psi_1 \phi_1 - \psi_0 \phi_2 \\
\vdots
\end{align} %]]></script>

<script type="math/tex; mode=display">\psi_j - \sum_{k=1}^p \phi_k \psi_{j-k} = \theta_j, j=0,1, ...</script>

<p>예제의 계수를 대입하면,</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
\psi_0 & = 1\\
\psi_1 & = 0.4 + 1 * 0.5   \\
\psi_2 & = 0.5 (0.4 + 0.5) \\
\psi_j & = 0.5^{j-1} (0.4 + 0.5)
\end{align} %]]></script>

<h3 id="yule-walker-equation">Yule-Walker Equation</h3>

<p>cauality 조건을 만족하는 ARMA(p,q) process에 대해서 ACF를 이용해 모델 파라미터를 체계적으로 구할수 있는 방법을 알아보도록 하겠습니다.</p>

<p>설명의 편의성을 위해 AR(2)를 가정하도록 하겠습니다.</p>

<script type="math/tex; mode=display">X_t - \phi_1 X_{t-1} - \phi_2 X_{t-2} = Z_t</script>

<p>위 양변에 <script type="math/tex">X_{t-k}</script>를 곱하고 Expectation을 취해보도록 하겠습니다. 
<script type="math/tex">E[X_t X_{t-k}] - \phi_1 E[X_{t-1} X_{t-k}] - \phi_2 E[X_{t-2} X_{t-k}] = E[Z_t X{t-k}]</script></p>

<p>k를 0부터 1, 2, … 순차적으로 대입하면 아래와 같습니다.</p>

<p>when k = 0,   <script type="math/tex">\gamma(0) -\phi_1 \gamma(1) -\phi_2 \gamma(2) = \sigma^2</script> <br />
when k = 1,   <script type="math/tex">\gamma(1) -\phi_1 \gamma(0) -\phi_2 \gamma(1) = 0</script> <br />
when k = 2,   <script type="math/tex">\gamma(2) -\phi_1 \gamma(1) -\phi_2 \gamma(0) = 0</script> <br />
… …</p>

<p>즉,</p>

<script type="math/tex; mode=display">% <![CDATA[
\gamma(h) -\phi_1 \gamma(h-1) -\phi_2 \gamma(h-2)
= \begin{cases} 
\sigma^2, & \mbox{h=0} \\
0, & \mbox{h=1}
\end{cases} %]]></script>

<p>양변을 <script type="math/tex">\gamma(0)</script>로 나누어, Auto-correlation으로 나타내면</p>

<script type="math/tex; mode=display">% <![CDATA[
\rho(h) -\phi_1 \rho(h-1) -\phi_2 \rho(h-2) 
= \begin{cases} 
1 & \mbox{h=0} \\
0, & \mbox{h=1}
\end{cases} %]]></script>

<p>위와 같은 식을 Yule-Walker equation 이라고 합니다. 
Yule-walker equation을 이용해 AR(2)모델의 <script type="math/tex">\phi_1, \phi_2</script>를 구하는 방법은 h=1일때, h=2일때를 대입하여 연립방정식을 푸는 것과 같습니다.</p>

<script type="math/tex; mode=display">h = 1 , \rho(1) -\phi_1 \rho(0) -\phi_2 \rho(1) = 0 \\
h = 2 , \rho(2) -\phi_1 \rho(1) -\phi_2 \rho(0) = 0</script>

<p><script type="math/tex">\rho(0)=1</script>이고, <script type="math/tex">\rho(1)</script> 와 <script type="math/tex">\rho(2)</script>는 주어진 데이터를 이용해 sample ACF로 계산하고, 위의 식을 이용해 <script type="math/tex">\phi_1</script>과 <script type="math/tex">\phi_2</script>를 계산할수 있습니다.</p>

<p>지금까지 AR(2) 모델에 대해서 살펴본 과정을 AR(p) process에 대해서 일반화할 수 있습니다. AR(p) process에 대한 Yule-walker equation을 적으면 아래와 같습니다.</p>

<script type="math/tex; mode=display">% <![CDATA[
\gamma(h) - \phi_1 \gamma(h-1) - ... - \phi_p \gamma(h-p) = \begin{cases} 
\sigma^2 & \mbox{h=0} \\
0, & \mbox{h >= 1}
\end{cases} %]]></script>

<script type="math/tex; mode=display">% <![CDATA[
\rho(h) - \phi_1 \rho(h-1) - ... - \phi_p \rho(h-p) = \begin{cases} 
1 & \mbox{h=0} \\
0, & \mbox{h >= 1}
\end{cases} %]]></script>

<p><script type="math/tex">\rho(h) = \phi_1 \rho(h-1) + ... + \phi_p \rho(h-p), \ \ \ \ when \ h \ge 1</script> 이를 Matrix 형태로 적어보도록 하겠습니다. (메트릭스 형태는 다음에 설명한 Partial ACF와의 관계를 설명할 때 유용합니다)</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
\rho(0) & = 1 \\
\rho(1) & = \phi_1  + \phi_2 \rho(1) + \phi_3 \rho(2) + ... +\phi_p \rho(p-1) \\
\rho(2) & = \phi_1 \rho(1) + \phi_2 +  \phi_3 \rho(1) + ... +\phi_p \rho(p-2) \\
\rho(3) & = \phi_1 \rho(2) + \phi_2 \rho(1) +  \phi_3 + ... +\phi_p \rho(p-3) \\
... \\
\rho(p-1) & = \phi_1 \rho(p-2) + \phi_2 \rho(p-3) +  \phi_3 \rho(p-4) + ... +\phi_p \rho(1) \\
\rho(p) & = \phi_1 \rho(p-1) + \phi_2 \rho(p-2) +  \phi_3 \rho(p-3) + ... +\phi_p 
\end{align} %]]></script>

<script type="math/tex; mode=display">% <![CDATA[
\begin{bmatrix}
\rho(1)\\ \rho(2)\\ \rho(3)\\ \vdots \\ \rho(p-1)\\\rho(p)
\end{bmatrix} =
\begin{bmatrix}
1 & \rho(1) & \rho(2) & \cdots & \rho(p-1) \\ 
\rho(1) & 1 & \rho(1) & \cdots & \rho(p-2) \\ 
\rho(2) & \rho(1) & 1 & \cdots & \rho(p-3) \\ 
 &  & \vdots & & \\
\rho(p-2) & \rho(p-3) & \rho(p-4) & \cdots & \rho(1) \\ 
\rho(p-1) & \rho(p-2) & \rho(p-3) & \cdots & 1 \\ 
\end{bmatrix} 
\begin{bmatrix}
\phi(1)\\ \phi(2)\\ \phi(3)\\ \vdots \\ \phi(p-1)\\\phi(p)
\end{bmatrix} %]]></script>

<p>여기까지 우리는 시계열 데이터가 주어졌을때, ARMA(p,q)의 파라미터를 추정하는 방법을 살펴보았습니다. 하지만 파라미터를 추정하기 전에 p와 q(모델의 order)를 결정하는 것이 우선되어야합니다.</p>

<p>MA(q)에 대해서는 간단합니다. MA process의 정의에 따라, MA(q)는 현재 시점의 데이터 <script type="math/tex">X_t</script>가 이전 q개의 noise로만 표현되기 때문에 q 이전의 데이터들과는 무관합니다. MA(q)의 ACF는 아래와 같이 q개의 유의미한 spike를 갖고 이후 값들은 모두 0에 가깝습니다(negligible)</p>

<p><img src="/assets/img/2018-12-18/MA_acf.png" width="500" /></p>

<p>반면 AR(p)는 ACF만으로 p를 결정하기가 어렵습니다. AR process의 ACF는 lag가 증가할수록 decay한 모습을 보일뿐, p에 대한 힌트를 주지 못하기 때문입니다.  다음에서는 주어진 데이터로부터 어떻게 AR(p)모델의 order를 결정할 수 있는지 알아보도록 하겠습니다.</p>

<p><img src="/assets/img/2018-12-18/AR_acf.png" width="500" /></p>

<h3 id="particial-acf">Particial ACF</h3>

<p>범죄(crime) 발생 수 와 교회의 수(church)의 상관계수는 양의 상관관계를 갖고 있다고 합니다. 정말로 범죄가 많은 지역에 교회의 수가 많고, 또는 교회의 수가 많은 지역에 범죄가 발생할 가능성도 높은 걸까요? 아닙니다. 이는 두 요인과 관련있는 다른 요인, 인구(population)에 대한 요인을 고려하지 못했기때문에 발생하는 잘못된 결과 해석입니다. 이처럼 두 변수 사이에 수치적 관계가 있는지 또는 어느 정도로 관련이 있는지를 찾을 때, 두 변수와 관련된 다른 변수가있을 경우 해당 상관 계수를 사용하면 잘못된 결과를 얻을 수 있습니다. 이런 경우, 부분 상관 계수(partical correlation)를 계산하여 혼동되는 변수를 제어할 수 있습니다. 범죄 발생 수(X)와 교회의 수(Y)를 각 각 인구(Z)를 독립변수로 하여 회귀 모형을 구한후, 두 회귀 모형의 residual 항만 사용하여 correlation을 구하는 것이 partial correlation입니다.</p>

<script type="math/tex; mode=display">% <![CDATA[
Let \ X = number \ of \ crime, Y = number \ of \ church, Z = population \\
\begin{align}
X &= w_X  Z + e_{X} \\
Y & = w_Y  Z + e_{Y} \\
\\
\rho_{XY \cdot Z} & = \frac{\rho_{XY}-\rho_{XZ}\rho_{YZ}}{\sqrt{1-\rho_{XZ}^2} \sqrt{1-\rho_{YZ}^2}}
\end{align} %]]></script>

<p>Partial correlation의 개념을 시계열 데이터에 적용한 것이 Partial Auto-Correlation Function(PACF)입니다.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
\phi_{hh} & = Corr(X_t, X_{t-h} | X_{t-h+1}, X_{t-h+2}, ..., X_{t-1}) \\
& = Corr(X_t - (\alpha_1 X_{t-h+1} + \alpha_2 X_{t-h+2} +  ... \alpha_h X_{t-1}), X_{t-h} - (\beta_1 X_{t-h+1} + \beta_2 X_{t-h+2} +  ... \beta_h X_{t-1}))
\end{align} %]]></script>

<p>예를 들어 AR(1) 모델의 partial autocorrelation function 은 아래와 같이 계산됩니다.</p>

<p>AR(1) :</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
\phi_{11} &= Corr(X_t, X_{t-1}) = \rho(1) = \phi \\
\phi_{22} &= Corr(X_t, X_{t-2} | X_{t-1}) = Corr(Z_t, Z_{t-1}) = 0 \\
\phi_{33} &= Corr(X_t, X_{t-3} | X_{t-1}, X_{t-2}) =  Corr(Z_t, X_{t-2}...) = 0
\end{align} %]]></script>

<p>(참고):</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
\phi_{22} & = Corr(X_t, X_{t-2} | X_{t-1}) \\
& = Corr(X_t - (\alpha X_{t-1}), X_{t-2} - (\beta X_{t-1})) \\
& = Corr(Z_t, Z_{t-1}) & since, \ \alpha = corr(X_t, X_{t-1})  = \phi, \beta  = & corr(X_{t-1}, X_{t-2}) = \phi  \\
& = 0
\end{align} %]]></script>

<p>일반적으로 <script type="math/tex">\phi_{hh}</script>는 위의 메트릭스 형태의 Yule-Walker equation의 마지막 컴포턴트와 같습니다.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{bmatrix}
\rho(1)\\ \rho(2)\\ \rho(3)\\ \vdots \\ \rho(h-1)\\\rho(h)
\end{bmatrix} =
\begin{bmatrix}
1 & \rho(1) & \rho(2) & \cdots & \rho(h-1) \\ 
\rho(1) & 1 & \rho(1) & \cdots & \rho(h-2) \\ 
\rho(2) & \rho(1) & 1 & \cdots & \rho(h-3) \\ 
 &  & \vdots & & \\
\rho(h-2) & \rho(h-3) & \rho(h-4) & \cdots & \rho(1) \\ 
\rho(h-1) & \rho(h-2) & \rho(h-3) & \cdots & 1 \\ 
\end{bmatrix} 
\begin{bmatrix}
\phi_{h1}\\ \phi_{h2}\\ \phi_{h3}\\ \vdots \\ \phi_{h(h-1)}\\\phi_{hh}
\end{bmatrix} %]]></script>

<p>AR(2)의 예시와 같이 주어진 데이터가 AR(p) process를 따를경우 PACF의 형태는 lag가 p일때까지는 constant 값을 갖고, 이후의 값은 모두 0에 가까운 값이 됩니다.</p>

<p><img src="/assets/img/2018-12-18/ar1_pacf.png" width="250" />
<img src="/assets/img/2018-12-18/ar2_pacf.png" width="250" />
<img src="/assets/img/2018-12-18/ar3_pacf.png" width="250" /></p>

<p>지금까지 알아본 것을 요약하여, 시계열 데이터가 주어졌을때 모델과 모델의 order를 결정하는 방법은 아래와 같습니다.</p>

<table>
  <thead>
    <tr>
      <th> </th>
      <th>AR(p)</th>
      <th>MA(q)</th>
      <th>ARMA(p, q)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><b>ACF</b></td>
      <td>tails off</td>
      <td>cuts off after lag q</td>
      <td>tails off</td>
    </tr>
    <tr>
      <td><b>PACF</b></td>
      <td>cuts off after lag p</td>
      <td>tails off</td>
      <td>tails off</td>
    </tr>
  </tbody>
</table>

<p><b>reference</b></p>

<p>[1] <a href="https://www.springer.com/us/book/9781475777505">Introduction to Time Series and Forecasting, Peter J. Brockwell, Richard A. Davis,</a></p>

<p>[2] <a href="https://www.statsmodels.org/dev/index.html">Statsmodel’s Documentation</a></p>

<p>[3] <a href="https://www.coursera.org/learn/practical-time-series-analysis/home/info">Coursera - Practical Time Series Analysis</a></p>

<p>[4] <a href="http://www.kocw.net/home/search/kemView.do?kemId=977301">시계열분석 강의, 한양대학교(이기천)</a></p>

<p>[5] <a href="https://en.wikipedia.org/wiki/Partial_correlation">wikipedia - Partial correlation</a></p>

        
      </section>

      <footer class="page__meta">
        
        


  


  
  
  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      
      
      <a href="/categories/#spatio-temporal-data" class="page__taxonomy-item" rel="tag">Spatio-Temporal Data</a>
    
    </span>
  </p>


        
          <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated:</strong> <time datetime="2018-12-18T00:00:00+09:00">December 18, 2018</time></p>
        
      </footer>

      <section class="page__share">
  

  <a href="https://twitter.com/intent/tweet?text=%EC%8B%9C%EA%B3%84%EC%97%B4+%EB%B6%84%EC%84%9D+-+part1%20http%3A%2F%2Flocalhost%3A4000%2Fspatio-temporal%2520data%2Ftime-series-part1%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=http%3A%2F%2Flocalhost%3A4000%2Fspatio-temporal%2520data%2Ftime-series-part1%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://plus.google.com/share?url=http%3A%2F%2Flocalhost%3A4000%2Fspatio-temporal%2520data%2Ftime-series-part1%2F" class="btn btn--google-plus" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Google Plus"><i class="fab fa-fw fa-google-plus" aria-hidden="true"></i><span> Google+</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=http%3A%2F%2Flocalhost%3A4000%2Fspatio-temporal%2520data%2Ftime-series-part1%2F" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/spatio-temporal%20data/deep%20learning%20paper/ST-resnet/" class="pagination--pager" title="Deep Spatio-Temporal Residual Networks for Citywide Crowd Flows Prediction
">Previous</a>
    
    
      <a href="/spatio-temporal%20data/time-series-part2/" class="pagination--pager" title="시계열 분석 - part2
">Next</a>
    
  </nav>

    </div>

    
  </article>

  
  
</div>
    </div>

    

    <div class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    

    
      
        
      
        
      
        
      
        
      
        
      
        
      
    

    <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2018 yjucho. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>
  <script src="https://use.fontawesome.com/releases/v5.3.1/js/all.js"></script>







    
  <script>
    var disqus_config = function () {
      this.page.url = "http://localhost:4000/spatio-temporal%20data/time-series-part1/";  // Replace PAGE_URL with your page's canonical URL variable
      this.page.identifier = "/spatio-temporal%20data/time-series-part1"; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };
    (function() { // DON'T EDIT BELOW THIS LINE
      var d = document, s = d.createElement('script');
      s.src = 'https://yjucho.disqus.com/embed.js';
      s.setAttribute('data-timestamp', +new Date());
      (d.head || d.body).appendChild(s);
    })();
  </script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>


  



<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>



  </body>
</html>