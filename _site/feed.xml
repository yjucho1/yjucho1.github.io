<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.7.4">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2018-12-24T20:45:16+09:00</updated><id>http://localhost:4000/</id><title type="html">yjucho’s blog</title><subtitle>DATA, Deep Learning, AI</subtitle><author><name>yjucho</name></author><entry><title type="html">시계열 분석 - part1</title><link href="http://localhost:4000/spatio-temporal%20data/time-series-part1/" rel="alternate" type="text/html" title="시계열 분석 - part1" /><published>2018-12-18T00:00:00+09:00</published><updated>2018-12-18T00:00:00+09:00</updated><id>http://localhost:4000/spatio-temporal%20data/time-series-part1</id><content type="html" xml:base="http://localhost:4000/spatio-temporal%20data/time-series-part1/">&lt;p&gt;시계열 데이터는 일정 시간동안 수집된 일련의 데이터로, 시간에 따라서 샘플링되었기 때문에 인접한 시간에 수집된 데이터는 서로 상관관계가 존재하게 됩니다. 따라서 시계열 데이터를 추론하는데, 전통적인 통계적 추론이 적합하지 않을수 있습니다. 현실 세계에서는 날씨, 주가, 마케팅 데이터 등등 다양한 시계열 데이터가 존재하고, 시계열 분석을 통해 이러한 데이터가 생성되는 메카니즘을 이해하여 설명가능한 모델로서 데이터를 표현하거나, 미래 값을 예측하는 것, 의미있는 신호값만만 분리하는 일에 활용할 수 있습니다.&lt;/p&gt;

&lt;p&gt;일반적인 시계열분석의 과정은 아래와 같습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Plot the series and examine the main features of the graph, checking in particular whether there is
    &lt;ul&gt;
      &lt;li&gt;A trend&lt;/li&gt;
      &lt;li&gt;A seasonal component&lt;/li&gt;
      &lt;li&gt;Any apparent sharp changes in behavior&lt;/li&gt;
      &lt;li&gt;Any outlying observations&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Remove the trend and seasonal components to get stationary residuals
    &lt;ul&gt;
      &lt;li&gt;To achieve goal, you may need to apply a preliminary transformation like taking logarithms&lt;/li&gt;
      &lt;li&gt;Estimate the trend and seasonal components using classical decomposition model&lt;/li&gt;
      &lt;li&gt;Or Eliminate the trend and seasonal components using differencing&lt;/li&gt;
      &lt;li&gt;Anyway, the goal is to get stationary residuals&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Choose a model to fit the residuals&lt;/li&gt;
  &lt;li&gt;Forecasting the residuals and then inverting the transformations for original series&lt;/li&gt;
  &lt;li&gt;Alternative approach is to express the series in terms of its Fourier components. But it will not be discussed in here.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;시계열 분석은 시간의 경과에 따라 변하지 않는 어떤 특성을 가진 프로세스를 분석하는 것입니다. 만약 시계열 데이터를 예측하고자 한다면, 우리는 시간에 따라 변하지 않는 무언가가 있다는 것을 반드시 가정해야합니다. 예를 들면 평균이 변하지 않는것(no trend), 분산이 변하지 않고, 주기적인 패턴이 없는 데이터를 가정합니다. 이러한 속성을 “stationary”하다고 합니다. 어떤 시계열 데이터가 stationary하다는 가정을 하면, 우리는 다양한 기법들을 활용할수 있습니다. 앞으로의 포스팅에서는 이러한 기법들이 무엇인지 소개하려고 합니다.&lt;/p&gt;

&lt;h3 id=&quot;stationary-auto-covariance-function-and-auto-correlation-function&quot;&gt;Stationary, Auto-Covariance Function and Auto-Correlation Function&lt;/h3&gt;

&lt;p&gt;시계열 &lt;script type=&quot;math/tex&quot;&gt;\{X_t, t=0, \pm 1, ...\}&lt;/script&gt; 가 stationary하다는 것은 h lag만큼 time-shifted 된 시계열 &lt;script type=&quot;math/tex&quot;&gt;\{X_{t+h}, t=0, \pm 1, ...\}&lt;/script&gt;와 통계적인 속성이 유사하다는 것을 의미합니다. 이 때 통계적인 속성을 first and second order moments(mean and covariance)로만 제한하면, 아래와 같이 수식적 정의를 할 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Definition&lt;b&gt;&lt;/b&gt;&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;Let &lt;script type=&quot;math/tex&quot;&gt;{X_t}&lt;/script&gt; be a time series with &lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
E(X_t^2) &lt; \infty %]]&gt;&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;The mean function of &lt;script type=&quot;math/tex&quot;&gt;{X_t}&lt;/script&gt; is &lt;script type=&quot;math/tex&quot;&gt;\mu_X(t) = E(X_t)&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;The covariance function of &lt;script type=&quot;math/tex&quot;&gt;{X_t}&lt;/script&gt; is &lt;script type=&quot;math/tex&quot;&gt;\rho_X(r, s) = Cov(X_r, X_s) = E[(X_r - \mu_X(r))(X_s - \mu_X(s))]&lt;/script&gt; for all integers r and s.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Definition&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;{X_t}&lt;/script&gt; is (weakly) stationary if&lt;/p&gt;

&lt;p&gt;1) &lt;script type=&quot;math/tex&quot;&gt;\mu_X(t)&lt;/script&gt; is independent of t.&lt;/p&gt;

&lt;p&gt;2) &lt;script type=&quot;math/tex&quot;&gt;\gamma_X(t+h, t)&lt;/script&gt; is independent of t for each h.&lt;/p&gt;

&lt;p&gt;Strictly stationary is also&lt;/p&gt;

&lt;p&gt;3) &lt;script type=&quot;math/tex&quot;&gt;(X_1, ..., Xn)&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;(X_{1+h}, ..., X_{n+h})&lt;/script&gt; have the same joint distributions for all h and n &amp;gt;0&lt;/p&gt;

&lt;p&gt;2)에서의 정의를 이용해 stationary한 타임시리즈의 &lt;script type=&quot;math/tex&quot;&gt;\gamma_X(t+h, t)&lt;/script&gt;는 t에 대해서 무관하기 때문에 &lt;script type=&quot;math/tex&quot;&gt;\gamma(\cdot)&lt;/script&gt;는 “autocovariance function”으로 부르며, &lt;script type=&quot;math/tex&quot;&gt;\gamma_X(h)&lt;/script&gt;는 h lag에서의 값을 지칭하는 것으로 하겠습니다. 또한 covariance를 normalization하여 correlation을 함께 정의할수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Definition&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;Let &lt;script type=&quot;math/tex&quot;&gt;{X_t}&lt;/script&gt; be a stationary time series.&lt;/p&gt;

&lt;p&gt;The autocovariance function (ACVF) of &lt;script type=&quot;math/tex&quot;&gt;{X_t}&lt;/script&gt; at lag h is &lt;script type=&quot;math/tex&quot;&gt;\gamma_X(h) = Cov(X_{t+h}, X_t)&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;The autocorrelation function (ACF) of &lt;script type=&quot;math/tex&quot;&gt;{X_t}&lt;/script&gt; at lag h is &lt;script type=&quot;math/tex&quot;&gt;\rho_X(h) \equiv \frac{\gamma_X(h)}{\gamma_X(0)} = Cor(X_{t+h}, X_t)&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;stationary time series의 대표적인 예는 iid noise와 white noise가 있습니다. iid noise는 &lt;script type=&quot;math/tex&quot;&gt;\{X_t\}&lt;/script&gt;가 동일하고(identically) 서로 독립적인(independent) 분포를 따르며, 평균이 0인 확률변수로 정의됩니다. t에 상관없이 평균이 0이고, &lt;script type=&quot;math/tex&quot;&gt;\gamma_X(\cdot)&lt;/script&gt;도 0이기때문에 stationary 조건을 만족합니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
\gamma_X(t+h, t) &amp; = \sigma^2, \ &amp; if \ h=0 \\ 
                 &amp; = 0, \ &amp; if \ h \ne 0
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;마찬가지로 white noise with zero mean and variance &lt;script type=&quot;math/tex&quot;&gt;\sigma^2&lt;/script&gt;도 (weak) staionary 조건을 만족합니다. 참고로 iid noise와 white noise는 다릅니다. 모든 iid noise는 white noise이지만, 그 역은 성립하지 않습니다. &lt;a href=&quot;https://www.researchgate.net/post/What_is_the_difference_between_white_noise_and_iid_noise&quot;&gt;참고&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;시계열 데이터(realization)가 주어졌을때, 이 프로세스의 mean과 covariance를 추정하기 위해 sample mean과 sample covariance function, sample autocorrelation function를 사용합니다.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Definition&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;Let &lt;script type=&quot;math/tex&quot;&gt;x_1, ..., x_n&lt;/script&gt; be observations of a time series.&lt;/p&gt;

&lt;p&gt;The sample mean of &lt;script type=&quot;math/tex&quot;&gt;x_1, ..., x_n&lt;/script&gt; is
&lt;script type=&quot;math/tex&quot;&gt;\bar{x} = \frac{1}{n} \sum_{t=1}^n x_t&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;The sample autocovariance function is
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\hat{\gamma} := n^{-1} \sum_{t=1}^{n-|h|}(x_{t+|h|} - \bar{x})(x_{t} - \bar{x}), -n &lt; h &lt; n %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;The sample autocorrelation function is
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\hat{\rho} := \frac{\hat{\gamma}(h)}{\hat{\gamma}(0)}, -n &lt; h &lt; n %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;h3 id=&quot;moving-average-and-auto-regressive-process&quot;&gt;Moving Average and Auto Regressive process&lt;/h3&gt;

&lt;p&gt;시계열 분석의 문제는 stocastic process의 realization인 시계열 데이터 &lt;script type=&quot;math/tex&quot;&gt;\{X_t\}&lt;/script&gt;가 주어졌을때(그리고 &lt;script type=&quot;math/tex&quot;&gt;\{X_t\}&lt;/script&gt;가 stationary할때), 우리는 이 데이터가 생성된 본래의 stocastic process를 모델링하고 싶다는 겁니다. 데이터가 주어져있기때문에 우리는 (sample) mean과 lag에 따른 covariance과 correlation은 쉽게 구할수 있는 상황입니다.(auto covariance function과 auto correlation function)&lt;/p&gt;

&lt;p&gt;만약 ACF가 주어졌을때 이에 대응되는 stationary stochatic process가 unique하게 결정된다면 아주 쉬운 문제가 됩니다. analytic한 솔루션이 1개 존재하는 것이고, 그건 공식에 맞춰 계산하면 되는 문제가 되니까요. 그러한 특징과 관련된 2가지 모델을 살펴보도록 하겠습니다.&lt;/p&gt;

&lt;p&gt;The MA(q) process:&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;\{X_t\}&lt;/script&gt; is a moving-average process of order q if
&lt;script type=&quot;math/tex&quot;&gt;X_t = Z_t + \theta_1 Z_{t-1} + ... + \theta_q Z_{t-q}&lt;/script&gt; &lt;br /&gt;
where &lt;script type=&quot;math/tex&quot;&gt;\{Z_t\} \sim WN(0, \sigma^2)&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\theta_1, ..., \theta_q&lt;/script&gt; are constants.&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\{X_t\}&lt;/script&gt;가 t 이전 시점의 white noise &lt;script type=&quot;math/tex&quot;&gt;\{Z_x\}&lt;/script&gt;(s &lt;script type=&quot;math/tex&quot;&gt;\le&lt;/script&gt; t)로 표현되는 프로세스를 Moving Average process라고 합니다.  앞서 본 “stationary” 정의에 따라, MA(q) process는 항상 (weakly) stationary합니다.&lt;/p&gt;

&lt;p&gt;invertibility :&lt;/p&gt;

&lt;p&gt;흥미롭게도 MA(1)은 AR(&lt;script type=&quot;math/tex&quot;&gt;\infty&lt;/script&gt;)로 변환될수 있습니다. 아래 수식에서 볼수 있듯이 &lt;script type=&quot;math/tex&quot;&gt;Z_t&lt;/script&gt;는 &lt;script type=&quot;math/tex&quot;&gt;X_t&lt;/script&gt;의 과거값들의 linear combination로 표현될수 있습니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
X_t &amp; = Z_t + \theta Z_{t-1} \\
Z_t &amp; = X_t - \theta Z_{t-1} \\
    &amp; = X_t - \theta(X_{t-1} - \theta Z_{t-2}) \\
    &amp; = X_t - \theta X_{t-1} + \theta^2 (X_{t-2} - \theta Z_{t-3}) \\
    &amp; = X_t - \theta X_{t-1} + \theta^2 X_{t-2} - \theta^3 X_{t-3} + ... + (-\theta)^n Z_{t-n}, &amp;
when \left\vert \theta \right\vert \lt 1, (-\theta)^n Z_{t-n} \approx 0 \\
    &amp; = \sum_{n=0}^{\infty} (-\theta)^nX_{t-n} \\
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;이러한 성질을 일반화하여 MA(q)에 대해 이야기할 수 있습니다. white noise인 &lt;script type=&quot;math/tex&quot;&gt;Z_t&lt;/script&gt;를 &lt;script type=&quot;math/tex&quot;&gt;X_t&lt;/script&gt;의 무한등비급수의 형태로 표현할수 있다면, 주어진 &lt;script type=&quot;math/tex&quot;&gt;\{X_t\}&lt;/script&gt;는 invertible하다고 정의합니다. 이 때 수렴 조건(MA(1)에서의 &lt;script type=&quot;math/tex&quot;&gt;\left\vert \theta \right\vert \lt 1&lt;/script&gt;)을 invertibility condition이라고 하고 합니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
Z_t &amp; = \theta(B)X_t, \ &amp; where \ \theta(\cdot) \ are \ the \ q-th \ degree \ polynomials \\
\theta(z) &amp; = 1 + \theta z + ... + \theta_q z^q
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;자세한 증명은 여기서 다루지 않지만, 결론적으로는 &lt;script type=&quot;math/tex&quot;&gt;\theta(z)&lt;/script&gt;의 해가 unit circle 밖에 있는 경우 invertible 조건을 만족하게 됩니다.&lt;/p&gt;

&lt;p&gt;Invertibility is equivalent to the condition&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\theta(z) = 1+ \theta_1 z + ... + \theta_q z^q \ne 0 \ for \ all \left\vert z \right\vert \le 1&lt;/script&gt;

&lt;p&gt;invertible이 중요한 이유는 ACF가 주어질 때, 이 ACF를 만족하는 MA process가 unique하게 결정되기 때문입니다.&lt;/p&gt;

&lt;p&gt;두번재 모델은 Auto-Regressive Process입니다. &lt;script type=&quot;math/tex&quot;&gt;\{X_t\}&lt;/script&gt;가 이전 시점의 자기 자신 값 &lt;script type=&quot;math/tex&quot;&gt;\{X_s\}&lt;/script&gt;(s &lt;script type=&quot;math/tex&quot;&gt;\le&lt;/script&gt; t)와 t 시점의 white noise &lt;script type=&quot;math/tex&quot;&gt;\{Z_x\}&lt;/script&gt;로 표현되는 프로세스를 Auto-Regress process라고 합니다. 기억해야할 점은 MA process와 달리 AR process는 항상 stationary한 것은 아니라는 점입니다.&lt;/p&gt;

&lt;p&gt;The AR(q) process:&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;\{X_t\}&lt;/script&gt; is a auto-regressive process of order q if
&lt;script type=&quot;math/tex&quot;&gt;X_t = Z_t + \phi_1 X_{t-1} + ... + \phi_q X_{t-q}&lt;/script&gt; &lt;br /&gt;
where &lt;script type=&quot;math/tex&quot;&gt;\{Z_t\} \sim WN(0, \sigma^2)&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\phi_1, ..., \phi_q&lt;/script&gt; are constants&lt;/p&gt;

&lt;p&gt;MA process의 invertibility 와 유사한 개념으로 AR process에 casuality 개념을 도입할수 있습니다. 아래는 AR(1) process를 MA(&lt;script type=&quot;math/tex&quot;&gt;\infty&lt;/script&gt;)로 변환하는 예시입니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
X_t &amp; = \phi X_{t-1} + Z_t \\
    &amp; = \phi (\phi X_{t-2} + Z_{t-1}) + Z_t \\
    &amp; = \phi^2 X_{t-2} + \phi Z_{t-1} + Z_t  \\
    &amp; = \phi^2 (\phi X_{t-3} + Z_{t-2}) + \phi Z_{t-1} + Z_t \\
    &amp; = Z_t + \phi Z_{t-1} + \phi^2 Z_{t-3} + ... + \phi^n X_{t-n}, &amp;
when \left\vert \phi \right\vert \lt 1, \phi^n X_{t-n} \approx 0 \\
    &amp; = \sum_{n=0}^{\infty} (\phi)^n Z_{t-n} \\
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;X_t&lt;/script&gt;를 white noise인 &lt;script type=&quot;math/tex&quot;&gt;Z_x&lt;/script&gt;(단, s &lt;script type=&quot;math/tex&quot;&gt;\le&lt;/script&gt; t)의 linear combination 형태로 표현된다면, 주어진 &lt;script type=&quot;math/tex&quot;&gt;\{X_t\}&lt;/script&gt;는 causal하다고 정의합니다. 위의 예시인 AR(1) process &lt;script type=&quot;math/tex&quot;&gt;\{X_t\}&lt;/script&gt;가 causal하기 위해서는 &lt;script type=&quot;math/tex&quot;&gt;\left\vert \phi \right\vert \lt 1&lt;/script&gt; 입니다.&lt;/p&gt;

&lt;p&gt;이러한 성질을 일반적인 AR(p) process에 대해서도 이야기할수 있습니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
X_t &amp; = \phi(B)X_t, \ &amp; where \ \phi(\cdot) \ are \ the \ p-th \ degree \ polynomials \\
\phi(z) &amp; = 1+ \phi z + ... + \phi_p z^p
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;자세한 증명은 여기서 다루지 않지만, 결론적으로는 &lt;script type=&quot;math/tex&quot;&gt;\phi(z)&lt;/script&gt;의 해가 unit circle 밖에 있는 경우 causality 조건을 만족하게 됩니다.&lt;/p&gt;

&lt;p&gt;Causality is equivalent to the condition&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\phi(z) = 1 + \phi_1 z + ... + \phi_p z^p \ne 0 \ for \ all \left\vert z \right\vert \le 1&lt;/script&gt;

&lt;p&gt;중요한 것은 causality를 만족하는 AR(p) process는 항상 stationary하고, stationary AR(p) process는 항상 causality를 만족합니다.&lt;/p&gt;

&lt;h3 id=&quot;armap-q&quot;&gt;ARMA(p, q)&lt;/h3&gt;

&lt;p&gt;AR(p)와 MA(q)가 합쳐진 process를 ARMA(p, q)로 표기하고 ARMA(p,q) process가 유일한 stationary solution &lt;script type=&quot;math/tex&quot;&gt;\{X_t\}&lt;/script&gt;를 갖는 조건은 causality condition을 만족할 때이며, 그 역도 성립합니다.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Definition&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\{X_t\}&lt;/script&gt; is an ARMA(p, q) process if &lt;script type=&quot;math/tex&quot;&gt;\{X_t\}&lt;/script&gt; is stationary and if for every t,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;X_t - \phi_1 X_{t-1} - ... - \phi_p X_{t-p} = Z_t + \theta_1 Z_{t-1} + ... + \theta_q Z_{t-q}&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;\{Z_t\} \sim WN(0, \sigma^2)&lt;/script&gt; and the polynomials &lt;script type=&quot;math/tex&quot;&gt;( 1 - \phi z - ... - \phi_p z^p)&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;(1+\theta_1 z + ... + \theta_q z^q )&lt;/script&gt; have no common factors.&lt;/p&gt;

&lt;p&gt;Existence and Uniqueness :&lt;/p&gt;

&lt;p&gt;A stationary solution &lt;script type=&quot;math/tex&quot;&gt;\{X_t\}&lt;/script&gt; of equation ARMA(p, q) exists (and is also the unique stationary solution) if and only if&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\phi(z) = 1 + \phi_1 z + ... + \phi_p z^p \ne 0 \ for \ all \left\vert z \right\vert \le 1&lt;/script&gt;

&lt;p&gt;&lt;b&gt;Example&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;아래와 같은 조건을 만족하는 ARMA(1, 1) process &lt;script type=&quot;math/tex&quot;&gt;\{X_t\}&lt;/script&gt;를 생각해보도록 하겠습니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;X_t - 0.5 X_{t-1} = Z_t + 0.4 Z_{t-1}, \ \ \ {Z_t} \sim WN(0, \sigma^2)&lt;/script&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\phi(z) = 1 - 0.5 z&lt;/script&gt;는 z=2 일때 0이 되고, 이는 unit circle밖에 위치하기 때문에 이는 causality 조건을 만족합니다. 즉, 유니크한 솔루션이 존재합니다. causal하기때문에 &lt;script type=&quot;math/tex&quot;&gt;X_t = \sum_{j=0}^\infty \psi_j Z_{t-j}&lt;/script&gt;로 표현되는 constant &lt;script type=&quot;math/tex&quot;&gt;\{\psi\}&lt;/script&gt;가 존재합니다.&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;X_t = \sum_{j=0}^\infty \psi_j Z_{t-j}&lt;/script&gt;를 &lt;script type=&quot;math/tex&quot;&gt;X_t - \phi X_{t-1} = Z_t + \theta Z_{t-1}&lt;/script&gt;에 대입하면 아래와 같은 식이 성립해야합니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;(1-\phi z - ... - \phi_p z^p)(\psi_0 +\psi_1 z + ...) = 1 +\theta_1z + ... + \theta_qz^q&lt;/script&gt;

&lt;p&gt;양변의 &lt;script type=&quot;math/tex&quot;&gt;z^j, j=0,1,...&lt;/script&gt; 계수가 동일해야하므로,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
1 &amp; = \psi_0 \\
\theta_1 &amp; = \psi_1 - \psi_0 \phi_1 \\
\theta_2 &amp; = \psi_2 - \psi_1 \phi_1 - \psi_0 \phi_2 \\
\vdots
\end{align} %]]&gt;&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\psi_j - \sum_{k=1}^p \phi_k \psi_{j-k} = \theta_j, j=0,1, ...&lt;/script&gt;

&lt;p&gt;예제의 계수를 대입하면,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
\psi_0 &amp; = 1\\
\psi_1 &amp; = 0.4 + 1 * 0.5   \\
\psi_2 &amp; = 0.5 (0.4 + 0.5) \\
\psi_j &amp; = 0.5^{j-1} (0.4 + 0.5)
\end{align} %]]&gt;&lt;/script&gt;

&lt;h3 id=&quot;yule-walker-equation&quot;&gt;Yule-Walker Equation&lt;/h3&gt;

&lt;p&gt;cauality 조건을 만족하는 ARMA(p,q) process에 대해서 ACF를 이용해 모델 파라미터를 체계적으로 구할수 있는 방법을 알아보도록 하겠습니다.&lt;/p&gt;

&lt;p&gt;설명의 편의성을 위해 AR(2)를 가정하도록 하겠습니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;X_t - \phi_1 X_{t-1} - \phi_2 X_{t-2} = Z_t&lt;/script&gt;

&lt;p&gt;위 양변에 &lt;script type=&quot;math/tex&quot;&gt;X_{t-k}&lt;/script&gt;를 곱하고 Expectation을 취해보도록 하겠습니다. 
&lt;script type=&quot;math/tex&quot;&gt;E[X_t X_{t-k}] - \phi_1 E[X_{t-1} X_{t-k}] - \phi_2 E[X_{t-2} X_{t-k}] = E[Z_t X{t-k}]&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;k를 0부터 1, 2, … 순차적으로 대입하면 아래와 같습니다.&lt;/p&gt;

&lt;p&gt;when k = 0,   &lt;script type=&quot;math/tex&quot;&gt;\gamma(0) -\phi_1 \gamma(1) -\phi_2 \gamma(2) = \sigma^2&lt;/script&gt; &lt;br /&gt;
when k = 1,   &lt;script type=&quot;math/tex&quot;&gt;\gamma(1) -\phi_1 \gamma(0) -\phi_2 \gamma(1) = 0&lt;/script&gt; &lt;br /&gt;
when k = 2,   &lt;script type=&quot;math/tex&quot;&gt;\gamma(2) -\phi_1 \gamma(1) -\phi_2 \gamma(0) = 0&lt;/script&gt; &lt;br /&gt;
… …&lt;/p&gt;

&lt;p&gt;즉,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\gamma(h) -\phi_1 \gamma(h-1) -\phi_2 \gamma(h-2)
= \begin{cases} 
\sigma^2, &amp; \mbox{h=0} \\
0, &amp; \mbox{h=1}
\end{cases} %]]&gt;&lt;/script&gt;

&lt;p&gt;양변을 &lt;script type=&quot;math/tex&quot;&gt;\gamma(0)&lt;/script&gt;로 나누어, Auto-correlation으로 나타내면&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\rho(h) -\phi_1 \rho(h-1) -\phi_2 \rho(h-2) 
= \begin{cases} 
1 &amp; \mbox{h=0} \\
0, &amp; \mbox{h=1}
\end{cases} %]]&gt;&lt;/script&gt;

&lt;p&gt;위와 같은 식을 Yule-Walker equation 이라고 합니다. 
Yule-walker equation을 이용해 AR(2)모델의 &lt;script type=&quot;math/tex&quot;&gt;\phi_1, \phi_2&lt;/script&gt;를 구하는 방법은 h=1일때, h=2일때를 대입하여 연립방정식을 푸는 것과 같습니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;h = 1 , \rho(1) -\phi_1 \rho(0) -\phi_2 \rho(1) = 0 \\
h = 2 , \rho(2) -\phi_1 \rho(1) -\phi_2 \rho(0) = 0&lt;/script&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\rho(0)=1&lt;/script&gt;이고, &lt;script type=&quot;math/tex&quot;&gt;\rho(1)&lt;/script&gt; 와 &lt;script type=&quot;math/tex&quot;&gt;\rho(2)&lt;/script&gt;는 주어진 데이터를 이용해 sample ACF로 계산하고, 위의 식을 이용해 &lt;script type=&quot;math/tex&quot;&gt;\phi_1&lt;/script&gt;과 &lt;script type=&quot;math/tex&quot;&gt;\phi_2&lt;/script&gt;를 계산할수 있습니다.&lt;/p&gt;

&lt;p&gt;지금까지 AR(2) 모델에 대해서 살펴본 과정을 AR(p) process에 대해서 일반화할 수 있습니다. AR(p) process에 대한 Yule-walker equation을 적으면 아래와 같습니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\gamma(h) - \phi_1 \gamma(h-1) - ... - \phi_p \gamma(h-p) = \begin{cases} 
\sigma^2 &amp; \mbox{h=0} \\
0, &amp; \mbox{h &gt;= 1}
\end{cases} %]]&gt;&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\rho(h) - \phi_1 \rho(h-1) - ... - \phi_p \rho(h-p) = \begin{cases} 
1 &amp; \mbox{h=0} \\
0, &amp; \mbox{h &gt;= 1}
\end{cases} %]]&gt;&lt;/script&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\rho(h) = \phi_1 \rho(h-1) + ... + \phi_p \rho(h-p), \ \ \ \ when \ h \ge 1&lt;/script&gt; 이를 Matrix 형태로 적어보도록 하겠습니다. (메트릭스 형태는 다음에 설명한 Partial ACF와의 관계를 설명할 때 유용합니다)&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
\rho(0) &amp; = 1 \\
\rho(1) &amp; = \phi_1  + \phi_2 \rho(1) + \phi_3 \rho(2) + ... +\phi_p \rho(p-1) \\
\rho(2) &amp; = \phi_1 \rho(1) + \phi_2 +  \phi_3 \rho(1) + ... +\phi_p \rho(p-2) \\
\rho(3) &amp; = \phi_1 \rho(2) + \phi_2 \rho(1) +  \phi_3 + ... +\phi_p \rho(p-3) \\
... \\
\rho(p-1) &amp; = \phi_1 \rho(p-2) + \phi_2 \rho(p-3) +  \phi_3 \rho(p-4) + ... +\phi_p \rho(1) \\
\rho(p) &amp; = \phi_1 \rho(p-1) + \phi_2 \rho(p-2) +  \phi_3 \rho(p-3) + ... +\phi_p 
\end{align} %]]&gt;&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{bmatrix}
\rho(1)\\ \rho(2)\\ \rho(3)\\ \vdots \\ \rho(p-1)\\\rho(p)
\end{bmatrix} =
\begin{bmatrix}
1 &amp; \rho(1) &amp; \rho(2) &amp; \cdots &amp; \rho(p-1) \\ 
\rho(1) &amp; 1 &amp; \rho(1) &amp; \cdots &amp; \rho(p-2) \\ 
\rho(2) &amp; \rho(1) &amp; 1 &amp; \cdots &amp; \rho(p-3) \\ 
 &amp;  &amp; \vdots &amp; &amp; \\
\rho(p-2) &amp; \rho(p-3) &amp; \rho(p-4) &amp; \cdots &amp; \rho(1) \\ 
\rho(p-1) &amp; \rho(p-2) &amp; \rho(p-3) &amp; \cdots &amp; 1 \\ 
\end{bmatrix} 
\begin{bmatrix}
\phi(1)\\ \phi(2)\\ \phi(3)\\ \vdots \\ \phi(p-1)\\\phi(p)
\end{bmatrix} %]]&gt;&lt;/script&gt;

&lt;p&gt;여기까지 우리는 시계열 데이터가 주어졌을때, ARMA(p,q)의 파라미터를 추정하는 방법을 살펴보았습니다. 하지만 파라미터를 추정하기 전에 p와 q(모델의 order)를 결정하는 것이 우선되어야합니다.&lt;/p&gt;

&lt;p&gt;MA(q)에 대해서는 간단합니다. MA process의 정의에 따라, MA(q)는 현재 시점의 데이터 &lt;script type=&quot;math/tex&quot;&gt;X_t&lt;/script&gt;가 이전 q개의 noise로만 표현되기 때문에 q 이전의 데이터들과는 무관합니다. MA(q)의 ACF는 아래와 같이 q개의 유의미한 spike를 갖고 이후 값들은 모두 0에 가깝습니다(negligible)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-12-18/MA_acf.png&quot; width=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;p&gt;반면 AR(p)는 ACF만으로 p를 결정하기가 어렵습니다. AR process의 ACF는 lag가 증가할수록 decay한 모습을 보일뿐, p에 대한 힌트를 주지 못하기 때문입니다.  다음에서는 주어진 데이터로부터 어떻게 AR(p)모델의 order를 결정할 수 있는지 알아보도록 하겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-12-18/AR_acf.png&quot; width=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;particial-acf&quot;&gt;Particial ACF&lt;/h3&gt;

&lt;p&gt;범죄(crime) 발생 수 와 교회의 수(church)의 상관계수는 양의 상관관계를 갖고 있다고 합니다. 정말로 범죄가 많은 지역에 교회의 수가 많고, 또는 교회의 수가 많은 지역에 범죄가 발생할 가능성도 높은 걸까요? 아닙니다. 이는 두 요인과 관련있는 다른 요인, 인구(population)에 대한 요인을 고려하지 못했기때문에 발생하는 잘못된 결과 해석입니다. 이처럼 두 변수 사이에 수치적 관계가 있는지 또는 어느 정도로 관련이 있는지를 찾을 때, 두 변수와 관련된 다른 변수가있을 경우 해당 상관 계수를 사용하면 잘못된 결과를 얻을 수 있습니다. 이런 경우, 부분 상관 계수(partical correlation)를 계산하여 혼동되는 변수를 제어할 수 있습니다. 범죄 발생 수(X)와 교회의 수(Y)를 각 각 인구(Z)를 독립변수로 하여 회귀 모형을 구한후, 두 회귀 모형의 residual 항만 사용하여 correlation을 구하는 것이 partial correlation입니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
Let \ X = number \ of \ crime, Y = number \ of \ church, Z = population \\
\begin{align}
X &amp;= w_X  Z + e_{X} \\
Y &amp; = w_Y  Z + e_{Y} \\
\\
\rho_{XY \cdot Z} &amp; = \frac{\rho_{XY}-\rho_{XZ}\rho_{YZ}}{\sqrt{1-\rho_{XZ}^2} \sqrt{1-\rho_{YZ}^2}}
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;Partial correlation의 개념을 시계열 데이터에 적용한 것이 Partial Auto-Correlation Function(PACF)입니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
\phi_{hh} &amp; = Corr(X_t, X_{t-h} | X_{t-h+1}, X_{t-h+2}, ..., X_{t-1}) \\
&amp; = Corr(X_t - (\alpha_1 X_{t-h+1} + \alpha_2 X_{t-h+2} +  ... \alpha_h X_{t-1}), X_{t-h} - (\beta_1 X_{t-h+1} + \beta_2 X_{t-h+2} +  ... \beta_h X_{t-1}))
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;예를 들어 AR(1) 모델의 partial autocorrelation function 은 아래와 같이 계산됩니다.&lt;/p&gt;

&lt;p&gt;AR(1) :&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
\phi_{11} &amp;= Corr(X_t, X_{t-1}) = \rho(1) = \phi \\
\phi_{22} &amp;= Corr(X_t, X_{t-2} | X_{t-1}) = Corr(Z_t, Z_{t-1}) = 0 \\
\phi_{33} &amp;= Corr(X_t, X_{t-3} | X_{t-1}, X_{t-2}) =  Corr(Z_t, X_{t-2}...) = 0
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;(참고):&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
\phi_{22} &amp; = Corr(X_t, X_{t-2} | X_{t-1}) \\
&amp; = Corr(X_t - (\alpha X_{t-1}), X_{t-2} - (\beta X_{t-1})) \\
&amp; = Corr(Z_t, Z_{t-1}) &amp; since, \ \alpha = corr(X_t, X_{t-1})  = \phi, \beta  = &amp; corr(X_{t-1}, X_{t-2}) = \phi  \\
&amp; = 0
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;일반적으로 &lt;script type=&quot;math/tex&quot;&gt;\phi_{hh}&lt;/script&gt;는 위의 메트릭스 형태의 Yule-Walker equation의 마지막 컴포턴트와 같습니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{bmatrix}
\rho(1)\\ \rho(2)\\ \rho(3)\\ \vdots \\ \rho(h-1)\\\rho(h)
\end{bmatrix} =
\begin{bmatrix}
1 &amp; \rho(1) &amp; \rho(2) &amp; \cdots &amp; \rho(h-1) \\ 
\rho(1) &amp; 1 &amp; \rho(1) &amp; \cdots &amp; \rho(h-2) \\ 
\rho(2) &amp; \rho(1) &amp; 1 &amp; \cdots &amp; \rho(h-3) \\ 
 &amp;  &amp; \vdots &amp; &amp; \\
\rho(h-2) &amp; \rho(h-3) &amp; \rho(h-4) &amp; \cdots &amp; \rho(1) \\ 
\rho(h-1) &amp; \rho(h-2) &amp; \rho(h-3) &amp; \cdots &amp; 1 \\ 
\end{bmatrix} 
\begin{bmatrix}
\phi_{h1}\\ \phi_{h2}\\ \phi_{h3}\\ \vdots \\ \phi_{h(h-1)}\\\phi_{hh}
\end{bmatrix} %]]&gt;&lt;/script&gt;

&lt;p&gt;AR(2)의 예시와 같이 주어진 데이터가 AR(p) process를 따를경우 PACF의 형태는 lag가 p일때까지는 constant 값을 갖고, 이후의 값은 모두 0에 가까운 값이 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-12-18/ar1_pacf.png&quot; width=&quot;200&quot; /&gt;
&lt;img src=&quot;/assets/img/2018-12-18/ar2_pacf.png&quot; width=&quot;200&quot; /&gt;
&lt;img src=&quot;/assets/img/2018-12-18/ar3_pacf.png&quot; width=&quot;200&quot; /&gt;&lt;/p&gt;

&lt;p&gt;지금까지 알아본 것을 요약하여, 시계열 데이터가 주어졌을때 모델과 모델의 order를 결정하는 방법은 아래와 같습니다.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt; &lt;/th&gt;
      &lt;th&gt;AR(p)&lt;/th&gt;
      &lt;th&gt;MA(q)&lt;/th&gt;
      &lt;th&gt;ARMA(p, q)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;b&gt;ACF&lt;/b&gt;&lt;/td&gt;
      &lt;td&gt;tails off&lt;/td&gt;
      &lt;td&gt;cuts off after lag q&lt;/td&gt;
      &lt;td&gt;tails off&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;b&gt;PACF&lt;/b&gt;&lt;/td&gt;
      &lt;td&gt;cuts off after lag p&lt;/td&gt;
      &lt;td&gt;tails off&lt;/td&gt;
      &lt;td&gt;tails off&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Model Building &lt;br /&gt;
1) identify model &lt;br /&gt;
2) estimate unknowns &lt;br /&gt;
3) diagonstic checking &lt;br /&gt;
4) prediction &lt;br /&gt;&lt;/p&gt;

&lt;p&gt;for 1), 3) use :&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;ACF, PACF&lt;/li&gt;
  &lt;li&gt;for large - n cases, Box-Ljung test, Sign test, Rank test, q-q plot .. =&amp;gt; for the residuals after fitting the model&lt;/li&gt;
  &lt;li&gt;theoretical predictive power : AIC, BIC&lt;/li&gt;
  &lt;li&gt;empirical predictive power : Cross validation&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;b&gt;reference&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;[1] &lt;a href=&quot;https://www.springer.com/us/book/9781475777505&quot;&gt;Introduction to Time Series and Forecasting, Peter J. Brockwell, Richard A. Davis,&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[2] &lt;a href=&quot;https://www.statsmodels.org/dev/index.html&quot;&gt;Statsmodel’s Documentation&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[3] &lt;a href=&quot;https://www.coursera.org/learn/practical-time-series-analysis/home/info&quot;&gt;Coursera - Practical Time Series Analysis&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[4] &lt;a href=&quot;http://www.kocw.net/home/search/kemView.do?kemId=977301&quot;&gt;시계열분석 강의, 한양대학교(이기천)&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[5] &lt;a href=&quot;https://en.wikipedia.org/wiki/Partial_correlation&quot;&gt;wikipedia - Partial correlation&lt;/a&gt;&lt;/p&gt;</content><author><name>yjucho</name></author><summary type="html">시계열 데이터는 일정 시간동안 수집된 일련의 데이터로, 시간에 따라서 샘플링되었기 때문에 인접한 시간에 수집된 데이터는 서로 상관관계가 존재하게 됩니다. 따라서 시계열 데이터를 추론하는데, 전통적인 통계적 추론이 적합하지 않을수 있습니다. 현실 세계에서는 날씨, 주가, 마케팅 데이터 등등 다양한 시계열 데이터가 존재하고, 시계열 분석을 통해 이러한 데이터가 생성되는 메카니즘을 이해하여 설명가능한 모델로서 데이터를 표현하거나, 미래 값을 예측하는 것, 의미있는 신호값만만 분리하는 일에 활용할 수 있습니다.</summary></entry><entry><title type="html">Deep Spatio-Temporal Residual Networks for Citywide Crowd Flows Prediction</title><link href="http://localhost:4000/spatio-temporal%20data/ST-resnet/" rel="alternate" type="text/html" title="Deep Spatio-Temporal Residual Networks for Citywide Crowd Flows Prediction" /><published>2018-12-16T00:00:00+09:00</published><updated>2018-12-16T00:00:00+09:00</updated><id>http://localhost:4000/spatio-temporal%20data/ST-resnet</id><content type="html" xml:base="http://localhost:4000/spatio-temporal%20data/ST-resnet/">&lt;p&gt;&lt;b&gt; Junbo Zhang, Yu Zheng, Dekang Qi (Microsoft Research) 2017 &lt;/b&gt;&lt;/p&gt;

&lt;p&gt;&lt;b&gt; Keras implementation &lt;/b&gt; : &lt;a href=&quot;https://github.com/lucktroy/DeepST&quot;&gt;https://github.com/lucktroy/DeepST&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Forecating the flow of crowds&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In this paper, we predict two types fo crowd flows : inflow and outflow&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Inflow and outflow of crowds are affected by the following
    &lt;ul&gt;
      &lt;li&gt;Spatial dependencies&lt;/li&gt;
      &lt;li&gt;Temporal dependencies&lt;/li&gt;
      &lt;li&gt;External influence : such as weather, events&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Contributions
    &lt;ul&gt;
      &lt;li&gt;ST-ResNet employs convolution-based residual networks to model nearby and distance spatial dependencies between any two regions&lt;/li&gt;
      &lt;li&gt;three categories of temporal properties : temporal closeness, period, and trend. ST-ResNet use three residual netowrks to model these, respectively&lt;/li&gt;
      &lt;li&gt;ST-ResNet dynamically aggregates the output of the three aforementioned networks.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;formulation-of-crowd-flows-problem&quot;&gt;Formulation of Crowd Flows Problem&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Region : we partition a city into an I*J grid map&lt;/li&gt;
  &lt;li&gt;Inflow/outflow : Let P be a collection of trajectories at the t&lt;sup&gt;th&lt;/sup&gt; time interval. For a grid (i, j) that lies at the i&lt;sup&gt;th&lt;/sup&gt; row and j&lt;sup&gt;th&lt;/sup&gt; column, the inflow and outflow of the crowds at the tiem interval t are defined respectively as
  &lt;script type=&quot;math/tex&quot;&gt;x_t^{in, i, j} = \sum_{T_r \in P} |{k &gt; 1 |g_{k-1} \notin (i, j) \land g_k \in (i, j)}| \\
  x_t^{out, i, j} = \sum_{T_r \in P} |{k \ge 1 |g_{k-1} \in (i, j) \land g_{k+1} \notin (i, j)}|&lt;/script&gt;
  where
    &lt;ul&gt;
      &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;T_r : g_1 \to g_2 \to ... \to g_{\left\vert T_r \right\vert}&lt;/script&gt; is a trajectory in P&lt;/li&gt;
      &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;g_k&lt;/script&gt; is the geospatial coordinate&lt;/li&gt;
      &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;g_k \in (i, j)&lt;/script&gt; means the point &lt;script type=&quot;math/tex&quot;&gt;g_k&lt;/script&gt; lies within grid (i, j), and vice versa&lt;/li&gt;
      &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;\left\vert \cdot \right\vert&lt;/script&gt; denotes the cardinality of a set&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;deep-spatio-temporal-residual-networks&quot;&gt;Deep Spatio-Temporal Residual Networks&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-12-16/ST-ResNet architecture.png&quot; width=&quot;400&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;comprised of four major components modeling &lt;i&gt;temporal closeness, period, trend, and external influence,&lt;/i&gt; respectively.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;First, we turn inflow and outflow throughout a city at each time interval into a 2-channel image-like matrix.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Then, we divide the time axis into three fragments, denoting recent time, near history and distant history. The 2-channel flow matrics of intervals in each time fragment are the fed into the first three components seperately to model the aforementioned three temporal properties: &lt;i&gt;closeness, period, and trend&lt;/i&gt;
    &lt;ul&gt;
      &lt;li&gt;three components share the same network structure(Regisudal Unit sequence)&lt;/li&gt;
      &lt;li&gt;The output of the three components are fused as &lt;script type=&quot;math/tex&quot;&gt;X_{Res}&lt;/script&gt; based on parameter metrics, which assign different weights to the results of different components in different regions.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In the &lt;i&gt;external&lt;/i&gt; component, we manually extract some feature form external datasets, such as weather conditions and events, feeding them into a two-layer fully-connected neural network.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;X_{Res}&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;X_{Ext}&lt;/script&gt; are integrated together. Then, the final output is mapped into [-1, 1] using Tanh function.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;structures-of-the-first-three-components&quot;&gt;Structures of the First Three Components&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/img/2018-12-16/residual unit.png&quot; width=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Do not user subsampling, but only convolutions&lt;/li&gt;
  &lt;li&gt;closeness component
    &lt;ul&gt;
      &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;[X_{t-l_c}, X_{t-l_c-1}, ..., X_{t-1}]&lt;/script&gt; : concatnate them along with the first axis&lt;/li&gt;
      &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;X_c^{(0)} \in R^{2l_c \times I \times J}&lt;/script&gt; is followed by &lt;code class=&quot;highlighter-rouge&quot;&gt;conv1&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Residual Unit&lt;/code&gt; : stack &lt;script type=&quot;math/tex&quot;&gt;L&lt;/script&gt; residual units to capture very large citywide dependencies&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Residual Unit&lt;/code&gt; combinations fo “ReLu + Convolution” and “BatchNormalization” is added before ReLu.&lt;/li&gt;
      &lt;li&gt;On top of the &lt;script type=&quot;math/tex&quot;&gt;L^{th}&lt;/script&gt; residual unit, we append a convolutional layer &lt;code class=&quot;highlighter-rouge&quot;&gt;conv2&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;output of the closeness componet is &lt;script type=&quot;math/tex&quot;&gt;X_c^{(L+2)}&lt;/script&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-12-16/component.png&quot; width=&quot;300&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;period component
    &lt;ul&gt;
      &lt;li&gt;Assume that there are &lt;script type=&quot;math/tex&quot;&gt;l_p&lt;/script&gt; time intervals from the period fragment and the period is &lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt; ;&lt;script type=&quot;math/tex&quot;&gt;[X_{t-l_p \cdot p}, X_{t-(l_p-1) \cdot p}, ..., X_{t-p}]&lt;/script&gt;&lt;/li&gt;
      &lt;li&gt;output : &lt;script type=&quot;math/tex&quot;&gt;X_p^{(L+2)}&lt;/script&gt;&lt;/li&gt;
      &lt;li&gt;in implementation, p is equal to one-day (daily periodicity)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;trend component
    &lt;ul&gt;
      &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;l_q&lt;/script&gt; is the length of the trend dependent sequence and q is the trend span&lt;/li&gt;
      &lt;li&gt;input : &lt;script type=&quot;math/tex&quot;&gt;[X_{t-l_q \cdot q}, X_{t-(l_q-1) \cdot q}, ..., X_{t-q}]&lt;/script&gt;&lt;/li&gt;
      &lt;li&gt;output : &lt;script type=&quot;math/tex&quot;&gt;X_q^{(L+2)}&lt;/script&gt;&lt;/li&gt;
      &lt;li&gt;in implementation, q is equal to one-week(week trend)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;the-structure-of-the-external-component&quot;&gt;The Structure of the External Component&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;mainly consider weather, holiday event, and metadata(DayOfWeek, Weekday/Weekend)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;stack two fully-connected layers upon &lt;script type=&quot;math/tex&quot;&gt;E_t&lt;/script&gt;&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;first layer : embedding layer&lt;/li&gt;
      &lt;li&gt;second layer : to map low to high dimensions that have the same shape with &lt;script type=&quot;math/tex&quot;&gt;X_t&lt;/script&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;fusion&quot;&gt;Fusion&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;flows of two regions are all affected by closeness, period, and trend, but the degrees of influence may be very different ; parametric-matrix-based fusion&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;X_{Res} = W_c \circ X_c^{L+2} + W_p \circ X_p^{L+2} + W_q \circ X_q^{L+2}&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;\circ&lt;/script&gt; is Hadamard product (i.e., element-wise multiplication)&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;W_c, W_p, W_q&lt;/script&gt; are learnable parameters&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;fusing the external component&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{X_t} = tanh(X_{Res} + X_{Ext})&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;objectives : minimizing mean squared error between the predicted flow matrix and the true flow matrix.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Datasets&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-12-16/datasets.png&quot; width=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Baselines
    &lt;ul&gt;
      &lt;li&gt;HA : historical data (previous week, same time)&lt;/li&gt;
      &lt;li&gt;ARIMA, SARIMA, VAR&lt;/li&gt;
      &lt;li&gt;ST-ANN : It first extracts spatial (nearby 8 regions’ values) and temporal (8 previous time intervals) features, then fed into an artificial neural network.&lt;/li&gt;
      &lt;li&gt;DeepST : (Zhang et al. 2016)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Preprocessing
    &lt;ul&gt;
      &lt;li&gt;min-max normalization : [-1, 1] (tanh)&lt;/li&gt;
      &lt;li&gt;one-hot encoding for external data&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Result&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-12-16/result1.png&quot; width=&quot;300&quot; /&gt;
&lt;img src=&quot;/assets/img/2018-12-16/result2.png&quot; width=&quot;300&quot; /&gt;&lt;/p&gt;</content><author><name>yjucho</name></author><summary type="html">Junbo Zhang, Yu Zheng, Dekang Qi (Microsoft Research) 2017</summary></entry><entry><title type="html">위경도 - 기상청 격자 맵핑</title><link href="http://localhost:4000/spatio-temporal%20data/latlon-to-grid/" rel="alternate" type="text/html" title="위경도 - 기상청 격자 맵핑" /><published>2018-12-15T00:00:00+09:00</published><updated>2018-12-15T00:00:00+09:00</updated><id>http://localhost:4000/spatio-temporal%20data/latlon-to-grid</id><content type="html" xml:base="http://localhost:4000/spatio-temporal%20data/latlon-to-grid/">&lt;p&gt;기상청은 전국을 5km×5km 간격의 촘촘한 격자화하여 읍,면,동 단위로 상세한 날씨를 제공하는 동네예보를 제공합니다. 구역별 기상데이터를 관리하기 위해 한반도를 가로로 149개, 세로로 253개의 선을 그어 그리드형태로 관리하며, 위경도 데이터를 이 그리드 상의 좌표로 변화하는 알고리즘을 제공하고 있습니다.&lt;/p&gt;

&lt;p&gt;위경도 정보가 포함된 다양한 데이터를 기상청의 격자와 맵핑하면 날씨 데이터를 이용한 다양한 분석을 수행할 수 있습니다.&lt;/p&gt;

&lt;p&gt;위경도 좌표를 기상청 격자로 변환하는 프로그램은 아래 오픈API의 활용가이드 문서 내에 공개되어 있습니다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;https://www.data.go.kr/dataset/15000099/openapi.do&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;C로 구현된 프로그램을 파이썬 버전으로 변경한 것은 아래와 같습니다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;math&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;NX&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;149&lt;/span&gt;            &lt;span class=&quot;c&quot;&gt;## X축 격자점 수&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;NY&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;253&lt;/span&gt;            &lt;span class=&quot;c&quot;&gt;## Y축 격자점 수&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;Re&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;6371.00877&lt;/span&gt;     &lt;span class=&quot;c&quot;&gt;##  지도반경&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;grid&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;5.0&lt;/span&gt;          &lt;span class=&quot;c&quot;&gt;##  격자간격 (km)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;slat1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;30.0&lt;/span&gt;        &lt;span class=&quot;c&quot;&gt;##  표준위도 1&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;slat2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;60.0&lt;/span&gt;        &lt;span class=&quot;c&quot;&gt;##  표준위도 2&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;olon&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;126.0&lt;/span&gt;        &lt;span class=&quot;c&quot;&gt;##  기준점 경도&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;olat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;38.0&lt;/span&gt;         &lt;span class=&quot;c&quot;&gt;##  기준점 위도&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;xo&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;210&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grid&lt;/span&gt;     &lt;span class=&quot;c&quot;&gt;##  기준점 X좌표&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;yo&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;675&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grid&lt;/span&gt;     &lt;span class=&quot;c&quot;&gt;##  기준점 Y좌표&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;first&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;first&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;PI&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;math&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;asin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;2.0&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;DEGRAD&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PI&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;180.0&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;RADDEG&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;180.0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PI&lt;/span&gt;


    &lt;span class=&quot;n&quot;&gt;re&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Re&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grid&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;slat1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;slat1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DEGRAD&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;slat2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;slat2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DEGRAD&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;olon&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;olon&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DEGRAD&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;olat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;olat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DEGRAD&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;sn&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;math&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tan&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;PI&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.25&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;slat2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;math&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tan&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;PI&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.25&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;slat1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;sn&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;math&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;math&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cos&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;slat1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;math&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cos&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;slat2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;math&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;sf&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;math&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tan&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;PI&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.25&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;slat1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;sf&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;math&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;pow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;math&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cos&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;slat1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sn&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ro&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;math&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tan&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;PI&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.25&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;olat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ro&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;re&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sf&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;math&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;pow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ro&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;first&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;mapToGrid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lon&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;code&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ra&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;math&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tan&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;PI&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.25&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DEGRAD&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ra&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;re&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sf&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;pow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ra&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lon&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DEGRAD&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;olon&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PI&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;2.0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PI&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;PI&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;2.0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PI&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sn&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ra&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;math&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xo&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ro&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ra&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;math&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cos&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;yo&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;gridToMap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;code&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;xn&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xo&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;yn&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ro&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;yo&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ra&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;math&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xn&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xn&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;yn&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;yn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sn&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;ra&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ra&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;alat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;math&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;pow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;re&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sf&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ra&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;alat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;2.0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;math&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;atan&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PI&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;math&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fabs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;math&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fabs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;yn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PI&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xn&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;math&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;atan2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;yn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;alon&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sn&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;olon&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;lat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;RADDEG&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;lon&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alon&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;RADDEG&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lon&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mapToGrid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;37.579871128849334&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;126.98935225645432&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mapToGrid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;35.101148844565955&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;129.02478725562108&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mapToGrid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;33.500946412305076&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;126.54663058817043&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;### result :&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#(60, 127)&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#(97, 74)&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#(53, 38)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gridToMap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;60&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;127&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gridToMap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;97&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;74&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gridToMap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;53&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;38&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;### result&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# 37.579871128849334, 126.98935225645432&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# 35.101148844565955, 129.02478725562108&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# 33.500946412305076, 126.54663058817043&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;위 알고리즘을 이용해 환경공단 제공의 초미세먼지 데이터를 시각화 예시 입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-12-15/fine-dust.png&quot; width=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;p&gt;미세먼지 측정소 리스트를 조회할수 있는 OPEN API를 이용하면 아래와 같은 형태로 397개의 측정소 위치를 얻을 수 있습니다.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;station&lt;/th&gt;
      &lt;th&gt;lat&lt;/th&gt;
      &lt;th&gt;lon&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;빛가람동&lt;/td&gt;
      &lt;td&gt;35.02174&lt;/td&gt;
      &lt;td&gt;126.790413&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;장성읍&lt;/td&gt;
      &lt;td&gt;35.303241&lt;/td&gt;
      &lt;td&gt;126.785419&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;…&lt;/td&gt;
      &lt;td&gt;….&lt;/td&gt;
      &lt;td&gt;…..&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;송파구&lt;/td&gt;
      &lt;td&gt;37.521597&lt;/td&gt;
      &lt;td&gt;127.124264&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;small&gt;&lt;i&gt;table : airkorea_stations&lt;/i&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;또한 대기오염 정보 조회 OPEN API를 이용하면 측정소별 실시간(1시간 단위) 대기오염 데이터를 얻을 수 있습니다.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;station&lt;/th&gt;
      &lt;th&gt;datatime&lt;/th&gt;
      &lt;th&gt;PM2.5&lt;/th&gt;
      &lt;th&gt;PM10&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;빛가람동&lt;/td&gt;
      &lt;td&gt;2018-12-13 14:00&lt;/td&gt;
      &lt;td&gt;24&lt;/td&gt;
      &lt;td&gt;28&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;장성읍&lt;/td&gt;
      &lt;td&gt;2018-12-13 14:00&lt;/td&gt;
      &lt;td&gt;37&lt;/td&gt;
      &lt;td&gt;31&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;…&lt;/td&gt;
      &lt;td&gt;….&lt;/td&gt;
      &lt;td&gt;…..&lt;/td&gt;
      &lt;td&gt;….&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;송파구&lt;/td&gt;
      &lt;td&gt;2018-12-13 14:00&lt;/td&gt;
      &lt;td&gt;45&lt;/td&gt;
      &lt;td&gt;35&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;small&gt;&lt;i&gt;table : airkorea_data&lt;/i&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;airkorea_stations에 있는 위경도를 기상청 격자 좌표로 변경하여 &lt;code class=&quot;highlighter-rouge&quot;&gt;gridx&lt;/code&gt; 와 &lt;code class=&quot;highlighter-rouge&quot;&gt;gridy&lt;/code&gt;로 저장합니다.&lt;/p&gt;

&lt;p&gt;이때, 주의할점은 격자 (1,1)에 대칭되는 점은 그리드의 좌하단이기때문에 실제 어레이의 포지션은 &lt;code class=&quot;highlighter-rouge&quot;&gt;grid_array[253+1-data.gridy, data.gridx]&lt;/code&gt;로 되어야합니다.&lt;/p&gt;

&lt;p&gt;시각화 예제 코드는 아래와 같습니다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;## read data&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;con&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sqlite3&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;connect&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;MyDataBase&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read_sql&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;select * from airkorea_data a join airkorea_stations b on a.station=b.station;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;con&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;gridx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gridy&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iterrows&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mapToGrid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lon&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;gridx&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gridy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;assign&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gridx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gridx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gridy&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gridy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;startdate&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2018&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;12&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;13&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;background&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imread&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'background.png'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;grid_array&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;empty&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;253&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;149&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iterrows&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;try&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;grid_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;253&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gridy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gridx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pm10Value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 
    &lt;span class=&quot;k&quot;&gt;except&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;pass&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figure&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figsize&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;15&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;masked_data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ma&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;masked_where&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grid_array&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grid_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imshow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;background&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imshow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;masked_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cmap&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'jet'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vmin&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vmax&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;colorbar&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;now&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;strftime&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;2018-12-13 14:00&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-12-15/background.png&quot; width=&quot;230&quot; /&gt; ►►
&lt;img src=&quot;/assets/img/2018-12-15/fine-dust.png&quot; width=&quot;300&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Good Bye ~ !&lt;/p&gt;</content><author><name>yjucho</name></author><summary type="html">기상청은 전국을 5km×5km 간격의 촘촘한 격자화하여 읍,면,동 단위로 상세한 날씨를 제공하는 동네예보를 제공합니다. 구역별 기상데이터를 관리하기 위해 한반도를 가로로 149개, 세로로 253개의 선을 그어 그리드형태로 관리하며, 위경도 데이터를 이 그리드 상의 좌표로 변화하는 알고리즘을 제공하고 있습니다.</summary></entry><entry><title type="html">django를 이용한 대시보드 만들기</title><link href="http://localhost:4000/django/django/" rel="alternate" type="text/html" title="django를 이용한 대시보드 만들기" /><published>2018-12-12T00:00:00+09:00</published><updated>2018-12-12T00:00:00+09:00</updated><id>http://localhost:4000/django/django</id><content type="html" xml:base="http://localhost:4000/django/django/">&lt;p&gt;django는 python 기반의 웹프레임워크로 비교적 쉽고 빠르게 웹어플리케이션을 제작할수 있도록 도와줍니다. django와 여러가지 오픈소스 라이브러리를 이용해 간단한 대시보드를 제작해보았습니다. 이 포스트에서는 1차 프로토타입을 소개하고, 사용한 라이브러리를 소개하도록 하겠습니다.&lt;/p&gt;

&lt;h3 id=&quot;missions&quot;&gt;Missions&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;데이터를 통한 인사이트 서비스를 제공하고 세상이 더 효율적으로 돌아가는데 기여하자&lt;br /&gt;
눈에 보이는 유형의 서비스로 만들자 &lt;br /&gt; 
빠르게 만들고, 피드백을 받아 수정하자&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-12-12/index.png&quot; width=&quot;500&quot; /&gt;
&lt;img src=&quot;/assets/img/2018-12-12/detail.png&quot; width=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Demo&lt;/b&gt; : &lt;a href=&quot;https://youtu.be/Xt-Yw83cv7E&quot;&gt;Youtue&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;b&gt;github repo.&lt;/b&gt; : &lt;a href=&quot;https://github.com/yjucho1/mysite&quot;&gt;github&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;related-tools-and-docs&quot;&gt;Related Tools and Docs&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;b&gt;django&lt;/b&gt; : &lt;a href=&quot;https://www.djangoproject.com/&quot;&gt;https://www.djangoproject.com/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Django is a high-level Python Web framework that encourages rapid development and clean, pragmatic design. 
Built by experienced developers, it takes care of much of the hassle of Web development, so you can focus on writing your app without needing to reinvent the wheel. 
It’s &lt;u&gt;free and open source&lt;/u&gt;.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;b&gt;bootstrap&lt;/b&gt; : &lt;a href=&quot;https://getbootstrap.com/&quot;&gt;https://getbootstrap.com/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Bootstrap is &lt;u&gt;an open source&lt;/u&gt; toolkit for developing with HTML, CSS, and JS. 
Quickly prototype your ideas or build your entire app with our Sass variables and mixins, responsive grid system, extensive prebuilt components, and powerful plugins built on jQuery.
Bootstrap is released under the MIT license and is copyright 2018 Twitter.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;b&gt;charts.js&lt;/b&gt; : &lt;a href=&quot;https://www.chartjs.org/&quot;&gt;https://www.chartjs.org/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Simple yet flexible JavaScript charting for designers &amp;amp; developers.
Chart.js is &lt;u&gt;open source&lt;/u&gt; and available under the MIT license.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;b&gt;leaflet.js&lt;/b&gt; : &lt;a href=&quot;https://leafletjs.com/&quot;&gt;https://leafletjs.com/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Leaflet is the leading &lt;u&gt;open-source&lt;/u&gt; JavaScript library for mobile-friendly interactive maps. Weighing just about 38 KB of JS, it has all the mapping features most developers ever need. As the code is published under the very permissive 2-clause BSD License. &lt;u&gt;Just make sure to attribute the use of the library somewhere in the app UI or the distribution&lt;/u&gt; (e.g. keep the Leaflet link on the map, or mention the use on the About page or a Readme file, etc.) and you’ll be fine.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;b&gt;git&lt;/b&gt; : &lt;a href=&quot;https://git-scm.com/&quot;&gt;https://git-scm.com/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Git is &lt;u&gt;a free and open source&lt;/u&gt; distributed version control system designed to handle everything from small to very large projects with speed and efficiency. The Git project chose to use GPLv2 to guarantee your freedom to share and change free software—to make sure the software is free for all its users.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;b&gt;공공데이터 포털&lt;/b&gt; : &lt;a href=&quot;https://www.data.go.kr/&quot;&gt;https://www.data.go.kr/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;공공데이터포털은 공공기관이 생성 또는 취득하여 관리하고 있는 공공데이터를 한 곳에서 제공하는 통합 창구입니다. 
포털에서는 국민이 쉽고 편리하게 공공데이터를 이용할 수 있도록 파일데이터, 오픈API, 시각화 등 다양한 방식으로 제공하고 있으며, 누구라도 쉽고 편리한 검색을 통해 원하는 공공데이터를 빠르고 정확하게 찾을 수 있습니다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;한국환경공단_측정소정보 조회 서비스 : https://www.data.go.kr/dataset/15000660/openapi.do&lt;/li&gt;
  &lt;li&gt;한국환경공단_대기오염정보 조회 서비스 : https://www.data.go.kr/dataset/15000581/openapi.do&lt;/li&gt;
&lt;/ul&gt;</content><author><name>yjucho</name></author><summary type="html">django는 python 기반의 웹프레임워크로 비교적 쉽고 빠르게 웹어플리케이션을 제작할수 있도록 도와줍니다. django와 여러가지 오픈소스 라이브러리를 이용해 간단한 대시보드를 제작해보았습니다. 이 포스트에서는 1차 프로토타입을 소개하고, 사용한 라이브러리를 소개하도록 하겠습니다.</summary></entry><entry><title type="html">Clustering and Unsupervised Anomaly Detection with l2 Normalized Deep Auto-Encoder Representations</title><link href="http://localhost:4000/clustering/clustering-with-l2-norm/" rel="alternate" type="text/html" title="Clustering and Unsupervised Anomaly Detection with l2 Normalized Deep Auto-Encoder Representations" /><published>2018-11-22T00:00:00+09:00</published><updated>2018-11-22T00:00:00+09:00</updated><id>http://localhost:4000/clustering/clustering-with-l2-norm</id><content type="html" xml:base="http://localhost:4000/clustering/clustering-with-l2-norm/">&lt;p&gt;&lt;b&gt; Caglar Aytekin, Xingyang Ni, Francesco Cricri and Emre Aksu (Nokia) 2017 &lt;/b&gt;&lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Recently, there are many works on learning deep unsupervised representations for clustering analysis.&lt;/li&gt;
  &lt;li&gt;Works rely on variants of auto-encoders and use encoder outputs as representation/features for cluster.&lt;/li&gt;
  &lt;li&gt;In this paper, l&lt;sub&gt;2&lt;/sub&gt; normalization constraint during auto-encoder training makes the representations more separable and compact in the Euclidean space.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;related-work&quot;&gt;Related Work&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;DEC : First, dense auto-encoder is trained with minimizing reconstruction error. Then, as clustering optimization state, minimizing the KL divergence between auto-encoder representation and an auxiliary target distribution.
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://github.com/yjucho1/articles/blob/master/DEC/readme.md&quot;&gt;DEC paper&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;IDEC : proposes to jointly optimize the clustering loss and reconstruction loss of the auto-encoder&lt;/li&gt;
  &lt;li&gt;DCEC : adopts a convolutional auto-encoder&lt;/li&gt;
  &lt;li&gt;GMVAE : adopts variational auto-encoder&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;proposed-method&quot;&gt;Proposed Method&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Clustering on l&lt;sub&gt;2&lt;/sub&gt; normalized deep auto-encoder representations&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L = \frac{1}{|J|} \sum_{j \in J} (I_j - D(E_c(I_j)))^2, \\
E_c(I) = \frac{E(I)}{\parallel E(I) \parallel _2}&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;after training auto-encoder with loss function, the clustering is simply performed by k-means algorithm.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Unsupervised Anomaly Detection using l&lt;sub&gt;2&lt;/sub&gt; normalized deep auto-encoder representations&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;v_i = max_j (E_c(I_i) \cdot \frac{C_j}{\parallel C_j \parallel _2} )&lt;/script&gt;

&lt;h2 id=&quot;experimental-result&quot;&gt;Experimental result&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;clustering : evaluation metrics - accuracy&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-11-22/dense-AE.png&quot; width=&quot;600&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-11-22/conv-AE.png&quot; width=&quot;600&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-11-22/comparison-norm.png&quot; width=&quot;600&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;comparision of normalization method : neither batch nor layer normalization provides a noticeable accuracy increase over CAE + k-means. Moreover in MNIST dataset, layer and batch normalization results into a significant accuracy decrease.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This is an important indicator showing that the performance upgrade of our method is not a result of a input conditioning, but it is a result of the specific normalization type that is more fit for clustering in Euclidean space.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;anomaly detection : evaluation metrics - AUC&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-11-22/anomaly-detection.png&quot; width=&quot;600&quot; /&gt;&lt;/p&gt;</content><author><name>yjucho</name></author><summary type="html">Caglar Aytekin, Xingyang Ni, Francesco Cricri and Emre Aksu (Nokia) 2017</summary></entry><entry><title type="html">Attention is All You Need</title><link href="http://localhost:4000/attention/attention-is-all-you-need/" rel="alternate" type="text/html" title="Attention is All You Need" /><published>2018-10-30T00:00:00+09:00</published><updated>2018-10-30T00:00:00+09:00</updated><id>http://localhost:4000/attention/attention-is-all-you-need</id><content type="html" xml:base="http://localhost:4000/attention/attention-is-all-you-need/">&lt;p&gt;&lt;b&gt; Ashish Vaswani et al. (Google Brain), 2017 &lt;/b&gt;&lt;/p&gt;

&lt;p&gt;Tensorflow implementtation :&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py&quot;&gt;https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/Kyubyong/transformer&quot;&gt;https://github.com/Kyubyong/transformer&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;PyTorch implementation : 
&lt;a href=&quot;https://github.com/jadore801120/attention-is-all-you-need-pytorch&quot;&gt;https://github.com/jadore801120/attention-is-all-you-need-pytorch&lt;/a&gt;
[guide annotating the paper with PyTorch implementation]
(http://nlp.seas.harvard.edu/2018/04/03/attention.html)&lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;언어 모델링과 기계 번역과 같은 시퀀스 모델링에서 RNN, LSTM, GRU는 최신 기법으로 확고하게 자리잡고 있습니다. 인코더-디코더 구조를 활용하는 등 향상된 성능을 얻기 위해 많은 시도들이 있어왔습니다.&lt;/p&gt;

&lt;p&gt;recurrent models에서 hidden state &lt;script type=&quot;math/tex&quot;&gt;h_t&lt;/script&gt;는 previus hidden state &lt;script type=&quot;math/tex&quot;&gt;h_{t-1}&lt;/script&gt;과 &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt;번째 입력값의 함수로 계선됩니다. 이러한 순차적 계산방식은 병렬처리가 어렵습니다. 입력 시퀀스가 길어질수록 이러한 제약 사항은 중요한 이슈가 됩니다.&lt;/p&gt;

&lt;p&gt;어텐션 메커니즘은 시퀀스 모델링에서 인풋과 아웃풋 시퀀스간 거리에 상관없이 의존성을 모델링할수 있는 방법으로 사용되었습니다. 하지만 기존 연구들은 recurrent netowork의 보완역할로만 어텐션 메커니즘을 사용하여 여전히 순차적 계산방식에 대한 제약이 남아있습니다.&lt;/p&gt;

&lt;p&gt;이 연구에서는 &lt;code class=&quot;highlighter-rouge&quot;&gt;&quot;Transformer&quot;&lt;/code&gt;라는 새로운 구조를 제안합니다. recurrence 구조를 탈피하고, 인풋과 아웃풋간의 글로벌 의존성을 모델링하는 어텐션 메커니즘만을 사용합니다. 이로 인해 병렬처리가 가능하고, P100 GPU 8장으로 12시간동안 학습시키는 것만으로도 state of art 수준의 번역 품질을 달성할수 있었습니다.&lt;/p&gt;

&lt;h2 id=&quot;backgroud&quot;&gt;Backgroud&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1610.08613&quot;&gt;Extended Neural GPU&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/1610.10099&quot;&gt;ByteNet&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/1705.03122&quot;&gt;ConvS2S&lt;/a&gt; 등은 컨볼루션 뉴럴 네트워크를 사용하여 병렬처리가 가능하도록 시퀀스 모델을 제안하였습니다. 하지만 인풋과 아웃풋 포지션의 거리가 멀어질수록 계산량도 이에 따라 증가하고, 멀리 떨어진 포지션 사이의 의존성을 학습하기가 더 어려워집니다.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;self-attention&lt;/code&gt;, 혹은 &lt;code class=&quot;highlighter-rouge&quot;&gt;intra-attention&lt;/code&gt;은 포지션을 고려하여 시퀀스의 representation을 계산하며, 이는 독해나 요약 등에서 다양하게 사용되고 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1503.08895&quot;&gt;End-to-end memory network&lt;/a&gt;는 시퀀스를 순차적으로 다루는 것이 아니라, 어텐션을 순차적으로 다루는 메커니즘으로 간단한 문답과 같은 언어 모델링에서 잘 작동하는 것으로 알려져 있습니다.&lt;/p&gt;

&lt;p&gt;(우리가 아는 한도에서는) Transformer는 RNN이나 CNN을 사용하지 않으면서도 인풋과 아웃풋의 표현력을 셀프-어텐션만으로 계산하는 최초의 접근법입니다.&lt;/p&gt;

&lt;h2 id=&quot;model-architecture&quot;&gt;Model Architecture&lt;/h2&gt;

&lt;p&gt;대부분의 시퀀스 변환 모델은 인코더-디코더 구조를 사용합니다. 인코더는 인풋 시퀀스 &lt;script type=&quot;math/tex&quot;&gt;(x_1, \cdots, x_n)&lt;/script&gt;를 &lt;script type=&quot;math/tex&quot;&gt;z=(z_1, \cdots, z_n)&lt;/script&gt;로 맵핑합니다. z가 주어지면, 디코더는 아웃풋 시퀀스 &lt;script type=&quot;math/tex&quot;&gt;(y_1 \cdots, y_m)&lt;/script&gt;를 한번에 하나씩 생성합니다. 매스텝마다 모델은 auto-regressive하게 다음단어를 생성할때마다 입력값과 이전에 생성된 심볼을 사용합니다.&lt;/p&gt;

&lt;p&gt;Transformer는 여러개의 셀프-어텐션을 쌓고, point-wise하게 계산하며, fully connected layer를 사용합니다.&lt;/p&gt;

&lt;h3 id=&quot;encoder-and-decoder-stacks&quot;&gt;Encoder and Decoder Stacks&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-10-30/fig1.png&quot; width=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small&gt;&lt;em&gt;그림1. The Transformer - model architecture&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Encoder&lt;/b&gt; : 인코더는 6개의 동일한 레이어로 구성됩니다. 각 레이어는 2개의 서브레이어를 갖습니다. 첫번째는 &lt;code class=&quot;highlighter-rouge&quot;&gt;multi-head self-attention mechanism&lt;/code&gt;, 두번째는 간단한 &lt;code class=&quot;highlighter-rouge&quot;&gt;position-wise fully connected feed-forward network&lt;/code&gt;입니다. 두 서브레이어 사이에는 &lt;code class=&quot;highlighter-rouge&quot;&gt;residual connection&lt;/code&gt;과 &lt;code class=&quot;highlighter-rouge&quot;&gt;layer normalization&lt;/code&gt;을 사용하였습니다. 즉 서브레이어의 아웃풋은 &lt;script type=&quot;math/tex&quot;&gt;LayerNorm(x+Sublayer(x))&lt;/script&gt;입니다. residual connection이 가능하도록 모델의 모든 레이어의 아웃풋 디멘전은 &lt;script type=&quot;math/tex&quot;&gt;d_{model}=512&lt;/script&gt;로 설정하였습니다.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Decoder&lt;/b&gt; : 디코더도 6개의 동일한 레이어로 구성됩니다. 마찬가지로 각 레이어는 위에서 언급한 2개의 서브레이어를 갖지만, 추가로 한가지가 더 있습니다. multi-head attention을 수정하여 생성하고자 하는 &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;번째 시퀀스가 후속 시퀀스에는 영향을 받지 않고, &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;보다 작은 위치의 아웃풋에만 의존할수 있도록 하였습니다(&lt;code class=&quot;highlighter-rouge&quot;&gt;Masked Multi-Head Attention&lt;/code&gt;).&lt;/p&gt;

&lt;h3 id=&quot;attention&quot;&gt;Attention&lt;/h3&gt;

&lt;p&gt;어텐션 함수는 Query, Key-Value 쌍을 아웃풋에 맵핑시키는 것입니다. query, key, valut, output은 모두 벡터입니다. 아웃풋은 value의 가중합이고, 이 때 가중치는 query와 값에 대응되는 key로 계산됩니다.&lt;/p&gt;

&lt;h4 id=&quot;scaled-dot-product-attention&quot;&gt;Scaled Dot-Product Attention&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-10-30/fig2.png&quot; width=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small&gt;&lt;em&gt;그림2. (왼쪽) Scaled Dot-Product Attention (오른쪽) Multi-head Attention은 병렬로 계산되는 여러개의 어텐션 레이어로 이뤄집니다.&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;여기서 사용한 어텐션 방식을 &lt;code class=&quot;highlighter-rouge&quot;&gt;&quot;Scaled Dot-Product Attention&quot;&lt;/code&gt;이라고 하겠습니다. 인풋은 &lt;script type=&quot;math/tex&quot;&gt;d_k&lt;/script&gt;차원의 query와 key, &lt;script type=&quot;math/tex&quot;&gt;d_v&lt;/script&gt;차원의 value입니다. query와 key를 dot-products한 후 &lt;script type=&quot;math/tex&quot;&gt;\sqrt{d_k}&lt;/script&gt;로 나누고, 소프트맥스함수를 취해 가중치를 구합니다.&lt;/p&gt;

&lt;p&gt;실제로는 여러개의 query 집합을 메트릭스 Q로 묶어 계산합니다. key와 value도 각 각 메트릭스 K와 V로 나타내어 메트릭스 형태의 아웃풋을 얻습니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Attention(Q, K, V) = softmax({QK^T\over{\sqrt{d_k}}})V&lt;/script&gt;

&lt;p&gt;널리 알려진 어텐션 함수는 &lt;code class=&quot;highlighter-rouge&quot;&gt;additive attention&lt;/code&gt;과 &lt;code class=&quot;highlighter-rouge&quot;&gt;dot-product(multiplicative) attention&lt;/code&gt;입니다. 여기서 사용한 알고리즘은 dot-product attention과 동일하지만, 스케일링을 위해 &lt;script type=&quot;math/tex&quot;&gt;1\over{\sqrt{d_k}}&lt;/script&gt;로 나눠주는 것을 추가하었습니다. additive attention은 싱글 히든 레이어로 이루어진 피드-포워드 네트워크를 사용하는 방식입니다. 두가지 모두 이론적인 계산복잡도는 유사하지만, dot-product attention이 최적화된 메트릭스 멀티플리케이션 코드으로 구현할수 있기때문에 더 빠르고 효율적입니다.&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;d_k&lt;/script&gt;가 작을 때는 두 메커니즘은 유사한 성능을 보이지만, &lt;script type=&quot;math/tex&quot;&gt;d_k&lt;/script&gt;가 클때는 additive attention이 스케일링이 없는 dot-product attention보다 더 우수한 성능을 보입니다. 우리는 &lt;script type=&quot;math/tex&quot;&gt;d_k&lt;/script&gt;가 클 때는 dot-product의 유효구간이 커지고, 이는 소프트맥스함수에서 그래디어트가 아주 작은 영역으로 가까워지게 하기 때문이라고 생각합니다.&lt;sup&gt;(*)&lt;/sup&gt; 이러한 효과를 줄이기 위해서 &lt;script type=&quot;math/tex&quot;&gt;1\over{\sqrt{d_k}}&lt;/script&gt;로 나눠주었습니다.&lt;/p&gt;

&lt;p&gt;&lt;small&gt;(*) dot-products의 유효구간이 커진다는 것을 설명하기 위해, q와 k가 평균 0이고 분산이 1인 독립적인 변수를 생각보겠습니다. q와 k의 dot-product는 &lt;script type=&quot;math/tex&quot;&gt;q \cdot k = \sum_{i=1}^{d_k}q_ik_i&lt;/script&gt;이고, 이는 평균이 0이고 분산은 &lt;script type=&quot;math/tex&quot;&gt;d_k&lt;/script&gt; 가 됩니다. &lt;/small&gt;&lt;/p&gt;

&lt;h4 id=&quot;multi-head-attention&quot;&gt;Multi-Head Attention&lt;/h4&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;d_{model}&lt;/script&gt; 차원의 keys, values, queries로 싱글 어텐션을 학습할수 있지만, 우리는 선형 프로젝션을 통해 h개의 어텐션을 이용하는 것이 더 효과적이라는 것을 발견하였습니다. 각 각의 프로젝션을 병렬로 계산하여, &lt;script type=&quot;math/tex&quot;&gt;d_v&lt;/script&gt;-차원의 아웃풋값을 얻고, h개의 아웃풋 값들은 concatenate한 후 다시 프로젝션하여 최종 값을 계산합니다.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;multi-head attention&lt;/code&gt;은 서로 다른 represenation subspace에서의 정보를 결합하여 사용하는 것입니다. 싱글 어텐션은 평균값으로 인해 이러한 정보들이 없어져버립니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;MultiHead(Q, K, V)=Concat(head_1, \cdots, head_h)W^O \\
where \ head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)&lt;/script&gt;

&lt;p&gt;프로젝션을 위한 파라미터는 &lt;script type=&quot;math/tex&quot;&gt;W_i^Q \in \mathbb{R}^{d_{model}\times d_k}&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;W_i^K \in \mathbb{R}^{d_{model}\times d_k}&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;W_i^V \in \mathbb{R}^{d_{model}\times d_v}&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;W_i^O \in \mathbb{R}^{hd_v\times d_{model}}&lt;/script&gt;입니다.&lt;/p&gt;

&lt;p&gt;실험에서 &lt;script type=&quot;math/tex&quot;&gt;h = 8&lt;/script&gt;의 병렬 어텐션 레이어를 사용하였습니다. 또한 &lt;script type=&quot;math/tex&quot;&gt;d_k = d_v =d_{model}/h = 64&lt;/script&gt; 사용하였습니다. 각 헤드의 차원이 줄어들었지만, 전체적인 계산비용은 full dimensionality와 유사합니다.&lt;/p&gt;

&lt;h4 id=&quot;application-of-attention-in-our-model&quot;&gt;Application of Attention in our Model&lt;/h4&gt;

&lt;p&gt;Transformer는 multi-head attention을 3가지 방식으로 사용합니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;인코더-디코더 어텐션 레이어에서 queries는 이전 디코더 레이어로부터 입력되고, keys-valus는 인코더 아웃풋으로부터 입력됩니다. 따라서 디코더가 시퀀스의 매 포지션을 생성할때마다, 인풋 시퀀스의 모든 포지션 정보를 이용할수 있습니다. 이는 sequence-to-sequence모델에서 인코더-디코더 어텐션 메커니즘을 그대로 차용한 것입니다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;인코더는 셀프-어텐션 레이어를 포함합니다. 셀프 어텐션 레이어의 keys, values, queries는 이전 레이어의 아웃풋입니다. 레이어의 포지션 정보는 이전 레이어가 생성한 모든 포지션 정보를 다 이용합니다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;유사하게 디코더도 셀프-어텐션 레이어를 포함하며, 디코더가 시퀀스의 매 포지션를 생성할때마다 그 위치까지의 모든 디코더 정보를 이용할수 있습니다. 다만 auto-regressive 속성을 유지하기 위해 소프트맥스의 인풋값 중 후속 포지션에 해당하는 값들은 모두 1로 마스킹합니다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;position-wise-feed-forward-networks&quot;&gt;Position-wise Feed-Forward Networks&lt;/h3&gt;

&lt;p&gt;어텐션 서브-레이어 이외에 인코더와 디코더 모두 &lt;code class=&quot;highlighter-rouge&quot;&gt;fully connected feed-forward network&lt;/code&gt;를 서브레이어로 갖고 있습니다. 각 포지션별로 동일하게 적용되는 네트워크로 ReLU활성함수과 선형변환으로 구성됩니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;FFN(x) = max(0, xW_1 + b_1)W_2 +b_2&lt;/script&gt;

&lt;p&gt;선형변화는 다른 포지션이더라도 동일한 선형 변환을 하지만, 레이어간에는 서로 다른 파라미터를 사용합니다. 이는 커널 사이즈가 1인 두개의 컨볼루션 오퍼레이션이라고도 생각할수 있습니다. 인풋과 아웃풋의 차원은 &lt;script type=&quot;math/tex&quot;&gt;d_{model}=512&lt;/script&gt;이고, 레이어 내부(W)는 &lt;script type=&quot;math/tex&quot;&gt;d_{ff} = 2048&lt;/script&gt;입니다.&lt;/p&gt;

&lt;h3 id=&quot;embedding-and-softmax&quot;&gt;Embedding and Softmax&lt;/h3&gt;

&lt;p&gt;다른 시퀀스 변환 모델과 유사하게, 인풋과 아웃풋 토큰을 &lt;script type=&quot;math/tex&quot;&gt;d_{model}&lt;/script&gt;차원의 벡터로 변환하는 임베딩을 학습합니다. 또한 디코더 아웃풋을 아웃풋 토큰 예측확률값으로 변환하기 위해 선형변환와 소프트맥스 함수를 사용하였습니다. 여기서는 두 임베딩 레이어와 소프트맥스앞의 선형변환에 대해서 모두 동일한 가중치 메트릭스를 사용하였습니다. 임베딩레이어에서는 가중치에 &lt;script type=&quot;math/tex&quot;&gt;\sqrt{d_{model}}&lt;/script&gt;를 곱하여 사용하였습니다.&lt;/p&gt;

&lt;h3 id=&quot;positional-encoding&quot;&gt;Positional Encoding&lt;/h3&gt;

&lt;p&gt;Transformer는 recurrence나 convolution을 사용하지 않기때문에, 시퀀스의 순서(order) 정보를 사용하기 위해서 시퀀스에서 토큰의 절대적인 위치나 상대적인 위치 정보를 강제로 입력해주어야 합니다. 따라서 포지셔널 인코딩을 인코더와 디코더의 가장 아래에 있는 인풋 임베딩 레이어에 추가하였습니다. 포지셔널 인코딩은 &lt;script type=&quot;math/tex&quot;&gt;d_{model}&lt;/script&gt;과 동일한 차원으로 임베딩 벡터와 sum할수 있도록 하였습니다. 여러 종류의 포지셔널 인코딩이 있습니다만, 여기서는 서로 다른 주기의 &lt;code class=&quot;highlighter-rouge&quot;&gt;sine&lt;/code&gt;과 &lt;code class=&quot;highlighter-rouge&quot;&gt;cosine&lt;/code&gt;함수를 사용하였습니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;PE_{(pos, 2i)} = sin(pos/10000^{2i/d_{model}}) \\
PE_{(pos, 2i+1)} = cos(pos/10000^{2i/d_{model}})&lt;/script&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;pos&lt;/script&gt;는 포지션이고, &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;는 디멘전입니다. 즉 포지셔널 인코딩의 각 차원은 sinusoid(사인모양의 파동)와 대응됩니다. 파장은 &lt;script type=&quot;math/tex&quot;&gt;2i&lt;/script&gt;에서 &lt;script type=&quot;math/tex&quot;&gt;10000\cdot 2i&lt;/script&gt;까지의 기하학적 진행을 의미합니다. 이 함수를 선택한 이유는 고정된 오프셋 &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt;에 대해서 &lt;script type=&quot;math/tex&quot;&gt;PE_{pos+k}&lt;/script&gt;는 &lt;script type=&quot;math/tex&quot;&gt;PE_{pos}&lt;/script&gt;의 선형변환으로 쉽게 표현될수 있기때문에 모델이 상대적인 포지션 정보를 쉽게 학습할수 있을것이라 가정했기 때문입니다.&lt;/p&gt;

&lt;p&gt;sinusoid 형태 외에 포지셔널 인코딩을 별도의 네트워크로 두고 학습하는 형태를 비교 실험해보았습니다. 그 결과, 두 버전 모두 거의 유사한 성능을 보였습니다(테이블3의 (E)). 최종적으로는 학습과정에서 마주친 것보다 더 긴 시퀀스에 대해서 extrapolate(외삽)할 수 있다는 점 때문에 sinusoid버전을 선택하였습니다.&lt;/p&gt;

&lt;h2 id=&quot;why-self-attention&quot;&gt;Why Self-Attention&lt;/h2&gt;

&lt;p&gt;이 섹션에서는 self-attention layer를 recurrent나 convolutional layer와 비교하여 설명하겠습니다. 모두 &lt;script type=&quot;math/tex&quot;&gt;(x_1, \cdots, x_n)&lt;/script&gt;의 시퀀스를 다른 시퀀스인 &lt;script type=&quot;math/tex&quot;&gt;(z_1, \cdots, z_n) \ with \ x_i, z_i \in \mathbb{R^d}&lt;/script&gt;로 맵핑하는데 일반적으로 사용되는 레이어들입니다. 이 논문에서 셀프-어텐션을 사용한 이유는 세가지 관점 때문입니다.&lt;/p&gt;

&lt;p&gt;첫번째는 레이어 당 계산 복잡도를 고려했기 때문이고, 두번째는 병렬계산할수 있는 총 계산량으로 필요한 순차적 오퍼레이션의 최소 수를 이용해 정량화하였습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-10-30/table1.png&quot; width=&quot;600&quot; /&gt;&lt;/p&gt;

&lt;p&gt;세번째는 네트워크에서 long-range dependencies간의 거리입니다. 멀리떨어진 단어들간의 의존성을 학습하는 것은 시퀀스 변환 문제에서 아주 중요한 이슈입니다. 이 이슈는 포워드와 백워드 시그널들이 네트워크에서 얼마나 이동할수 있는지에 영향을 받습니다. 인풋과 아웃풋 시퀀스들의 포지션 결합이 짧으면 짧을수록 장기 의존성은 쉽게 학습할수 있습니다. 따라서 우리는 네트워크 상에서 인풋과 아웃풋 포지션간의 최대 경로 길이를 측정하여 서로 다른 구조의 레이어를 비교 평가하였습니다.&lt;/p&gt;

&lt;p&gt;테이블1에서 볼수 있듯이 셀프 어텐션 레이어는 고정된 횟수만큼의 순차적인 오퍼레이션를 통해서 모든 포지션을 연결할수 있지만, recurrent layer는 &lt;script type=&quot;math/tex&quot;&gt;O(n)&lt;/script&gt;만큼의 순차적 오퍼레이션이 필요합니다. 계산복잡도 측면에서 시퀀스의 길이 &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt;이 representation 차원인 &lt;script type=&quot;math/tex&quot;&gt;d&lt;/script&gt;보다 작을 때 (word-piece, byte-pair와 같은 일반적인 기계번역에서 SOTA모델이 사용하는 방식) 셀프어텐션 레이어가 recurrent layer보다 더 빠릅니다. 아주 긴 시퀀스를 다룰 때 계산복잡도를 개선하기 위해 셀프어텐션은 아웃풋 포지션 주위로 r개의 인접 포지션만으로 제약하여 고려하도록 할수 있습니다. (in future work)&lt;/p&gt;

&lt;p&gt;커널 사이즈 &lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
k &lt; n %]]&gt;&lt;/script&gt;인 단일 컨볼루션 레이어는 인풋과 아웃풋 포지션의 모든 쌍을 연결하지 못합니다. 인풋과 아웃풋의 모든 포지션을 연결하기 위해서는 contiguous kernels을 이용하여 &lt;script type=&quot;math/tex&quot;&gt;O(n/k)&lt;/script&gt;만큼의 콘볼루션 레이어를 쌓거나, dilated convlutions을 이용하여 &lt;script type=&quot;math/tex&quot;&gt;O(log_k(n))&lt;/script&gt;를 쌓아야합니다. 이는 위에서 말한 경로 길이를 더 늘리는 것입니다. 컨볼루션 레이어는 일반적으로 recurrent layer보다 계산 비용이 더 큽니다. seperable convolution은 &lt;script type=&quot;math/tex&quot;&gt;O(k \cdot n \cdot d + n \cdot d^2)&lt;/script&gt;만큼 복잡도를 줄일수 있습니다. &lt;script type=&quot;math/tex&quot;&gt;k=n&lt;/script&gt;일때 seperable convolution는 이 논문에서 제안한 셀프-어텐션과 point-wise feed-forward layer를 결합한 것과 동일한 계산복잡도를 갖습니다.&lt;/p&gt;

&lt;p&gt;추가적으로 셀프-어텐션은 조금더 해석가능한 모델을 학습합니다. 어텐션 분포를 살펴보고 토의한 예제가 어펜딕스에 있습니다. 각각의 head가 명확하게 다른 작업을 수행하는 것을 학습할뿐만 아니라, 여러 head가 문장의 구조 및 의미 구조와 관련된 행동을 하는 것을 확인하였습니다.&lt;/p&gt;

&lt;h2 id=&quot;training&quot;&gt;Training&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;WMT 2014 English-German dataset and WMT 2014 English-French dataset&lt;/li&gt;
  &lt;li&gt;8 NVIDIA P100 GPUs&lt;/li&gt;
  &lt;li&gt;100,000 steps or 12 hours (each traini step took about 0.4 seconds)
    &lt;ul&gt;
      &lt;li&gt;for big model(bottom line of table 3), step time : 1.0 sec, total 300,000 steps(3.5 days)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Adam optimizer, &lt;script type=&quot;math/tex&quot;&gt;\beta_1&lt;/script&gt; = 0.9, &lt;script type=&quot;math/tex&quot;&gt;\beta_2&lt;/script&gt; = 0.98, &lt;script type=&quot;math/tex&quot;&gt;\epsilon_1&lt;/script&gt; = &lt;script type=&quot;math/tex&quot;&gt;10^{-9}&lt;/script&gt;
    &lt;ul&gt;
      &lt;li&gt;learning rate : increasing linearly for the first warmup_steps, decreasing it thereafter proportionally the the inverse square root fo the step number 
  &lt;script type=&quot;math/tex&quot;&gt;warmup\_steps&lt;/script&gt; = 4000
  &lt;script type=&quot;math/tex&quot;&gt;lrate = d_{model}^{-0.5} \cdot min(step\_num^{-0.5}, step\_num \cdot warmup\_steps^{-1.5})&lt;/script&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;three types of regularization
    &lt;ul&gt;
      &lt;li&gt;residual dropout : dropout to the output of each sub-layer, dropout to the sums of the embeddings and the positional encodings, &lt;script type=&quot;math/tex&quot;&gt;P_{drop} = 0.1&lt;/script&gt;&lt;/li&gt;
      &lt;li&gt;Label Smoothing : label smoothing of value &lt;script type=&quot;math/tex&quot;&gt;\epsilon_{ls} = 0.1&lt;/script&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-10-30/table2.png&quot; width=&quot;600&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;WMT 2014 English-to-German : new state of the art BLEU 28.4&lt;/li&gt;
  &lt;li&gt;WMT 2014 English-to-French : outperforming all previous single models with 1/4 traing cost of previous model&lt;/li&gt;
  &lt;li&gt;single model obtained by averaging the last 5 checkpoints(for big modle, 20 checkpoints)&lt;/li&gt;
  &lt;li&gt;beam searching with a beam size of 4 and length penalty &lt;script type=&quot;math/tex&quot;&gt;\alpha = 0.6&lt;/script&gt;
    &lt;ul&gt;
      &lt;li&gt;beam searching? &lt;a href=&quot;https://ratsgo.github.io/deep%20learning/2017/06/26/beamsearch/&quot;&gt;뭐냐&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;set the maximum output length during inferece to input length + 50&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-10-30/table3.png&quot; width=&quot;600&quot; /&gt;&lt;/p&gt;

&lt;p&gt;표3은 Transformer의 여러 요소들의 중요도를 평가하기 위해서 베이스 모델을 변형하면서 실험데이터(English-to-German translation on the development set, newstest2013)에 대한 성능 변화를 확인한 내용입니다.&lt;/p&gt;

&lt;p&gt;(A) : 멀티헤드의 개수 - 싱글-헤드 어텐션은 최적모델 대비 0.9만큼 낮은 BLEU를 보였습니다. 너무 많은 헤드일때도 성능이 떨어집니다. &lt;br /&gt;
(B) : key의 차원(&lt;script type=&quot;math/tex&quot;&gt;d_k&lt;/script&gt;)을 줄이면 성능이 떨어집니다. compatibility를 결정하는 것은 쉽지 않고, dot-product보다 더 정교한 함수가 유용할지 모른다는 것을 의미합니다. &lt;br /&gt;
(C) &amp;amp; (D) : bigger models are better. dropout is very helpful in avoiding over-fitting &lt;br /&gt;
(E) : sinusoidal positional encoding 대신에 learned positional embedding으로 변경한 결과, 거의 유사한 결과를 얻었습니다. (0.1 낮음)&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;conclusion&lt;/h2&gt;
&lt;p&gt;이 논문은 어텐션만 사용하여 시퀀스 모델을 학습한 최초의 접근인 Transformer를 제안하였습니다. 향후에는 텍스트 이외에 이미지, 오디오, 비디오와 같은 인풋과 아웃풋 문제로 확장하고, 로컬로 제약된 어텐션 메커니즘을 연결할 계획입니다. 또한 덜 순차적인 방식으로 일반화시키는 것이 또 다른 목표입니다.&lt;/p&gt;</content><author><name>yjucho</name></author><summary type="html">Ashish Vaswani et al. (Google Brain), 2017</summary></entry><entry><title type="html">recommender systems 2</title><link href="http://localhost:4000/recommender%20systems/recommendation2/" rel="alternate" type="text/html" title="recommender systems 2" /><published>2018-10-28T00:00:00+09:00</published><updated>2018-10-28T00:00:00+09:00</updated><id>http://localhost:4000/recommender%20systems/recommendation2</id><content type="html" xml:base="http://localhost:4000/recommender%20systems/recommendation2/">&lt;p&gt;추천시스템에 대해서 알아보자! - 지난 1편에서는 앤드류 응의 강의를 통해서 추천시스템의 전반적인 내용에 대해 알아보았습니다. 이번에는 Collaboratvie Filtering에 대해서 더 자세히 알아보고자 합니다.&lt;/p&gt;

&lt;p&gt;Collaborative filtering을 이용해 상품을 추천하는 방법은 크게 2가지 접근 방식이 있습니다. &lt;code class=&quot;highlighter-rouge&quot;&gt;neighborhood method&lt;/code&gt;와 &lt;code class=&quot;highlighter-rouge&quot;&gt;latent factor models&lt;/code&gt; 입니다.&lt;/p&gt;

&lt;h2 id=&quot;neighborhood-method&quot;&gt;Neighborhood method&lt;/h2&gt;
&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;neighborhood method&lt;/code&gt;는 아이템간 혹은 유저간 관계를 계산하는 것에 중점을 둡니다.&lt;/p&gt;

&lt;p&gt;유저 기반의 방법은 해당 유저와 유사한 다른 유저를 찾은 후, 비슷한 유저가 좋아하는 아이템을 추천하는 방식입니다. 그림1에서처럼, 세가지 영화를 좋아하는 Joe를 위해서, 세가지 영화를 동일하게 좋아하는 비슷한 유저를 찾습니다. 이들이 좋아하는 영화 중에서 가장 인기있는 영화인 Saving Private Ryan(라이언 일병 구하기, denoted #1)를 Joe에게 추천할 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-10-28/user-based-CF.png&quot; width=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small&gt;&lt;em&gt;그림1. user-oriented neighborhood method (Image source: Fig 1 in &lt;a href=&quot;https://datajobs.com/data-science-repo/Recommender-Systems-[Netflix].pdf&quot;&gt;Yehuda Koren et al., 2009&lt;/a&gt;)&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;아이템 기반의 방법은 해당 유저가 좋아하는 아이템과 유사한 아이템을 추천하는 방식입니다. 유사한 아이템은 해당 유저에게 동일한 평가를 받을 가능성이 크기 때문입니다. 예를 들어, Saving Private Ryan와 유사한 영화는 전쟁 영화거나, 톰행크스가 나오거나, 스필버그 감동의 다른 영화일 수 있습니다. 만약 누군가가 Saving Private Ryan를 어떻게 평가할지 궁금하다면, 그 사람이 실제로 본 영화 중에서 Saving Private Ryan와 유사한 영화를 어떻게 평가했는지 찾는 것과 같은 맥락입니다.&lt;/p&gt;

&lt;h3 id=&quot;similarity&quot;&gt;similarity&lt;/h3&gt;
&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;neighborhood method&lt;/code&gt;는 두 유저간 혹은 아이템간 유사도를 계산해야합니다. 유사도를 정량적으로 평가하기 위해서 일반적으로 2가지 measure를 사용합니다.&lt;/p&gt;

&lt;p&gt;&lt;b&gt; (1) pearson correlation coefficient&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;유저 &lt;script type=&quot;math/tex&quot;&gt;u&lt;/script&gt;와 유저 &lt;script type=&quot;math/tex&quot;&gt;v&lt;/script&gt;의 유사도를 &lt;script type=&quot;math/tex&quot;&gt;s(u, v)&lt;/script&gt;로 나타내면,&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;s(u, v) = {
{\sum_{i \in I_{uv}} (r_{ui} - \mu_u) \cdot (r_{vi} - \mu_v)}\over{\sqrt{\sum_{i \in I_{uv}} {(r_{ui} - \mu_u)}^2} \cdot \sqrt{\sum_{i \in I_{uv}} {(r_{vi} - \mu_v)}^2}}}&lt;/script&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;r_{ui}&lt;/script&gt;는 사용자 u가 아이템 i에 대해서 평가한 평점&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;I_{uv}&lt;/script&gt;는 유저 &lt;script type=&quot;math/tex&quot;&gt;u&lt;/script&gt; 와 유저 &lt;script type=&quot;math/tex&quot;&gt;v&lt;/script&gt; 모두에 의해 평가된 아이템의 집합&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;\mu_u&lt;/script&gt;는 유저 &lt;script type=&quot;math/tex&quot;&gt;u&lt;/script&gt;의 평균 평점과 &lt;script type=&quot;math/tex&quot;&gt;\mu_v&lt;/script&gt;는 유저 &lt;script type=&quot;math/tex&quot;&gt;v&lt;/script&gt;의 평균 평점&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;마찬가지로 아이템 &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;와 아이템 &lt;script type=&quot;math/tex&quot;&gt;j&lt;/script&gt;의 유사도를 &lt;script type=&quot;math/tex&quot;&gt;s(i, j)&lt;/script&gt;로 나타내면,&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;s(i, j) = {
{\sum_{u \in U_{ij}} (r_{ui} - \mu_i) \cdot (r_{uj} - \mu_j)}\over{\sqrt{\sum_{u \in U_{ij}} {(r_{ui} - \mu_i)}^2} \cdot \sqrt{\sum_{u \in U_{ij}} {(r_{uj} - \mu_j)}^2}}}&lt;/script&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;U_{ij}&lt;/script&gt;는 아이템 &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; 와 아이템 &lt;script type=&quot;math/tex&quot;&gt;j&lt;/script&gt;를 모두 평가한 유저들의 집합&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;\mu_i&lt;/script&gt;는 아이템 &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;의 평균 평점과 &lt;script type=&quot;math/tex&quot;&gt;\mu_j&lt;/script&gt;는 아이템 &lt;script type=&quot;math/tex&quot;&gt;j&lt;/script&gt;의 평균 평점&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;의미적으로는 유저 &lt;script type=&quot;math/tex&quot;&gt;u&lt;/script&gt;(혹은 아이템 &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;) 평점이 1점 증가할때, 유저 &lt;script type=&quot;math/tex&quot;&gt;v&lt;/script&gt;(혹은 아이템 &lt;script type=&quot;math/tex&quot;&gt;j&lt;/script&gt;) 평점은 어느정도 증가/감소하는지를 평가하는 것입니다. 상관계수는 상관관계가 클수록 &lt;script type=&quot;math/tex&quot;&gt;\pm1&lt;/script&gt;에 가까운 값을 갖고, 0일 경우 상관관계가 거의 없는 것을 의미합니다.&lt;/p&gt;

&lt;p&gt;&lt;b&gt; (2) Cosine Similarity &lt;/b&gt;&lt;/p&gt;

&lt;p&gt;코사인 유사도는 두 non-zero vector간의 코사인 각을 측정하는 것입니다. 
두 벡터 A와 B의 코사인 유사도는 두 벡터의 내적을 이용해 정의됩니다. 각도 &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;가 0도이면 코사인 유사도는 1이 되고, &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;가 90도이면 코사인 유사도는 0이 됩니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
A \cdot B &amp; = |A| |B| cos \theta \\ \\
similarity  =  cos \theta &amp; = \frac{A \cdot B}{|A||B|} \\
&amp; =  \frac{\sum_{i=1}^{n}A_i \cdot B_i}{\sqrt{\sum_{i=1}^{n}(A_i)^2}{\sqrt{\sum_{i=1}^{n}(B_i)^2}}}
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;유저 &lt;script type=&quot;math/tex&quot;&gt;u&lt;/script&gt;와 유저 &lt;script type=&quot;math/tex&quot;&gt;v&lt;/script&gt;의 유사도를 &lt;script type=&quot;math/tex&quot;&gt;s(u, v)&lt;/script&gt;로 나타내면,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;s(u, v) = \frac{\sum_{i \in I_{uv}} r_{ui} \cdot r_{vi} }{ \sqrt{\sum_{i \in I_{uv}}r_{ui}^2} \sqrt{\sum_{i \in I_{uv}}r_{vi}^2} }&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;r_{ui}&lt;/script&gt;는 사용자 u가 아이템 i에 대해서 평가한 평점&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;I_{uv}&lt;/script&gt;는 유저 &lt;script type=&quot;math/tex&quot;&gt;u&lt;/script&gt; 와 유저 &lt;script type=&quot;math/tex&quot;&gt;v&lt;/script&gt; 모두에 의해 평가된 아이템의 집합&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;마찬가지로 아이템 &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;와 아이템 &lt;script type=&quot;math/tex&quot;&gt;j&lt;/script&gt;의 유사도를 &lt;script type=&quot;math/tex&quot;&gt;s(i, j)&lt;/script&gt;로 나타내면,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;s(i, j) = \frac{\sum_{u \in U_{ij}} r_{ui} \cdot r_{uj} }{ \sqrt{\sum_{u \in U_{ij}}r_{ui}^2} \sqrt{\sum_{u \in U_{ij}}r_{uj}^2} }&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;U_{ij}&lt;/script&gt;는 아이템 &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; 와 아이템 &lt;script type=&quot;math/tex&quot;&gt;j&lt;/script&gt;를 모두 평가한 유저들의 집합&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;pearson correlation coefficient와 비교하여 평균 평점에 대한 교정이 포함되지 않은 것을 볼수 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;matrix-factorization&quot;&gt;Matrix Factorization&lt;/h2&gt;

&lt;h3 id=&quot;sgd-vs-als&quot;&gt;SGD vs. ALS&lt;/h3&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;

&lt;p&gt;https://en.wikipedia.org/wiki/Collaborative_filtering&lt;/p&gt;

&lt;p&gt;https://medium.com/@cfpinela/recommender-systems-user-based-and-item-based-collaborative-filtering-5d5f375a127f&lt;/p&gt;

&lt;p&gt;https://datajobs.com/data-science-repo/Recommender-Systems-[Netflix].pdf&lt;/p&gt;

&lt;p&gt;http://jeongchul.tistory.com/553&lt;/p&gt;

&lt;p&gt;http://nicolas-hug.com/blog/matrix_facto_2&lt;/p&gt;

&lt;p&gt;https://datascienceschool.net/view-notebook/fcd3550f11ac4537acec8d18136f2066/&lt;/p&gt;</content><author><name>yjucho</name></author><summary type="html">추천시스템에 대해서 알아보자! - 지난 1편에서는 앤드류 응의 강의를 통해서 추천시스템의 전반적인 내용에 대해 알아보았습니다. 이번에는 Collaboratvie Filtering에 대해서 더 자세히 알아보고자 합니다.</summary></entry><entry><title type="html">Big Data Analysis with Scala and Spark</title><link href="http://localhost:4000/spark/scala/spark-with-scala/" rel="alternate" type="text/html" title="Big Data Analysis with Scala and Spark " /><published>2018-10-21T00:00:00+09:00</published><updated>2018-10-21T00:00:00+09:00</updated><id>http://localhost:4000/spark/scala/spark-with-scala</id><content type="html" xml:base="http://localhost:4000/spark/scala/spark-with-scala/">&lt;p&gt;https://www.coursera.org/learn/scala-spark-big-data/home/welcome&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Shared Memory Data Parallelism (SDP)와 Distributed Data Parallelism (DDP)의 공통점과 차이점을 얘기해주세요.&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;공통점 : 데이터를 나눠서, 병렬로 데이터를 처리한 후 결과를 합침(data-parallel programming). Collection abstraction을 처리할 수 있음. &lt;br /&gt;차이점 : SDP의 경우 한 머신 내 메모리 상에서 데이터가 나눠져  처리가 일어나지만, DDP는 여러개의 노드(머신)에서 처리가 됨. DDP는 노드간의 통신이 필요하기 때문에 latency를 고려해야함&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;분산처리 프레임워크 Haddop의 Fault Tolerance는 DDP의 어떤 문제를 해결했나요?&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Computations on unthinkably large data sets to succeed to completion. 수백~수천개의 노드로 확장가능하도록 함. 노드 중 한개라도 failure 발생하더라도 recover할 수 있음&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Spark가 하둡과 달리 데이터를 메모리에 저장하면서 개선한 것 무엇이고, 왜 메모리에 저장하면 그것이 개선이 되나요?&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;latency를 개선함. 같은 job에 대해서 100배 이상의 성능을 보임. Functional programming을 통해 latency를 줄임&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;val ramyons = List(&quot;신라면&quot;, &quot;틈새라면&quot;, &quot;너구리&quot;)&lt;/code&gt; &lt;br /&gt;
&lt;code class=&quot;highlighter-rouge&quot;&gt;val kkodulRamyons = ramyons.map(ramyon =&amp;gt; &quot;꼬들꼬들 &quot; + ramyon)&lt;/code&gt; &lt;br /&gt;
&lt;code class=&quot;highlighter-rouge&quot;&gt;kkodulRamyonsList.map()을 사용하여 ramyons 리스트에서 kkodulRamyon List를 새로 만들었습니다. kkodulRamyons랑 똑같이 생긴 List를 만드는 Scala 코드를 써주세요.&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;val kkodulRamyons = List(“꼬들꼬들 신라면”, “꼬들꼬들 틈새라면”, “꼬들꼬들 너구리”)&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;val noodles = List(List(&quot;신라면&quot;, &quot;틈새라면&quot;, &quot;너구리&quot;), List(&quot;짜파게티&quot;, &quot;짜왕&quot;, &quot;진짜장&quot;))&lt;/code&gt;
&lt;code class=&quot;highlighter-rouge&quot;&gt;val flatNoodles = noodles.flatMap(list =&amp;gt; list)&lt;/code&gt;
&lt;code class=&quot;highlighter-rouge&quot;&gt;flatNoodlesList.flatmap() 을 사용하여 noodles 리스트에서 flatNoodles List를 새로 만들었습니다. flatNoodles랑 똑같이 생긴 List를 만드는 Scala 코드를 써주세요.&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;val flatNoodles = List(“신라면”, “틈새라면”, “너구리”, “짜파게티”, “짜왕”, “진짜장”)&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;val jajangs = flatNoodles.filter(noodle =&amp;gt; noodle.contains(&quot;짜&quot;))&lt;/code&gt;
&lt;code class=&quot;highlighter-rouge&quot;&gt;jajangsList.filter() 를 사용하여 flatNoodles 리스트에서 jajangs List를 새로 만들었습니다. jajangs랑 똑같이 생긴 List를 만드는 Scala 코드를 써주세요.&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;val jajangs = List(“짜파게티”, “짜왕”, “진짜장”)&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;val jajangMenu = jajangs.reduce((first, second) =&amp;gt; first +&quot;,&quot; + second)&lt;/code&gt;
&lt;code class=&quot;highlighter-rouge&quot;&gt;jajangMenuList.reduce()를 사용하여 jajangs 리스트에서 jajangMenu String을 만들었습니다. jajangMenu랑 똑같이 생긴 String을 만드는 Scala 코드를 써주세요.&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;var jajangMenu = “짜파게티,짜왕,진짜장”&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Eager execution와 Lazy execution의 차이점은 무엇인가요?&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Lazy execution은 결과값이 바로 계산되지 않고 eager execution은 결과가 바로 계산됨. spark의 transformation은 lazy execution이라 action이 나타날때까지 실제로는 아무것도 수행되지 않음&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Transformation과 Action의 결과물 (Return Type)은 어떻게 다를까요?&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Transformation은 새로운 RDD를 결과물로 리턴하고, Action은 HDFS같은 외부 저장소에 값을 리턴함&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;RDD.cache()는 어떤 작동을 하고, 언제 쓰는 것이 좋은가?&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Action을 수행할때마다 RDD를 다시 계산하는 것은 시간적 비용이 크기때문에 메모리에 캐시 저장함&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Lazy Execution이 Eager Execution 보다 더 빠를 수 있는 예를 얘기해주세요.&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;logistic regression처럼 Iterative algoritm의 경우, 값(weight)이 업데이트될때마다 데이터를 평가하하는 경우 매 iteration마다 반복 계산되는 것은 계산 비용이 큼. 따라서 RDD를 persist()나 cache()를 이용해 메모리에 저장하는 방법이 효과적임
other’s best answer : RDD에서 transformation 이 수행 된 데이터를 재사용하는 경우 Lazy Execution이 Eager Excution보다 더 빠르다. 예를 들어 로그파일에서 “ERROR”가 포함된 로그만 필터링하기 위해 filter를 예약한 뒤 캐싱 기능을 호출한다. 이후 take를 이용하여 10개의 에러로그를 획득하는데, 액션이 호출되어 필터링이 수행되면서 필터링된 로그를 메모리에 캐싱을 하게 된다. 이처럼 캐싱된 에러 로그는 이후 총 개수를 구한다던가 하는 다른 actino이 호출될 때 메모리에 적재된 데이터를 이용하여 성능이 향상된다. 최초로 호출되는 action은 disk에서 읽어오는 것보다 조금 느릴 수 있다.
참고 - http://knight76.tistory.com/entry/%ED%8E%8C-lazy-evaluation%EB%8A%90%EA%B8%8B%ED%95%9C-%EA%B3%84%EC%82%B0%EB%B2%95%EC%97%90-%EB%8C%80%ED%95%9C-%EC%A2%8B%EC%9D%80-%EC%84%A4%EB%AA%85-%EA%B7%B8%EB%A6%BC-%EC%9E%90%EB%A3%8C&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;foldLft 와 aggregate 둘다 inputType과 outputType이 다른데 왜 aggregate 만 병렬 처리가 가능한지 설명해주세요.&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;foldLeft는 시퀀셜하게 처리되기 때문에 병렬처리가 불가능함. 만약 두개 콜렉션으로 나눠서 병렬처리한다고 했을때, 아웃풋 타입이 바뀌기때문에 두개의 아웃풋을 합치려고 할 때 타입 에러가 나서 더이상 동일한 함수를 적용할수 없음. aggregate는 seqop과 combop 펑션으로 이루어져있어, chunk로 나눠 처리된 결과를 combop함수를 통해 합칠수 있기 때문에 리턴 타입 변환과 병렬처리가 모두 가능함&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;pairRDD는 어떤 데이터 구조에 적합한지 설명해주세요. 또 pairRDD는 어떻게 만드나요?&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Key-value 형태로 구조화된 데이터를 다룰 때 유용함. 이미 존재하는 RDD에서 map을 이용해서 아래와 같이 만들수 있음
val rdd: RDD[WikipediaPage] = … 
val pairRdd = rdd.map(page =&amp;gt; (page.title, page.text))&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;groupByKey()와 mapValues()를 통해 나온 결과를 reduceByKey()를 사용해서도 똑같이 만들 수 있습니다. 그렇지만 reduceByKey를 쓰는 것이 더 효율적인 이유는 무엇일까요?&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;reduceByKey는 싱글머신에 있는 데이터끼리 합친 후 셔플됨. 반면 groupByKey()는 모든 key-value 값들이 셔플되기때문에 네트워크 상에 데이터가 불필요하게 많이 이동하면서 계산됨. 참고 - https://databricks.gitbooks.io/databricks-spark-knowledge-base/content/best_practices/prefer_reducebykey_over_groupbykey.html&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;join 과 leftOuterJoin, rightOuterJoin이 어떻게 다른지 설명하세요. &lt;/code&gt;&lt;/p&gt;

&lt;p&gt;join(inner join) return a new RDD containing combined pairs whose keys are present in both input RDDs —inner join(join)은 두개의 RDD에 모두 포함되는 key만 포함하여 새로운 pair-RDD를 리턴함. Outer join(leftOuterJoin, rightOuterJoin) return a new RDD containing combined pairs whose keys don’t have to be present in both input RDDs — 왼쪽 혹은 오른쪽 RDD 중에서 유지하고 싶은 keys를 중심으로 결합하여 새로운 RDD를 만듦&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Shuffling은 무엇인가요? 이것은 어떤 distributed data paraellism의 성능에 어떤 영향을 가져오나요?&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;we typically have to move data from one node to another to be “grouped with” its key. Doing this is called “shuffling”. 인메모리 대비 노드간 네트워크 통신이 필요함으로 되도록이면 최소화하는 것이 좋음&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;셔플링은 무엇이고 언제 발생하나요?&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;데이터를 키값을 기준으로 그룹핑하여 한 노드에서 다른 노드로 이동시키기는 것. groupByKey()를 수행할 때 발생.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;파티션은 무엇인가요? 파티션의 특징을 2가지 알려주세요.&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;pair-RDD를 키값을 중심으로 여러 노드에 나눠 저장하는 것&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;동일한 파티션에 있는 데이터들은 반드시 같은 머신에 존재한다.&lt;/li&gt;
  &lt;li&gt;클러스터 안에 한개의 머신에는 적어도 하나 이상의 파티션이 존재할수 있다.&lt;/li&gt;
  &lt;li&gt;파티션의 수는 설정할수 있다. 기본적으로는 executor node의 코어 수와 같다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;스파크에서 제공하는 partitioning 의 종류 두가지를 각각 설명해주세요.&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Hash partitioning :튜플(k, v)마다 p=k.hashCode()%numPartitions 해서 p가 같은 데이터끼리 모아 파티셔닝하는 것. &lt;br /&gt;
Range partitionin :ordering이 있는 key일 경우, 범위별로 데이터를 나누는 것&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;파티셔닝은 어떻게 퍼포먼스를 높여주나요?&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;키값을 중심으로 데이터가 어떤 머신에 있는지 알기 때문에 데이터가 셔플링되는 것을 최소화할수 있음&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;rdd 의 toDebugString 의 결과값은 무엇을 보여주나요?&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;RDD’s lineage를 시각적으로 보여줌&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;파티션 된 rdd에 map 을 실행하면 결과물의 파티션은 어떻게 될까요? mapValues의 경우는 어떤가요?&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;map을 실행하면 파티션이 없어짐. map은 키를 바꿀수 있는 오퍼레이션 이기때문임. mapValues는 키값을 유지하기때문에 파티션도 유지됨.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Narrow Dependency 와 Wide Dependency를 설명해주세요. 각 Dependency를 만드는 operation은 무엇이 있을까요?&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Narrow Dependency - 1개의 부모RDD는 최대 1개의 자식RDD에만 영향을 줌. map, filter, union, join with co-partitioned inputs &lt;br /&gt;
Wide Dependency - 1개의 부모RDD가 여러개의 자식RDD에 영향을 줌. groupByKey, join with inputs not co-partitioned&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Lineage 는 어떻게 Fault Tolerance를 가능하게 하나요?&lt;/code&gt;
RDD는 immutable하고, 우리가 higher-order function을 사용하고, 그 function 역시 RDD를 리턴하기 때문에 lineage graphs를 통해서 dependency information을 추적하여 잃어버린 파티션을 다시 계산하여 failure로부터 회복할수 있다.&lt;/p&gt;</content><author><name>yjucho</name></author><summary type="html">https://www.coursera.org/learn/scala-spark-big-data/home/welcome</summary></entry><entry><title type="html">recommender systems</title><link href="http://localhost:4000/recommender%20systems/recommendation/" rel="alternate" type="text/html" title="recommender systems" /><published>2018-10-20T00:00:00+09:00</published><updated>2018-10-20T00:00:00+09:00</updated><id>http://localhost:4000/recommender%20systems/recommendation</id><content type="html" xml:base="http://localhost:4000/recommender%20systems/recommendation/">&lt;p&gt;추천시스템에 대해서 알아보자! 앤드류응의 머신러닝 강의 중 추천시스템 부분에 대해서 정리하였습니다.&lt;/p&gt;

&lt;h2 id=&quot;problem-formulation&quot;&gt;problem formulation&lt;/h2&gt;

&lt;p&gt;아래와 같이 4명의 유저가 5개 영화를 평가한 데이터가 있다고 하겠습니다. 추천시스템은 이와 같은 평점 데이터를 이용해, 유저가 아직 평가하지 않은 영화를 몇점으로 평가할지 예측하는 문제로 생각할 수 있습니다.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Movie&lt;/th&gt;
      &lt;th&gt;Alice(1)&lt;/th&gt;
      &lt;th&gt;Bob(2)&lt;/th&gt;
      &lt;th&gt;Carol(3)&lt;/th&gt;
      &lt;th&gt;Dave(4)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Love at last&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Romance forever&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;&lt;b&gt;?&lt;/b&gt;&lt;/td&gt;
      &lt;td&gt;&lt;b&gt;?&lt;/b&gt;&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Cute puppies of love&lt;/td&gt;
      &lt;td&gt;&lt;b&gt;?&lt;/b&gt;&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;&lt;b&gt;?&lt;/b&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Nonstop car chases&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Swords vs. karete&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;&lt;b&gt;?&lt;/b&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;n_u&lt;/script&gt; = number of users &lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;n_m&lt;/script&gt; = number of movies &lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;r(i, j)&lt;/script&gt; = 1 if user &lt;script type=&quot;math/tex&quot;&gt;j&lt;/script&gt; has rated movie &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; &lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;y^{(i, j)}&lt;/script&gt; = rating given by user &lt;script type=&quot;math/tex&quot;&gt;j&lt;/script&gt; to movie &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; (defined only if &lt;script type=&quot;math/tex&quot;&gt;r(i, j) = 1&lt;/script&gt; )&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;given &lt;script type=&quot;math/tex&quot;&gt;r(i, j)&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;y^{(i, j)}&lt;/script&gt;, we try to predict what these values of the question mark should be&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;content-based-recommendations&quot;&gt;Content Based Recommendations&lt;/h2&gt;

&lt;p&gt;우선 모든 영화가 로맨스인지, 액션인지 평가된 특징 정보가 있다고 가정하겠습니다. 아래 테이블은 각 영화의 특징에 해당하는 정보(피쳐 벡터), &lt;script type=&quot;math/tex&quot;&gt;x_1&lt;/script&gt;과 &lt;script type=&quot;math/tex&quot;&gt;x_2&lt;/script&gt;를 추가한 것입니다. 이 경우, 각 사용자의 평점은 피쳐벡서를 입력으로 하는 회귀 문제가 됩니다. 회귀문제에서 가중치 &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;는 사용자마다 다르며, 이미 평가한 데이터와 예측값의 오차를 최소화하는 방향으로 회귀식의 가중치을 학습할 수 있습니다.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Movie&lt;/th&gt;
      &lt;th&gt;Alice(1)&lt;/th&gt;
      &lt;th&gt;Bob(2)&lt;/th&gt;
      &lt;th&gt;Carol(3)&lt;/th&gt;
      &lt;th&gt;Dave(4)&lt;/th&gt;
      &lt;th&gt;&lt;script type=&quot;math/tex&quot;&gt;x_1 \\ (romance)&lt;/script&gt;&lt;/th&gt;
      &lt;th&gt;&lt;script type=&quot;math/tex&quot;&gt;x_2 \\ (action)&lt;/script&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Love at last&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0.9&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Romance forever&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;&lt;b&gt;?&lt;/b&gt;&lt;/td&gt;
      &lt;td&gt;&lt;b&gt;?&lt;/b&gt;&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;0.01&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Cute puppies of love&lt;/td&gt;
      &lt;td&gt;&lt;b&gt;?&lt;/b&gt;&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;&lt;b&gt;?&lt;/b&gt;&lt;/td&gt;
      &lt;td&gt;0.99&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Nonstop car chases&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;0.1&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Swords vs. karete&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;&lt;b&gt;?&lt;/b&gt;&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0.9&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;For each user &lt;script type=&quot;math/tex&quot;&gt;j&lt;/script&gt;, learn a prarameter &lt;script type=&quot;math/tex&quot;&gt;\theta ^{(j)} \in \mathbb{R}^{n+1}&lt;/script&gt; &lt;br /&gt;
( n is the number of features. In above example, n = 2 because of &lt;script type=&quot;math/tex&quot;&gt;x_1,\ x_2&lt;/script&gt;. By default &lt;script type=&quot;math/tex&quot;&gt;x_0&lt;/script&gt;=0 )&lt;/p&gt;

&lt;p&gt;Predict user &lt;script type=&quot;math/tex&quot;&gt;j&lt;/script&gt; as rating movie &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; with &lt;script type=&quot;math/tex&quot;&gt;(\theta ^{(j)})^T x^{(i)}&lt;/script&gt; &lt;br /&gt;
( &lt;script type=&quot;math/tex&quot;&gt;x^{(i)}&lt;/script&gt; = feature vector of movie &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; )&lt;/p&gt;

&lt;p&gt;&lt;b&gt;&lt;em&gt;example of Alice’s rating of “Cute pupples of love”&lt;/em&gt;&lt;/b&gt; &lt;br /&gt;
&lt;small&gt;(Assumed &lt;script type=&quot;math/tex&quot;&gt;\theta ^{(1)}&lt;/script&gt; learned by model) &lt;/small&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;x^{(3)} = 
\begin{bmatrix}
1  \\
0.99 \\
0 
\end{bmatrix} \ \ \ 
\theta^{(1)} = 
\begin{bmatrix}
0  \\
5 \\
0 
\end{bmatrix}
\\
(\theta ^{(1)})^T x^{(3)} = 5 * 0.99 = 4.95&lt;/script&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\theta ^{(j)}&lt;/script&gt; = parameter vector of user &lt;script type=&quot;math/tex&quot;&gt;j&lt;/script&gt; &lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;x ^{(i)}&lt;/script&gt; = feature vector for movie &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; &lt;br /&gt;
For user &lt;script type=&quot;math/tex&quot;&gt;j&lt;/script&gt;, movie &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;, predicted rating : &lt;script type=&quot;math/tex&quot;&gt;(\theta ^{(j)})^T x^{(i)}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;&lt;b&gt;To learn &lt;script type=&quot;math/tex&quot;&gt;\theta ^{(j)}&lt;/script&gt; (parameter for user &lt;script type=&quot;math/tex&quot;&gt;j&lt;/script&gt;)&lt;/b&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\min_{\theta ^{(j)}} \frac{1}{2} \sum_{i:r(i,j)=1} ((\theta ^{(j)})^T x^{(i)} - y^{(i,j)})^2 + \frac{\lambda}{2} \sum_{k=1}^{n}(\theta_k^{(j)})^2&lt;/script&gt;

&lt;p&gt;&lt;b&gt;To learn  &lt;script type=&quot;math/tex&quot;&gt;\theta ^{(1)},\theta ^{(2)}, ..., \theta ^{(n_u)}&lt;/script&gt; &lt;/b&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\min_{\theta ^{(j)}} \frac{1}{2} \sum_{j=1}^{n_u} \sum_{i:r(i,j)=1} ((\theta ^{(j)})^T x^{(i)} - y^{(i,j)})^2 + \frac{\lambda}{2} \sum_{j=1}^{n_u} \sum_{k=1}^{n}(\theta_k^{(j)})^2&lt;/script&gt;

&lt;p&gt;&lt;b&gt;Gradient descent update&lt;/b&gt; :&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\theta_k^{(j)} := \theta_k^{(j)} -  \alpha \sum_{i:r(i,j)=1} ((\theta ^{(j)})^T x^{(i)} - y^{(i,j)})x_k^{(i)} \ \ (for \ k=0) \\
\theta_k^{(j)} := \theta_k^{(j)} -  \alpha \left( \sum_{i:r(i,j)=1} ((\theta ^{(j)})^T x^{(i)} - y^{(i,j)})x_k^{(i)} + \lambda \theta_k^{(j)} \right) \ \ (for \ k \neq 0)&lt;/script&gt;

&lt;h2 id=&quot;collaborative-filtering&quot;&gt;Collaborative Filtering&lt;/h2&gt;
&lt;p&gt;이제까지는 모든 영화가 로맨스인지, 액션인지 평가된 피쳐벡터가 존재한다는 가정을 하였습니다. 하지만 모든 영화를 보고 특징 정보를 정리하는 것은 매우 비용이 많이 드는 작업입니다. 이를 극복하기 위해서 반대로 사용자에게 로맨스 영화를 얼마나 좋아하는지, 액션 영화를 얼마나 좋아하는지를 조사하고, 이 값을 토대로 피쳐 벡터를 추정하는 방법을 택할 수 있습니다. 사용자 &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;가 응답한 정보를 &lt;script type=&quot;math/tex&quot;&gt;\theta^{(i)}&lt;/script&gt;로 이용하여 아래와 같이 계산할 수 있습니다.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Movie&lt;/th&gt;
      &lt;th&gt;Alice(1)&lt;/th&gt;
      &lt;th&gt;Bob(2)&lt;/th&gt;
      &lt;th&gt;Carol(3)&lt;/th&gt;
      &lt;th&gt;Dave(4)&lt;/th&gt;
      &lt;th&gt;&lt;script type=&quot;math/tex&quot;&gt;x_1 \\ (romance)&lt;/script&gt;&lt;/th&gt;
      &lt;th&gt;&lt;script type=&quot;math/tex&quot;&gt;x_2 \\ (action)&lt;/script&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Love at last&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;&lt;b&gt;?&lt;/b&gt;&lt;/td&gt;
      &lt;td&gt;&lt;b&gt;?&lt;/b&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Romance forever&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;&lt;b&gt;?&lt;/b&gt;&lt;/td&gt;
      &lt;td&gt;&lt;b&gt;?&lt;/b&gt;&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;&lt;b&gt;?&lt;/b&gt;&lt;/td&gt;
      &lt;td&gt;&lt;b&gt;?&lt;/b&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Cute puppies of love&lt;/td&gt;
      &lt;td&gt;&lt;b&gt;?&lt;/b&gt;&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;&lt;b&gt;?&lt;/b&gt;&lt;/td&gt;
      &lt;td&gt;&lt;b&gt;?&lt;/b&gt;&lt;/td&gt;
      &lt;td&gt;&lt;b&gt;?&lt;/b&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Nonstop car chases&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;&lt;b&gt;?&lt;/b&gt;&lt;/td&gt;
      &lt;td&gt;&lt;b&gt;?&lt;/b&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Swords vs. karete&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;&lt;b&gt;?&lt;/b&gt;&lt;/td&gt;
      &lt;td&gt;&lt;b&gt;?&lt;/b&gt;&lt;/td&gt;
      &lt;td&gt;&lt;b&gt;?&lt;/b&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\theta^{(1)} = 
\begin{bmatrix}
0  \\
5 \\
0 
\end{bmatrix} \ \ \ 
\theta^{(2)} = 
\begin{bmatrix}
0  \\
5 \\
0 
\end{bmatrix} \ \ \ 
\theta^{(3)} = 
\begin{bmatrix}
0  \\
0 \\
5 
\end{bmatrix} \ \ \ 
\theta^{(4)} = 
\begin{bmatrix}
0  \\
0 \\
5 
\end{bmatrix} \ \ \&lt;/script&gt;

&lt;p&gt;&lt;b&gt; Given &lt;script type=&quot;math/tex&quot;&gt;\theta ^{(1)},\theta ^{(2)}, ..., \theta ^{(n_u)}&lt;/script&gt;, to learn &lt;script type=&quot;math/tex&quot;&gt;x^{(i)}&lt;/script&gt; &lt;/b&gt; :&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\min_{x^{(i)}} \frac{1}{2} \sum_{j:r(i,j)=1} ((\theta ^{(j)})^T x^{(i)} - y^{(i,j)})^2 + \frac{\lambda}{2} \sum_{k=1}^{n}(x_k^{(i)})^2&lt;/script&gt;

&lt;p&gt;&lt;b&gt; Given &lt;script type=&quot;math/tex&quot;&gt;\theta ^{(1)},\theta ^{(2)}, ..., \theta ^{(n_u)}&lt;/script&gt;, to learn &lt;script type=&quot;math/tex&quot;&gt;x^{(1)}, ..., x^{(n_m)}&lt;/script&gt; &lt;/b&gt; :&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\min_{x^{(1)},...,x^{(n_m)}} \frac{1}{2} \sum_{i=1}^{n_m} \sum_{j:r(i,j)=1} ((\theta ^{(j)})^T x^{(i)} - y^{(i,j)})^2 + \frac{\lambda}{2} \sum_{i=1}^{n_m} \sum_{k=1}^{n}(x_k^{(i)})^2&lt;/script&gt;

&lt;blockquote&gt;
  &lt;p&gt;Collaborative filtering refers to the observation that when you run this algorithm with a large set of users, all of these users are effectively doing collaboratively to get better movie ratings for everyone. because with every user rating, some subset of the movies, every user is helping the algorithm a little bit to learn better features. By rating a few movies myself, I would be hoping the systerm learn better features and then these features can be used by the system to make better movies predictions for everyone else. &lt;u&gt;So there's a sense of collaboration where every user is helping the system learn better features for the common good.&lt;/u&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Content based 방법은 영화별 특징 정보가 존재할 때, 유저별 가중치를 추정하는 것이고, Collaborative filtering은 유저별 가중치가 존재할 때 영화별 특징 정보를 추정하는 것입니다. 따라서 우리는 이 두가지를 결합하여, forward and backward 방식으로 초기값 &lt;script type=&quot;math/tex&quot;&gt;\theta^{(i)}&lt;/script&gt;를 랜덤하게 설정한 후 피쳐벡터를 추정하고, 추정된 피쳐벡터로 다시 유저 가중치를 구할수 있습니다. 더 간단하게는 &lt;script type=&quot;math/tex&quot;&gt;\theta^{(i)}&lt;/script&gt;와 &lt;script type=&quot;math/tex&quot;&gt;x^{j}&lt;/script&gt;를 동시에 추정할 수 있습니다.&lt;/p&gt;

&lt;p&gt;if we are given &lt;script type=&quot;math/tex&quot;&gt;x^{(i)}&lt;/script&gt;, we can estimate &lt;script type=&quot;math/tex&quot;&gt;\theta^{(i)}&lt;/script&gt;. Likewise if we are given &lt;script type=&quot;math/tex&quot;&gt;\theta^{(i)}&lt;/script&gt;, we can estimate &lt;script type=&quot;math/tex&quot;&gt;x^{(i)}&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;So we can initialize &lt;script type=&quot;math/tex&quot;&gt;\theta^{(i)}&lt;/script&gt; randomly, then estimate &lt;script type=&quot;math/tex&quot;&gt;x^{(i)}&lt;/script&gt;. After that, we update  &lt;script type=&quot;math/tex&quot;&gt;\theta^{(i)}&lt;/script&gt; and repeat.&lt;/p&gt;

&lt;p&gt;Putting togeter, we can solve for theta and x simultaneously.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\min_{x^{(1)}, ... , x^{(n_m)}, \theta^{(1)}, ..., \theta ^{(n_u)}} J(x^{(1)}, ... , x^{(n_m)}, \theta^{(1)}, ..., \theta ^{(n_u)}) \\
J(x^{(1)}, ... , x^{(n_m)}, \theta^{(1)}, ..., \theta ^{(n_u)}) = \frac{1}{2} \sum_{(i, j):r(i,j)=1}  ((\theta ^{(j)})^T x^{(i)} - y^{(i,j)})^2 + \frac{\lambda}{2} \sum_{i=1}^{n_m} \sum_{k=1}^{n}(x_k^{(i)})^2 + \frac{\lambda}{2} \sum_{j=1}^{n_u} \sum_{k=1}^{n}(\theta_k^{(j)})^2&lt;/script&gt;

&lt;p&gt;note : in this version, &lt;script type=&quot;math/tex&quot;&gt;x \in \mathbb{R}^{n}, \theta \in \mathbb{R}^{n}&lt;/script&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Initialize &lt;script type=&quot;math/tex&quot;&gt;x^{(1)}, ... , x^{(n_m)}, \theta^{(1)}, ..., \theta ^{(n_u)}&lt;/script&gt; to small random values&lt;/li&gt;
  &lt;li&gt;Minimize &lt;script type=&quot;math/tex&quot;&gt;J(x^{(1)}, ... , x^{(n_m)}, \theta^{(1)}, ..., \theta ^{(n_u)})&lt;/script&gt; using gradient descent(or an advanced optimization algorithm). E.g. for every &lt;script type=&quot;math/tex&quot;&gt;j=1, ..., n_u, i=1,...,n_m&lt;/script&gt; :
&lt;script type=&quot;math/tex&quot;&gt;x_k^{(i)} := x_k^{(i)} -  \alpha \left( \sum_{j:r(i,j)=1} ((\theta ^{(j)})^T x^{(i)} - y^{(i,j)})\theta_k^{(j)} + \lambda x_k^{(i)} \right) \\
\theta_k^{(j)} := \theta_k^{(j)} -  \alpha \left( \sum_{i:r(i,j)=1} ((\theta ^{(j)})^T x^{(i)} - y^{(i,j)})x_k^{(i)} + \lambda \theta_k^{(j)} \right)&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;For a user with parameters &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; and a movie with (learned) features &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt;, predict a star rating of &lt;script type=&quot;math/tex&quot;&gt;\theta^Tx&lt;/script&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;vectorization--low-rank-matrix-factorization&quot;&gt;Vectorization : Low Rank Matrix Factorization&lt;/h2&gt;
&lt;p&gt;위에서 설명한 것을 각 element별로 구하는 것이 아니라 matrix 형태로 vectorization하여 계산할 수도 있습니다.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Movie&lt;/th&gt;
      &lt;th&gt;Alice(1)&lt;/th&gt;
      &lt;th&gt;Bob(2)&lt;/th&gt;
      &lt;th&gt;Carol(3)&lt;/th&gt;
      &lt;th&gt;Dave(4)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Love at last&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Romance forever&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;&lt;b&gt;?&lt;/b&gt;&lt;/td&gt;
      &lt;td&gt;&lt;b&gt;?&lt;/b&gt;&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Cute puppies of love&lt;/td&gt;
      &lt;td&gt;&lt;b&gt;?&lt;/b&gt;&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;&lt;b&gt;?&lt;/b&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Nonstop car chases&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Swords vs. karete&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;&lt;b&gt;?&lt;/b&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
Y = 
\begin{bmatrix}
5 &amp; 5 &amp; 0 &amp; 0 \\
5 &amp; ? &amp; ? &amp; 0 \\
? &amp; 4 &amp; 0 &amp; ? \\
0 &amp; 0 &amp; 5 &amp; 4 \\
0 &amp; 0 &amp; 5 &amp; 0 \\
\end{bmatrix} \ \ \ \

predicted \ ratings : 
\begin{bmatrix}
(\theta ^{(1)})^T x^{(1)} &amp; (\theta ^{(2)})^T x^{(1)} &amp; \cdots &amp; (\theta ^{(n_u)})^T x^{(1)} \\
(\theta ^{(1)})^T x^{(2)} &amp; (\theta ^{(2)})^T x^{(2)} &amp; \cdots &amp; (\theta ^{(n_u)})^T x^{(2)} \\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots \\
(\theta ^{(1)})^T x^{(n_m)} &amp; (\theta ^{(2)})^T x^{(n_m)} &amp; \cdots &amp; (\theta ^{(n_u)})^T x^{(n_m)} 
\end{bmatrix} %]]&gt;&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
X = 
\begin{bmatrix}
-(x^{(1)})^T- \\
-(x^{(2)})^T- \\
\cdots\\
-(x^{(n_m)})^T- \\
\end{bmatrix} \ \ \ 

\Theta = 
\begin{bmatrix}
| &amp; | &amp; \ &amp; | \\
(\Theta^{(1)})^T &amp; (\Theta^{(2)})^T &amp; \cdots &amp;(\Theta^{(n_u)})^T \\
| &amp; | &amp; \ &amp; | \\
\end{bmatrix} 
\\
then, \\
predicted \ ratings = X\Theta %]]&gt;&lt;/script&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;X\Theta&lt;/script&gt; has mathematical property of low rank matrix&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;b&gt; How to find movies &lt;script type=&quot;math/tex&quot;&gt;j&lt;/script&gt; related to movie &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; &lt;/b&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;small &lt;script type=&quot;math/tex&quot;&gt;\parallel x^{(i)} - x^{(j)} \parallel&lt;/script&gt; \to moving &lt;script type=&quot;math/tex&quot;&gt;j&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; are “similar”&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;implementation-detail-mean-normalization&quot;&gt;Implementation Detail Mean Normalization&lt;/h2&gt;
&lt;p&gt;지금까지 설명한 collaborative filtering은 평점 데이터가 없은 유저에 대해서는 항상 0값을 예측한다는 단점이 있습니다.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Movie&lt;/th&gt;
      &lt;th&gt;Alice(1)&lt;/th&gt;
      &lt;th&gt;Bob(2)&lt;/th&gt;
      &lt;th&gt;Carol(3)&lt;/th&gt;
      &lt;th&gt;Dave(4)&lt;/th&gt;
      &lt;th&gt;Eve(5)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Love at last&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;&lt;b&gt;?&lt;/b&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Romance forever&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;&lt;b&gt;?&lt;/b&gt;&lt;/td&gt;
      &lt;td&gt;&lt;b&gt;?&lt;/b&gt;&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;&lt;b&gt;?&lt;/b&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Cute puppies of love&lt;/td&gt;
      &lt;td&gt;&lt;b&gt;?&lt;/b&gt;&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;&lt;b&gt;?&lt;/b&gt;&lt;/td&gt;
      &lt;td&gt;&lt;b&gt;?&lt;/b&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Nonstop car chases&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;&lt;b&gt;?&lt;/b&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Swords vs. karete&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;&lt;b&gt;?&lt;/b&gt;&lt;/td&gt;
      &lt;td&gt;&lt;b&gt;?&lt;/b&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;b&gt;For Eve&lt;/b&gt;, compute &lt;script type=&quot;math/tex&quot;&gt;\theta^{(5)}&lt;/script&gt; : 
let’s say that n is equal to 2.&lt;/p&gt;

&lt;p&gt;Since Eve rated no movies, there are no movies for which r(i, j) is equal to one. So the first term of the objective plays no role at all in determining &lt;script type=&quot;math/tex&quot;&gt;\theta^{(5)}&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;the only term that affects  &lt;script type=&quot;math/tex&quot;&gt;\theta^{(5)}&lt;/script&gt; is the last term&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\min \frac{\lambda}{2} \sum_{j=1}^{n_u} \sum_{k=1}^{n}(\theta_k^{(j)})^2 = \frac{\lambda}{2} 
\begin{bmatrix}
(\Theta_1^{(5)})^2 + (\Theta_2^{(5)})^2
\end{bmatrix}&lt;/script&gt;

&lt;p&gt;Minimizing above term, we’re going to end up with &lt;script type=&quot;math/tex&quot;&gt;(\theta^{(5)})^T =[0 \ \ 0]&lt;/script&gt;. So all predicted ratings for Eve(&lt;script type=&quot;math/tex&quot;&gt;(\theta^{(5)})^T x^{(i)}&lt;/script&gt;) is equal to zero.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;This approach is not useful. The idea of mean normalization will let us fix this problem.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;이를 해결하기 위해 일반적으로 mean normalization이라는 전처리 과정을 추가합니다. 직관적으로 평점데이터가 없는 유저의 경우, 영화별 평균 평점으로 예측하도록 하는 방법입니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
Y = 
\begin{bmatrix}
5 &amp; 5 &amp; 0 &amp; 0 &amp; ? \\
5 &amp; ? &amp; ? &amp; 0 &amp; ? \\
? &amp; 4 &amp; 0 &amp; ? &amp; ? \\
0 &amp; 0 &amp; 5 &amp; 4 &amp; ? \\
0 &amp; 0 &amp; 5 &amp; 0 &amp; ? \\
\end{bmatrix} \ \ \ \

\mu = 
\begin{bmatrix}
2.5\\
2.5\\
2\\
2.25\\
1.25\\
\end{bmatrix} \ \ \ \
\to
Y = 
\begin{bmatrix}
2.5 &amp; 2.5 &amp; -2.5 &amp; -2.5 &amp; ? \\
2.5 &amp; ? &amp; ? &amp; -2.5 &amp; ? \\
? &amp; 2 &amp; -2 &amp; ? &amp; ? \\
-2.25 &amp; -2.25 &amp; 2.75 &amp; 1.75 &amp; ? \\
-1.25 &amp; -1.25 &amp; 3.75 &amp; -1.25 &amp; ? \\
\end{bmatrix} \ \ \ \ %]]&gt;&lt;/script&gt;

&lt;p&gt;&lt;b&gt;For user &lt;script type=&quot;math/tex&quot;&gt;j&lt;/script&gt;, on movie &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; predict :
&lt;script type=&quot;math/tex&quot;&gt;(\theta^{(j)})^T(x^{i}) + \mu_i&lt;/script&gt;&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;&lt;b&gt;For Eve :
&lt;script type=&quot;math/tex&quot;&gt;(\theta^{(j)})^T(x^{i}) + \mu_i = 0 + \mu_i = \mu_i&lt;/script&gt;&lt;/b&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Mean normalization as a solid pre-processing step for collaborative filtering.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;감사합니다!&lt;/p&gt;

&lt;h2 id=&quot;reference-&quot;&gt;Reference :&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=giIXNoiqO_U&quot;&gt;Recommender Systems - Problem Formulation&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=9siFuMMHNIA&quot;&gt;Recommender Systems - Content Based Recommendations&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=9AP-DgFBNP4&quot;&gt;Recommender Systems - Collaborative Filtering_1&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=YW2b8La2ICo&quot;&gt;Recommender Systems - Collaborative Filtering_2&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=5R1xOJOFRzs&quot;&gt;Recommender Systems - Vectorization Low Rank Matrix Factorization&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=Am9fhp2Q91o&quot;&gt;Recommender Systems - Implementational Detail Mean Normalization&lt;/a&gt;&lt;/p&gt;</content><author><name>yjucho</name></author><summary type="html">추천시스템에 대해서 알아보자! 앤드류응의 머신러닝 강의 중 추천시스템 부분에 대해서 정리하였습니다.</summary></entry><entry><title type="html">[번역] Attention? Attention!</title><link href="http://localhost:4000/attention/attention/" rel="alternate" type="text/html" title="[번역] Attention? Attention!" /><published>2018-10-13T00:00:00+09:00</published><updated>2018-10-13T00:00:00+09:00</updated><id>http://localhost:4000/attention/attention</id><content type="html" xml:base="http://localhost:4000/attention/attention/">&lt;blockquote&gt;
  &lt;p&gt;이 글은 &lt;a href=&quot;https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html&quot;&gt;lilianweng의 Attention? Attention! 포스팅&lt;/a&gt;을 번역한 글입니다.&lt;br /&gt;&lt;br /&gt;Attention은 최근 딥러닝 커뮤니티에서 자주 언급되는 유용한 툴입니다. 이 포스트에서는 어떻게 어텐션 개념과 다양한 어텐션 메커니즘을 설명하고 transformer와 SNAIL과 같은 모델들에 대해서 알아보고자 합니다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#whats-wrong-with-seq2seq-model&quot;&gt;What’s Wrong with Seq2Seq Model?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#born-for-translation&quot;&gt;Born for Translation&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#definition&quot;&gt;Definition&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#a-family-of-attention-mechanisms&quot;&gt;A Family of Attention Mechanisms&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#summary&quot;&gt;Summary&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#self-attention&quot;&gt;Self-Attention&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#soft-vs-hard-attention&quot;&gt;Soft vs Hard Attention&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#global-vs-local-attention&quot;&gt;Global vs Local Attention&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#transformer&quot;&gt;Transformer&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#key-value-and-query&quot;&gt;Key, Value and Query&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#multi-head-self-attention&quot;&gt;Multi-Head Self-Attention&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#encoder&quot;&gt;Encoder&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#decoder&quot;&gt;Decoder&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#full-architecture&quot;&gt;Full Architecture&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#snail&quot;&gt;SNAIL&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#self-attention-gan&quot;&gt;Self-Attention GAN&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#references&quot;&gt;References&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Attention은 우리가 이미지에서 어떤 영역을 주목하는지, 한 문장에서 연관된 단어는 무엇인지를 찾는데서 유래하였습니다. 그림1에 있는 시바견을 살펴보세요.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-10-13/shiba-example-attention.png&quot; width=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small&gt;&lt;em&gt;그림1. 사람옷을 입은 시바견. 이미지의 모든 권리는 인스타그램 &lt;a href=&quot;https://www.instagram.com/mensweardog/?hl=en&quot;&gt;@mensweardog&lt;/a&gt;에 있습니다.&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;인간은 이미지의 특정 부분을 고해상도로(노란 박스안에 뽀족한 귀) 집중하는 반면, 주변 부분들은 저해상도((눈이 쌓인 배경과 복장)로 인식하고 이후 초점영역을 조정하여 그에 따른 추론을 합니다. 이미지의 작은 패치가 가려져있을때, 나머지 영역의 픽셀들은 그 영역에 어떤 것이 들어가야 하는지를 알려주는 힌트가 됩니다. 우리는 노란 박스 안은 뽀족한 귀가 있어야 하는 것을 알고 있습니다. 왜냐하면 개의 코, 오른쪽의 다른 귀, 시바견의 몽롱한 눈(빨란 박스안에 것들)를 이미 봤기 때문입니다. 반면 이 추론을 하는데 아래쪽에 있는 스웨터나 담요는 별 도움이 되지 못합니다.&lt;/p&gt;

&lt;p&gt;마찬가지로, 한 문장이나 가까운 문맥 상에서 단어들간의 관계를 설명할수 있습니다. “eating”이라는 단어를 보았을때, 음식 종류에 해당하는 단어가 가까이 위치에 있을 것을 예상할수 있습니다. 그림2에서 “green”은 eating과 더 가까이 위치해있지만 직접적으로 관련있는 단어는 아닙니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-10-13/sentence-example-attention.png&quot; width=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small&gt;&lt;em&gt;그림2. 한 단어는 같은 문장의 단어들에 서로 다른 방식으로 주목하게 만듭니다.&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;간단히 말해, 딥러닝에서 어텐션은 weights의 중요도 벡터로 설명할수 있습니다. 이미지의 픽셀값이나 문장에서 단어 등 어떤 요소를 예측하거나 추정하기 위해, 다른 요소들과 얼마나 강하게 연관되어 있는지 확인하고(많은 논문들에서 읽은 것처럼) 이것들과 어텐션 백터로 가중 합산된 값의 합계를 타겟값으로 추정할 수 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;whats-wrong-with-seq2seq-model&quot;&gt;What’s Wrong with Seq2Seq Model?&lt;/h2&gt;

&lt;p&gt;seq2seq 모델은 언어 모델링에서 유래되었습니다. 간단히 말해서 입력 시퀀스를 새로운 시퀀스로 변형하는 것을 목적으로 하며, 이때 입력값이나 결과값 모두 임의 길이를 갖습니다. seq2seq의 예로는 기계번역, 질의응답 생성, 문장을 문법 트리로 구문 분석하는 작업 등이 있습니다.&lt;/p&gt;

&lt;p&gt;seq2seq 모델은 보통 인코더-디코더 구조로 이루어져있습니다 :&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;인코더는 입력 시퀀스를 처리하여 고정된 길이의 컨텍스트 벡터(context vector, sentence embedding 또는 thought vector로도 알려진)로 정보를 압축합니다. 이러한 차원 축소된 벡터 표현은 소스 시퀀스의 문맥적인 요약 정보로 간주할수 있습니다.&lt;/li&gt;
  &lt;li&gt;디코더는 컨텍스트 벡터를 다시 처리하여 결과값을 만들어 냅니다. 인코더 네트워크의 결과값을 입력으로 받아 변형을 수행합니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;인코더와 디코더 모두 &lt;a href=&quot;http://colah.github.io/posts/2015-08-Understanding-LSTMs/&quot;&gt;LSTM이나 GRU&lt;/a&gt; 같은 Recurrent Neural Networks 구조를 사용합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-10-13/encoder-decoder-example.png&quot; width=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small&gt;&lt;em&gt;그림3. 인코더-디코더 모델, she is eating a green apple 이란 문장을 중국어로 변형함. 순차적인 방식으로 풀어서 시각화함&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;고정된 길이의 컨텍스트 벡터로 디자인하는 것의 문제점은 아주 긴 문장의 경우, 모든 정보를 다 기억하지 못한다 것입니다. 일단 전체 문장을 모두 처리하고 나면 종종 앞 부분을 잊어버리곤 합니다. 어텐션 메커니즘은 이 문제점을 해결하기 위해 제안되었습니다. (&lt;a href=&quot;https://arxiv.org/pdf/1409.0473.pdf&quot;&gt;Bahdanau et al., 2015&lt;/a&gt;)&lt;/p&gt;

&lt;h2 id=&quot;born-for-translation&quot;&gt;Born for Translation&lt;/h2&gt;
&lt;p&gt;어텐션 메커니즘은 딥러닝 기반의 기계번역(&lt;a href=&quot;https://arxiv.org/pdf/1409.0473.pdf&quot;&gt;NMT&lt;/a&gt;)에서 긴 소스 문장을 기억하기 위해서 만들어졌습니다. 인코더의 마지막 히든 스테이트의 컨텍스트 벡터뿐만아니라, 어텐션을 이용해 컨텍스트 벡터와 전체 소스 문장 사이에 지름길(shortcuts)을 만들어 사용하는 것입니다. 이 지름길의 가중치들은 각 아웃풋 요소들에 맞게 정의할 수 있습니다.&lt;/p&gt;

&lt;p&gt;컨텍스트 벡터는 전체 입력 시퀀스에 접근할수 있고, 잊어 버릴 염려가 없습니다. 소스와 타겟 간의 관계은 컨텍스트 벡터에 의해 학습되고 제어됩니다. 기본적으로 컨텍스트 벡터는 세가지 정보를 사용합니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;인코더 히든 스테이트&lt;/li&gt;
  &lt;li&gt;디코더 히든 스테이트&lt;/li&gt;
  &lt;li&gt;소스와 타겟 사이의 순차적 정보(alignment)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-10-13/encoder-decoder-attention.png&quot; width=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small&gt;&lt;em&gt;그림4. additive attention mechanism이 있는 인코더-디코더 모델 &lt;a href=&quot;https://arxiv.org/pdf/1409.0473.pdf&quot;&gt;Bahdanau et al., 2015&lt;/a&gt;&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;h3 id=&quot;definition&quot;&gt;Definition&lt;/h3&gt;
&lt;p&gt;NMT에서 사용되는 어텐션 메커니즘을 과학적으로 정의해보도록 하겠습니다. 우리는 길이가 &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt;인 소스 문장 &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt;를 이용해 길이가 &lt;script type=&quot;math/tex&quot;&gt;m&lt;/script&gt;인 타겟 문장 &lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt;을 만든다고 해봅시다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbf{x} = [x_1, x_2, ..., x_n] \\
\mathbf{y} = [y_1, y_2, ..., y_m]&lt;/script&gt;

&lt;p&gt;(볼드 표시된 변수는 벡터를 의미합니다. 이하의 모든 내용에 적용됩니다)&lt;/p&gt;

&lt;p&gt;인코더는 &lt;a href=&quot;https://www.coursera.org/lecture/nlp-sequence-models/bidirectional-rnn-fyXnn&quot;&gt;bidirectional RNN&lt;/a&gt;(또는 다른 구조의 RNN를 갖을 수 있습니다)로 히든 스테이트 &lt;script type=&quot;math/tex&quot;&gt;\mathbf{\overrightarrow{h_i}}&lt;/script&gt; 와 반대방향 히든 스테이트 &lt;script type=&quot;math/tex&quot;&gt;\mathbf{\overleftarrow{h_i}}&lt;/script&gt;를 갖습니다. 두 표현식을 간단히 연결(concatenation)하여 인코더의 히든 스테이트를 나타냅니다. 이렇게 하여 한 단어의 앞 뒷 단어를 표시할수 있습니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbf{h_i} = [\mathbf{\overrightarrow{h_i}}^\top; \mathbf{\overrightarrow{h_i}}^\top]^\top, \ i=1, ..., n&lt;/script&gt;

&lt;p&gt;디코더의 히든 스테이트는 t번째 아웃풋 단어를 만들기 위해 &lt;script type=&quot;math/tex&quot;&gt;s_t = f(s_{t-1}, y_{t-1}, c_t)&lt;/script&gt; 로 정의됩니다. 이때, &lt;script type=&quot;math/tex&quot;&gt;c_t&lt;/script&gt;(context vector)는 어라인먼트 스코어를 가중치로 갖는 인코더 히든스테이트의 가중 합계입니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
\mathbf{c_t} &amp; = \sum_{i=1}^{n}\alpha_{t, i} \mathbf{h_i} &amp; ; \ Context \ vector \ for \ output \ y_t \\\\
\alpha_{t,i} &amp; = align(y_t, x_i) &amp; ; \ How \ well \ two \ words \ y_t \ and \ x_i \ are \ aligned. \\\\
 &amp; = \frac{score(s_{t-1}, \mathbf{h_{i^{'}}})}{\sum_{i=1}^{n} score(s_{t-1},\mathbf{h_{i^{'}}})} &amp; ; \ Softmax \ of \ some \ predefined \ alignment \ score. &amp;
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;alignment model은 i번째 입력과 t번째 결과값이 얼마나 잘 매치되는지 확인 한 후  스코어 &lt;script type=&quot;math/tex&quot;&gt;\alpha_{t, i}&lt;/script&gt;를 이 쌍 &lt;script type=&quot;math/tex&quot;&gt;(y_t, x_i)&lt;/script&gt;에 할당합니다. &lt;script type=&quot;math/tex&quot;&gt;{\alpha_{t,i}}&lt;/script&gt;의 집합은 각 소스의 히든 스테이트가 결과값에 어느정도 연관되어 있는지를 정의하는 가중치 입니다. Bahdanau의 논문은 alignment score &lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt;는 한개의 히든 레이어를 가진 &lt;b&gt;feed-forward network&lt;/b&gt;로 파라미터라이즈됩니다. 그리고 이 네트워크는 모델의 다른 부분들과 함께 학습됩니다. 스코어 함수는 아래와 같은 형태이고, tanh는 비선형 활성함수로 사용되었습니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;score(\mathbf{s_t}, \mathbf{h_i}) = \mathbf{v_a^\top} tanh(\mathbf{W_a}[\mathbf{s_t} ; \mathbf{h_i}])&lt;/script&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\mathbf{v_a}&lt;/script&gt; 와 &lt;script type=&quot;math/tex&quot;&gt;\mathbf{W_a}&lt;/script&gt;는 alignment model에서 학습되는 가중치 메트릭스입니다.&lt;/p&gt;

&lt;p&gt;alignment score를 메트릭스로 표시하여 시각적으로 소스 단어와 타겟 단어 사이의 상관관계를 명시적으로 확인할수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-10-13/bahdanau-fig3.png&quot; width=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small&gt;&lt;em&gt;그림5. 프랑스어 “L’accord sur l’Espace économique européen a été signé en août 1992”와 영어 “The agreement on the European Economic Area was signed in August 1992”의 기계번역 모델의 Alignment matrix입니다. (출저 : Fig 3 in &lt;a href=&quot;https://arxiv.org/pdf/1409.0473.pdf&quot;&gt;Bahdanau et al., 2015&lt;/a&gt;)&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;구현 방법은 텐서플로우팀의 &lt;a href=&quot;https://www.tensorflow.org/versions/master/tutorials/seq2seq&quot;&gt;튜토리얼&lt;/a&gt;을 확인하세요.&lt;/p&gt;

&lt;h2 id=&quot;a-family-of-attention-mechanisms&quot;&gt;A Family of Attention Mechanisms&lt;/h2&gt;

&lt;p&gt;어텐션으로 인해서 소스와 타겟 시퀀스간의 의존성은 더이상 둘 간의 거리에 의해 제한되지 않습니다. 어텐션은 기계 번역에서 큰 성과를 보였고, 곧 컴퓨터 비전 분야로 확대되었으며(&lt;a href=&quot;http://proceedings.mlr.press/v37/xuc15.pdf&quot;&gt;Xu et al. 2015&lt;/a&gt;) 다양한 어텐션 메커니즘이 연구되기 시작했습니다.(&lt;a href=&quot;https://arxiv.org/pdf/1508.04025.pdf&quot;&gt;Luong, et al., 2015&lt;/a&gt;;&lt;a href=&quot;https://arxiv.org/abs/1703.03906&quot;&gt;Britz et al., 2017&lt;/a&gt;;&lt;a href=&quot;http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf&quot;&gt;Vaswani, et al., 2017&lt;/a&gt;)&lt;/p&gt;

&lt;h3 id=&quot;summary&quot;&gt;Summary&lt;/h3&gt;

&lt;p&gt;아래는 대표적인 어텐션 메커니즘의 요약 정보입니다(또는 어텐션 메커니즘의 대략적인 분류).&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Name&lt;/th&gt;
      &lt;th&gt;Aligment socre function&lt;/th&gt;
      &lt;th&gt;citation&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Additive(*)&lt;/td&gt;
      &lt;td&gt;&lt;script type=&quot;math/tex&quot;&gt;score(\mathbf{s}_t&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;\mathbf{h}_i&lt;/script&gt;) = &lt;script type=&quot;math/tex&quot;&gt;\mathbf{v}_a^\top tanh(\mathbf{W}_a[\mathbf{s}_t; \mathbf{h}_i]&lt;/script&gt;)&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://arxiv.org/pdf/1409.0473.pdf&quot;&gt;Bahdanau2015&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Location-Base&lt;/td&gt;
      &lt;td&gt;&lt;script type=&quot;math/tex&quot;&gt;\alpha_{t,i} = softmax(\mathbf{W}_a \mathbf{s}_t)&lt;/script&gt; &lt;br /&gt; Note : This simplifies the softmax alignment max to only depend on the target position.&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://arxiv.org/pdf/1508.04025.pdf&quot;&gt;Luong2015&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;General&lt;/td&gt;
      &lt;td&gt;&lt;script type=&quot;math/tex&quot;&gt;score(\mathbf{s}_t, \mathbf{h}_i)=\mathbf{s}_t^\top \mathbf{W}_a \mathbf{h}_i&lt;/script&gt; &lt;br /&gt; where &lt;script type=&quot;math/tex&quot;&gt;\mathbf{W}_a&lt;/script&gt; is a trainable weight matrix in the attention layer.&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://arxiv.org/pdf/1508.04025.pdf&quot;&gt;Luong2015&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Dot-Product&lt;/td&gt;
      &lt;td&gt;&lt;script type=&quot;math/tex&quot;&gt;score(\mathbf{s}_t, \mathbf{h}_i) = \mathbf{s}_t^\top \mathbf{h}_i&lt;/script&gt;&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://arxiv.org/pdf/1508.04025.pdf&quot;&gt;Luong2015&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Scaled Dot-Product(^)&lt;/td&gt;
      &lt;td&gt;&lt;script type=&quot;math/tex&quot;&gt;score(\mathbf{s}_t, \mathbf{h}_i) =&lt;/script&gt;  &lt;script type=&quot;math/tex&quot;&gt;{\mathbf{s}_t^\top \mathbf{h}_i}\over{\sqrt{n}}&lt;/script&gt; &lt;br /&gt; Note: very similar to dot-product attention except for a scaling factor; where n is the dimension of the source hidden state.&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf&quot;&gt;Vaswani2017&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Self-Attention(&amp;amp;)&lt;/td&gt;
      &lt;td&gt;Retating different position of the same input sequence. Theoretically the self-attention can adopt any score functions above, but just replace the target sequence with the same input sequence.&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://arxiv.org/pdf/1601.06733.pdf&quot;&gt;Cheng2016&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Global/Soft&lt;/td&gt;
      &lt;td&gt;Attending to the entire input state space.&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;http://proceedings.mlr.press/v37/xuc15.pdf&quot;&gt;Xu2015&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Local/Hard&lt;/td&gt;
      &lt;td&gt;Attending to the part of input state space; i.e. a patch of the input image.&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;http://proceedings.mlr.press/v37/xuc15.pdf&quot;&gt;Xu2015&lt;/a&gt;;&lt;a href=&quot;https://arxiv.org/pdf/1508.04025.pdf&quot;&gt;Luong2015&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;(*) 이 방식은 Luong, et al., 2015 에서는 “concat”이라고 언급되었으며, Vaswani, et al., 2017에서는 “additive attention”이라고 언급되었습니다.&lt;/p&gt;

&lt;p&gt;(^)인풋이 매우 길어서 소프트맥스 함수의 그래디언트가 아주 작아져 학습이 어려운 경우를 보완하기 위해서 스케일링 펙터, &lt;script type=&quot;math/tex&quot;&gt;1/\sqrt{n}&lt;/script&gt;,가 더해진 것입니다.&lt;/p&gt;

&lt;p&gt;(&amp;amp;) Cheng et al., 2016 등 다른 논문들에서는 intra-attention이라고도 불리웁니다.&lt;/p&gt;

&lt;h3 id=&quot;self-attention&quot;&gt;Self-Attention&lt;/h3&gt;

&lt;p&gt;&lt;b&gt;Self-attetion&lt;/b&gt;, 또는 &lt;b&gt;intra-attention &lt;/b&gt;으로 알려진 어텐션 메커니즘은 시퀀스의 representation을 계산하기 위해 시퀀스의 서로 다른 포지션과 연관된 방법입니다. 기계 판독, 추상 요약 또는 이미지 설명 생성에 매우 유용합니다.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1601.06733.pdf&quot;&gt;long short-term memory network&lt;/a&gt; 논문에서 기계판독 문제를 해결하기위해 셀프어텐션 기법을 사용하였습니다. 아래 예제와 같이 셀프 어텐션 메커니즘을 통해 현재 단어와 이전 단어들간의 상관관계를 학습할수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-10-13/cheng2016-fig1.png&quot; width=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small&gt;&lt;em&gt;그림6. 현재 단어는 빨간색으로 표시하였고, 파란색 그림자의 크기는 엑티베이션 정도를 나타남(출저 : &lt;a href=&quot;https://arxiv.org/pdf/1601.06733.pdf&quot;&gt;Cheng et al., 2016&lt;/a&gt;)&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://proceedings.mlr.press/v37/xuc15.pdf&quot;&gt;show, attend and tell&lt;/a&gt; 논문에서는 셀프어텐션을 이미지에 적용하여 적절한 설명 문구을 생성하였습니다. 이미지는 먼저 컨볼루션 뉴럴 넷을 이용해 인코딩되었고, 인코딩된 피쳐 멥을 인풋으로하는 리커런트 네트워크(셀프 어텐션이 적용된)를 이용해 묘사하는 단어를 하나 하나 생성하였습니다. 어텐션 가중치를 시각화한 결과, 모델이 특정 단어를 생성할 때 이미지에서 어떤 영역을 중점으로 반영하는지 확인할 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-10-13/xu2015-fig6b.png&quot; width=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small&gt;&lt;em&gt;그림7. “A woman is throwing a frisbee in a park.” (Image source: Fig. 6(b) in &lt;a href=&quot;http://proceedings.mlr.press/v37/xuc15.pdf&quot;&gt;Xu et al. 2015&lt;/a&gt;)&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;h3 id=&quot;soft-vs-hard-attention&quot;&gt;Soft vs Hard Attention&lt;/h3&gt;
&lt;p&gt;어텐션의 또 다른 정의 방식은 soft와 hard 어텐션입니다. 기본적인 아이디어는 &lt;a href=&quot;http://proceedings.mlr.press/v37/xuc15.pdf&quot;&gt;show, attend and tell&lt;/a&gt; 논문에서 제안되었습니다. 어텐션이 전체 이미지를 대상으로하는지 혹은 일부 패치 영역을 대상으로 하는지에 따라 :&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;soft attention : 가중치가 학습되어, 소스 이미지의 모든 패치에 “소프트하게” 맵핑됨; 근본적으로 &lt;a href=&quot;https://arxiv.org/pdf/1409.0473.pdf&quot;&gt;Bahdanau et al., 2015&lt;/a&gt;와 유사함
    &lt;ul&gt;
      &lt;li&gt;장점 : 모델이 스무스하고 미분가능함&lt;/li&gt;
      &lt;li&gt;단점 : 소스 이미지가 클 때 계산비용이 큼&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;hard attention : 이미지의 일부 패치영역이 한번에 하나씩 선택되는 방식
    &lt;ul&gt;
      &lt;li&gt;장점 : 인퍼런스에서 더 적은 계산 비용&lt;/li&gt;
      &lt;li&gt;단점 : 모델이 미분불가능하고, 학습 시 variance reduction이나 reinforcement learning같은 더 복잡한 기법들이 필요함 (&lt;a href=&quot;https://arxiv.org/pdf/1508.04025.pdf&quot;&gt;Luong et al., 2015&lt;/a&gt;)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;global-vs-local-attention&quot;&gt;Global vs Local Attention&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1508.04025.pdf&quot;&gt;Luong et al., 2015&lt;/a&gt;)에서는 global과 local 어텐션을 제안하였습니다. 글로벌 어텐션은 소프트 어텐션과 유사하고, 로컬 어텐션은 하드와 소프트 개념이 모두 이용해 미분가능하도록 만든 하드 어텐션이라고 생각할수 있습니다. 현재 타겟 단어를 위해 한개의 포지션을 예측하고 소스 포지션 주위로 센터된 윈도우을 이용해 컨텍스트 벡터를 계산합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-10-13/luong2015-fig2-3.png&quot; width=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small&gt;&lt;em&gt;그림8. “글로벌 vs 로컬 어텐션” (Image source: Fig 2 &amp;amp; 3 in &lt;a href=&quot;https://arxiv.org/pdf/1508.04025.pdf&quot;&gt;Luong et al., 2015&lt;/a&gt;)&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;h2 id=&quot;transformer&quot;&gt;Transformer&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf&quot;&gt;“Attention is All you Need”&lt;/a&gt;(Vaswani, et al., 2017), 는 2017년 논문중에서 가장 임팩트있고 흥미로운 논문입니다. 기존 소프트 어텐션 방식을 대폭 개선시키고 &lt;em&gt;recurrent network units없이&lt;/em&gt; seq2seq를 모델링할수 있다는 것을 보였습니다. &lt;b&gt;transformer&lt;/b&gt;라는 것을 제안하여 순차적인 계산 구조 없이 셀프 어텐션 메커니즘을 구현할수 있습니다.&lt;/p&gt;

&lt;p&gt;핵심은 바로 모델 구조에 있습니다.&lt;/p&gt;

&lt;h3 id=&quot;key-value-and-query&quot;&gt;key, Value and Query&lt;/h3&gt;
&lt;p&gt;가장 중요한 부분은 &lt;em&gt;multi-head self-attention mechanism&lt;/em&gt;입니다. 트랜스포머는 인풋의 인코딩된 representation을 &lt;b&gt;key-value&lt;/b&gt; 쌍, &lt;script type=&quot;math/tex&quot;&gt;(\mathbf{K, V})&lt;/script&gt;의 집합체로 보았습니다; 둘다 n(인풋 시퀀스 길이)차원 벡터로 인코더의 히든 스테이트에 해당. 디코더에서 이전 결과값들은 &lt;b&gt;query&lt;/b&gt;(&lt;script type=&quot;math/tex&quot;&gt;\mathbf{Q}&lt;/script&gt; of dimension m)로 압축되고, 다음 아웃풋은 이 쿼리와 키-벨류 셋트를 맵핑함으로써 계산됩니다.&lt;/p&gt;

&lt;p&gt;트렌스포머는 &lt;a href=&quot;&quot;&gt;scaled dot-product attention&lt;/a&gt;을 사용하였습니다: 아웃풋은 가중합산된 값이고, 가중치들은 쿼리와 키값들의 dot-product로 결정됩니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Attention(\mathbf{Q, K, V}) = softmax( {\mathbf{Q}\mathbf{K}^\top \over {\sqrt{n}}} )\mathbf{V}&lt;/script&gt;

&lt;h3 id=&quot;multi-head-self-attention&quot;&gt;multi-Head Self-Attention&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-10-13/multi-head-attention.png&quot; width=&quot;300&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small&gt;&lt;em&gt;그림9. 멀티-헤드 스케일드 닷-프로덕트 어텐션 메커니즘 (Image source: Fig 2 in &lt;a href=&quot;http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf&quot;&gt;Vaswani, et al., 2017&lt;/a&gt;)&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;어텐션을 한번만 계산하는 것보다 멀티-헤드 메커니즘은 스케일 닷-프로덕트 어텐션을 병렬로 여러번 계산된다. 독립적인 어텐션 아웃풋은 단순히 concatenated되며, 선형으로 예상되는 차원으로 변형됩니다. 이렇게 하는 이유는 앙상블은 항상 도움이 되기 때문이 아닐까요? 논문에 따르면 “multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this (멀티-헤드 어텐션은 서로 다른 representation 공간에 있는 포지션 정보를 결합하여 이용할수 있게 해줍니다. 싱글 어텐션 헤드를 이용하면 이런 정보들이 서로 평균화되어 버립니다.)&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;MultiHead(\mathbf{Q, K, V}) = [head_1; ... ; head_h]\mathbf{W}^O \\
where \ head_i = Attenton(\mathbf{QW}_i^Q, \mathbf{KW}_i^K, \mathbf{VW}_i^V)&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;\mathbf{W}_i^Q, \mathbf{W}_i^K, \mathbf{W}_i^V&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\mathbf{W}^O&lt;/script&gt; are parameter matrics to be learned.&lt;/p&gt;

&lt;h3 id=&quot;encoder&quot;&gt;Encoder&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-10-13/transformer-encoder.png&quot; width=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small&gt;&lt;em&gt;그림10. 트랜스포머의 인코더 (Image source: &lt;a href=&quot;http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf&quot;&gt;Vaswani, et al., 2017&lt;/a&gt;)&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;인코더는 무한히 클수있는 문백에서 특정 정보 조각을 찾을수 있도록 어텐션 기반의 representation을 생성합니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;동일한 6개의 레이어를 쌓습니다.&lt;/li&gt;
  &lt;li&gt;각 레이어는 멀티-헤드 셀프어텐션 레이어와 포지션-와이즈 풀리 커넥티드 피드-포워드 네트워크를 서브 레이어로 갖습니다.&lt;/li&gt;
  &lt;li&gt;각 서브 레이어는 &lt;a href=&quot;https://arxiv.org/pdf/1512.03385.pdf&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;residual&lt;/code&gt;&lt;/a&gt; 커넥션과 &lt;code class=&quot;highlighter-rouge&quot;&gt;layer normalization&lt;/code&gt; 이 적용됩니다. 모든 서브 레이어는 &lt;script type=&quot;math/tex&quot;&gt;d_{model}=512&lt;/script&gt;로 동일한 차원의 아웃풋을 갖습니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;decoder&quot;&gt;Decoder&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-10-13/transformer-decoder.png&quot; width=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small&gt;&lt;em&gt;그림11. 트랜스포머의 디코더 (Image source: &lt;a href=&quot;http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf&quot;&gt;Vaswani, et al., 2017&lt;/a&gt;)&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;디코더는 인코딩된 representation으로부터 정보를 다시 되돌리는 역할을 합니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;동일한 6개의 레이어를 쌓습니다.&lt;/li&gt;
  &lt;li&gt;각 레이어는 멀티-헤드 셀프어텐션 레이어와 포지션-와이즈 풀리 커넥티드 피드-포워드 네트워크를 서브 레이어로 갖습니다.&lt;/li&gt;
  &lt;li&gt;인코더와 유사하게 각 서브 레이어는 residual 커넥션과 레이어 노말리제이션이 적용됩니다.&lt;/li&gt;
  &lt;li&gt;첫번째 서브레이어의 멀티-헤드 어텐션은 타겟 시퀀스의 미래을 보는 것은 관심이 없으므로, 현재 위치 이후의 포지션 정보는 이용하지 않도록 변형됩니다. (현재 포지션의 이전 정보만 이용하도록)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;full-architecture&quot;&gt;Full Architecture&lt;/h3&gt;

&lt;p&gt;트렌스포머의 전체적인 구조는 다음과 같습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;먼저 소스와 타겟 시퀀스 모두 동일한 디멘션 &lt;script type=&quot;math/tex&quot;&gt;d_{model} = 512&lt;/script&gt;을 갖도록 임베딩 레이어를 거칩니다.&lt;/li&gt;
  &lt;li&gt;포지션 정보를 유지하기 위해 sinusoid-wave-based positional encoding을 적용한 후 임베딩 아웃풋과 합칩니다.&lt;/li&gt;
  &lt;li&gt;마지막 디코더 아웃풋에 소프트맥스와 선형 레이어가 추가됩니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-10-13/transformer.png&quot; width=&quot;600&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small&gt;&lt;em&gt;그림12. 트랜스포머의 전체 모델 구조 (Image source: Fig 1&amp;amp; 2 in &lt;a href=&quot;http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf&quot;&gt;Vaswani, et al., 2017&lt;/a&gt;)&lt;/em&gt;&lt;/small&gt;&amp;gt;&lt;/p&gt;

&lt;h2 id=&quot;snail&quot;&gt;SNAIL&lt;/h2&gt;

&lt;p&gt;트랜스포머는 리커런트 또는 컨볼루션 구조를 사용하지 않고, 임베딩 벡터에 포지션 인코딩이 더해지긴 하지만 시퀀스의 순서는 약하게 통합되는 수준입니다. &lt;a href=&quot;https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html&quot;&gt;강화 학습&lt;/a&gt;과 같이 위치 종속성에 민감한 경우, 큰 문제가 될 수 있습니다. 
&lt;b&gt;Simple Neural Attention &lt;a href=&quot;http://bair.berkeley.edu/blog/2017/07/18/learning-to-learn/&quot;&gt;Meta-Learner&lt;/a&gt;(SNAIL)&lt;/b&gt;&lt;a href=&quot;http://metalearning.ml/papers/metalearn17_mishra.pdf&quot;&gt;Mishra et al., 2017&lt;/a&gt;는 트랜스포머의 셀프-어텐션 메커니즘과 &lt;a href=&quot;https://deepmind.com/blog/wavenet-generative-model-raw-audio/&quot;&gt;시간적 컨볼루션&lt;/a&gt;을 결합하여 &lt;a href=&quot;#full-architecture&quot;&gt;포지션 문제&lt;/a&gt;를 부분적으로 개선하기 위해 제안되었습니다. SNAIL은 지도학습과 강화학습 모두에서 좋은 결과를 보입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-10-13/snail.png&quot; width=&quot;600&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small&gt;&lt;em&gt;그림13. SNAIL 모델 구조 (Image source: &lt;a href=&quot;http://metalearning.ml/papers/metalearn17_mishra.pdf&quot;&gt;Mishra et al., 2017&lt;/a&gt;)&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;SNAIL은 그 자체만으로도 중요한 토픽인 메타-러닝 분야에서 최초 제안되었습니다. 간단히 말해서 메타 러닝 모델은 비슷한 분포에서 nevel, unseen tasks들에 일반화할수 있습니다. 더 자세한 정보는 &lt;a href=&quot;http://bair.berkeley.edu/blog/2017/07/18/learning-to-learn/&quot;&gt;이 글&lt;/a&gt;을 확인하세요.&lt;/p&gt;

&lt;h2 id=&quot;self-attention-gan&quot;&gt;Self-Attention GAN&lt;/h2&gt;
&lt;p&gt;마지막으로 &lt;a href=&quot;https://lilianweng.github.io/lil-log/2017/08/20/from-GAN-to-WGAN.html&quot;&gt;Generative Adversarial Network (GAN)&lt;/a&gt;타입의 모델인, `self-attention GAN(SAGAN; &lt;a href=&quot;https://arxiv.org/pdf/1805.08318.pdf&quot;&gt;Zhang et al., 2018&lt;/a&gt;)을 통해서 어텐션이 생성이미지의 퀄리티를 향상시키는지 설명하도록 하겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1511.06434&quot;&gt;DCGAN&lt;/a&gt;(Deep Convolutional GAN)에서 discriminator와 generator은 멀티-레이어 컨볼루션 네트워크입니다. 하지만 하나의 픽셀은 작은 로컬 영역으로 제한되기 때무네, 네트워크의 representation capacity는 필터 사이즈에 의해 제한됩니다. 멀리 떨어진 영역을 연결하기 위해서 피쳐들이 컨볼루션 오퍼레이션을 통해 희석되어야하여 종속성이 유지되는 것이 보장되지 않습니다.&lt;/p&gt;

&lt;p&gt;비전 컨텍스트에서 (소프트) 셀프-어텐션은 한 픽셀과 다른 포지션의 픽셀들간에 관계를 명시적으로 학습하도록 설계되어 있습니다. 멀리 떨어진 영역이더라도 쉽게 글로벌 디펜던시를 학습할수 있습니다. 따라서 셀프-어텐션이 적용된 GAN은 디테일한 정보를 더 잘 처리할수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-10-13/conv-vs-self-attention.png&quot; width=&quot;600&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small&gt;*그림14. 컨볼루션 오퍼레이션과 셀프-어텐션은 서로 다른 사이즈의 영역을 다룹니다. *&lt;/small&gt;&amp;gt;&lt;/p&gt;

&lt;p&gt;SAGAN은 어텐션 계산을 위해서 &lt;a href=&quot;https://arxiv.org/pdf/1711.07971.pdf&quot;&gt;non-local neural network&lt;/a&gt;를 도입하였습니다. 컨볼루셔널 이미지 피쳐맵 &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt;는 3개로 복제되어 나눠지며, 이는 트랜스포머에서 각 각 &lt;a href=&quot;#key-value-and-query&quot;&gt;key, value, and query&lt;/a&gt; 개념에 대응됩니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Key : &lt;script type=&quot;math/tex&quot;&gt;f(x)=W_fx&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;Query : &lt;script type=&quot;math/tex&quot;&gt;g(x)=W_gx&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;Value : &lt;script type=&quot;math/tex&quot;&gt;h(x)=W_hx&lt;/script&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;그리고 나서 dot-product 어텐션을 셀프-어텐션 피쳐맵에 적용합니다 :&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\alpha_{i, j} = softmax(f(\mathbf{x}_i)^{\top}g(\mathbf{x}_j))\\
\mathbf{o}_j = \sum_{i=1}^{N} \alpha_{i,j}h(\mathbf{x}_i)&lt;/script&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-10-13/self-attention-gan-network.png&quot; width=&quot;600&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small&gt;*그림15. SAGAN에서 셀프-어텐션 메커니즘 (Image source : Fig 2 in &lt;a href=&quot;https://arxiv.org/pdf/1805.08318.pdf&quot;&gt;Zhang et al., 2018&lt;/a&gt;) *&lt;/small&gt;&amp;gt;&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\alpha_{i,j}&lt;/script&gt;는 j번째 위치를 합성할 때 모델이 i번째 위치에 얼마나 많은 주의를 기울여야하는지를 나타내는 어텐션 맵의 엔트리입니다. &lt;script type=&quot;math/tex&quot;&gt;\mathbf{W}_f, \mathbf{W}_g, \mathbf{W}_h&lt;/script&gt;는 1x1 컨볼루션 필터입니다. 만약 1x1 conv가 이상하다고 생각되면(단순히 피쳐맵 전체 값에 한개 값을 곱하는 것 아니냐?라고 생각한다면) 앤드류 응의 &lt;a href=&quot;https://www.youtube.com/watch?v=9EZVpLTPGz8&quot;&gt;튜토리얼&lt;/a&gt;을 보세요. 아웃풋 &lt;script type=&quot;math/tex&quot;&gt;\mathbf{o}_j&lt;/script&gt;는 마지막 아웃풋 &lt;script type=&quot;math/tex&quot;&gt;\mathbf{o} = (\mathbf{o}_1, \mathbf{o}_2, ..., \mathbf{o}_j, ..., \mathbf{o}_N)&lt;/script&gt;의 컬럼 벡터입니다.&lt;/p&gt;

&lt;p&gt;추가로 어텐션 레이어의 아웃풋에 스케일 파라미터를 곱하고, 오리지날 인풋 피쳐맵을 더해줍니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbf{y} = \mathbf{x}_i + \rho \mathbf{o}_i&lt;/script&gt;

&lt;p&gt;스케일링 파라미터 &lt;script type=&quot;math/tex&quot;&gt;\rho&lt;/script&gt;는 학습과정에서 0에서 점차 증가하고, 네트워크는 처음에는 로컬 영역에만 의존하다가 점차 멀리있는 영역에 더 많은 가중치를 주는 방법을 배우도록 구성됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-10-13/SAGAN-examples.png&quot; width=&quot;600&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small&gt;*그림16. SAGAN에 의해 생성된 이미지(128x128) 예들 (Image source : partial Fig 6 in &lt;a href=&quot;https://arxiv.org/pdf/1805.08318.pdf&quot;&gt;Zhang et al., 2018&lt;/a&gt;) *&lt;/small&gt;&amp;gt;&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;p&gt;[0] &lt;a href=&quot;https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html&quot;&gt;lilianweng의 Attention? Attention!&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[1] &lt;a href=&quot;http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/&quot;&gt;“Attention and Memory in Deep Learning and NLP.”&lt;/a&gt; - Jan 3, 2016 by Denny Britz&lt;/p&gt;

&lt;p&gt;[2] &lt;a href=&quot;https://www.tensorflow.org/versions/master/tutorials/seq2seq&quot;&gt;“Neural Machine Translation (seq2seq) Tutorial”&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[3] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. &lt;a href=&quot;https://arxiv.org/pdf/1409.0473.pdf&quot;&gt;“Neural machine translation by jointly learning to align and translate.”&lt;/a&gt; ICLR 2015.&lt;/p&gt;

&lt;p&gt;[4] Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich Zemel, and Yoshua Bengio. &lt;a href=&quot;http://proceedings.mlr.press/v37/xuc15.pdf&quot;&gt;“Show, attend and tell: Neural image caption generation with visual attention.”&lt;/a&gt; ICML, 2015.&lt;/p&gt;

&lt;p&gt;[5] Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. &lt;a href=&quot;https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf&quot;&gt;“Sequence to sequence learning with neural networks.”&lt;/a&gt;NIPS 2014.&lt;/p&gt;

&lt;p&gt;[6] Thang Luong, Hieu Pham, Christopher D. Manning. &lt;a href=&quot;https://arxiv.org/pdf/1508.04025.pdf&quot;&gt;“Effective Approaches to Attention-based Neural Machine Translation.”&lt;/a&gt; EMNLP 2015.&lt;/p&gt;

&lt;p&gt;[7] Denny Britz, Anna Goldie, Thang Luong, and Quoc Le. &lt;a href=&quot;https://arxiv.org/abs/1703.03906&quot;&gt;“Massive exploration of neural machine translation architectures.”&lt;/a&gt; ACL 2017.&lt;/p&gt;

&lt;p&gt;[8] Ashish Vaswani, et al. “Attention is all you need.” NIPS 2017. http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf&lt;/p&gt;

&lt;p&gt;[9] Jianpeng Cheng, Li Dong, and Mirella Lapata. &lt;a href=&quot;https://arxiv.org/pdf/1601.06733.pdf&quot;&gt;“Long short-term memory-networks for machine reading.”&lt;/a&gt; EMNLP 2016.&lt;/p&gt;

&lt;p&gt;[10] Xiaolong Wang, et al. &lt;a href=&quot;https://arxiv.org/pdf/1711.07971.pdf&quot;&gt;“Non-local Neural Networks.”&lt;/a&gt; CVPR 2018&lt;/p&gt;

&lt;p&gt;[11] Han Zhang, Ian Goodfellow, Dimitris Metaxas, and Augustus Odena. &lt;a href=&quot;https://arxiv.org/pdf/1805.08318.pdf&quot;&gt;“Self-Attention Generative Adversarial Networks.”&lt;/a&gt; arXiv preprint arXiv:1805.08318 (2018).&lt;/p&gt;

&lt;p&gt;[12] Nikhil Mishra, Mostafa Rohaninejad, Xi Chen, and Pieter Abbeel. &lt;a href=&quot;http://metalearning.ml/papers/metalearn17_mishra.pdf&quot;&gt;“A simple neural attentive meta-learner.”&lt;/a&gt; NIPS Workshop on Meta-Learning. 2017.&lt;/p&gt;

&lt;p&gt;[13] &lt;a href=&quot;https://deepmind.com/blog/wavenet-generative-model-raw-audio/&quot;&gt;“WaveNet: A Generative Model for Raw Audio”&lt;/a&gt; - Sep 8, 2016 by DeepMind.&lt;/p&gt;</content><author><name>yjucho</name></author><summary type="html">이 글은 lilianweng의 Attention? Attention! 포스팅을 번역한 글입니다.Attention은 최근 딥러닝 커뮤니티에서 자주 언급되는 유용한 툴입니다. 이 포스트에서는 어떻게 어텐션 개념과 다양한 어텐션 메커니즘을 설명하고 transformer와 SNAIL과 같은 모델들에 대해서 알아보고자 합니다.</summary></entry></feed>