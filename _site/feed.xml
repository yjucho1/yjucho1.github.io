<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.7.4">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2018-12-12T11:35:28+09:00</updated><id>http://localhost:4000/</id><title type="html">yjucho’s blog</title><subtitle>DATA, Deep Learning, AI</subtitle><author><name>yjucho</name></author><entry><title type="html">django를 이용한 대시보드 만들기</title><link href="http://localhost:4000/developement/django/" rel="alternate" type="text/html" title="django를 이용한 대시보드 만들기" /><published>2018-12-12T00:00:00+09:00</published><updated>2018-12-12T00:00:00+09:00</updated><id>http://localhost:4000/developement/django</id><content type="html" xml:base="http://localhost:4000/developement/django/">&lt;p&gt;django는 python 기반의 웹프레임워크로 비교적 쉽고 빠르게 웹어플리케이션을 제작할수 있도록 도와줍니다. django와 여러가지 오픈소스 라이브러리를 이용해 간단한 대시보드를 제작해보았습니다. 이 포스트에서는 1차 프로토타입을 소개하고, 사용한 라이브러리를 소개하도록 하겠습니다.&lt;/p&gt;

&lt;h3 id=&quot;missions&quot;&gt;Missions&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;데이터를 통한 인사이트 서비스를 제공하고 세상이 더 효율적으로 돌아가는데 기여하자&lt;br /&gt;
눈에 보이는 유형의 서비스로 만들자 &lt;br /&gt; 
빠르게 만들고, 피드백을 받아 수정하자&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-12-12/index.png&quot; width=&quot;600&quot; /&gt;
&lt;img src=&quot;/assets/img/2018-12-12/detail.png&quot; width=&quot;600&quot; /&gt;
&lt;b&gt;Live demo&lt;/b&gt; : &lt;a href=&quot;https://www.youtube.com/embed/mJcx6-VOzKs&quot;&gt;Youtue&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;related-tools-and-docs&quot;&gt;Related Tools and Docs&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;b&gt;django&lt;/b&gt; : &lt;a href=&quot;https://www.djangoproject.com/&quot;&gt;https://www.djangoproject.com/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Django is a high-level Python Web framework that encourages rapid development and clean, pragmatic design. 
Built by experienced developers, it takes care of much of the hassle of Web development, so you can focus on writing your app without needing to reinvent the wheel. 
It’s &lt;u&gt;free and open source&lt;/u&gt;.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;b&gt;bootstrap&lt;/b&gt; : &lt;a href=&quot;https://getbootstrap.com/&quot;&gt;https://getbootstrap.com/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Bootstrap is &lt;u&gt;an open source&lt;/u&gt; toolkit for developing with HTML, CSS, and JS. 
Quickly prototype your ideas or build your entire app with our Sass variables and mixins, responsive grid system, extensive prebuilt components, and powerful plugins built on jQuery.
Bootstrap is released under the MIT license and is copyright 2018 Twitter.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;b&gt;charts.js&lt;/b&gt; : &lt;a href=&quot;https://www.chartjs.org/&quot;&gt;https://www.chartjs.org/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Simple yet flexible JavaScript charting for designers &amp;amp; developers.
Chart.js is &lt;u&gt;open source&lt;/u&gt; and available under the MIT license.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;b&gt;leaflet.js&lt;/b&gt; : &lt;a href=&quot;https://leafletjs.com/&quot;&gt;https://leafletjs.com/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Leaflet is the leading &lt;u&gt;open-source&lt;/u&gt; JavaScript library for mobile-friendly interactive maps. Weighing just about 38 KB of JS, it has all the mapping features most developers ever need. As the code is published under the very permissive 2-clause BSD License. &lt;u&gt;Just make sure to attribute the use of the library somewhere in the app UI or the distribution&lt;/u&gt; (e.g. keep the Leaflet link on the map, or mention the use on the About page or a Readme file, etc.) and you’ll be fine.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;b&gt;git&lt;/b&gt; : &lt;a href=&quot;https://git-scm.com/&quot;&gt;https://git-scm.com/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Git is &lt;u&gt;a free and open source&lt;/u&gt; distributed version control system designed to handle everything from small to very large projects with speed and efficiency. The Git project chose to use GPLv2 to guarantee your freedom to share and change free software—to make sure the software is free for all its users.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;b&gt;공공데이터 포털&lt;/b&gt; : &lt;a href=&quot;https://www.data.go.kr/&quot;&gt;https://www.data.go.kr/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;공공데이터포털은 공공기관이 생성 또는 취득하여 관리하고 있는 공공데이터를 한 곳에서 제공하는 통합 창구입니다. 
포털에서는 국민이 쉽고 편리하게 공공데이터를 이용할 수 있도록 파일데이터, 오픈API, 시각화 등 다양한 방식으로 제공하고 있으며, 누구라도 쉽고 편리한 검색을 통해 원하는 공공데이터를 빠르고 정확하게 찾을 수 있습니다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;한국환경공단_측정소정보 조회 서비스 : https://www.data.go.kr/dataset/15000660/openapi.do&lt;/li&gt;
  &lt;li&gt;한국환경공단_대기오염정보 조회 서비스 : https://www.data.go.kr/dataset/15000581/openapi.do&lt;/li&gt;
&lt;/ul&gt;</content><author><name>yjucho</name></author><summary type="html">django는 python 기반의 웹프레임워크로 비교적 쉽고 빠르게 웹어플리케이션을 제작할수 있도록 도와줍니다. django와 여러가지 오픈소스 라이브러리를 이용해 간단한 대시보드를 제작해보았습니다. 이 포스트에서는 1차 프로토타입을 소개하고, 사용한 라이브러리를 소개하도록 하겠습니다.</summary></entry><entry><title type="html">Clustering and Unsupervised Anomaly Detection with l2 Normalized Deep Auto-Encoder Representations</title><link href="http://localhost:4000/clustering/clustering-with-l2-norm/" rel="alternate" type="text/html" title="Clustering and Unsupervised Anomaly Detection with l2 Normalized Deep Auto-Encoder Representations" /><published>2018-11-22T00:00:00+09:00</published><updated>2018-11-22T00:00:00+09:00</updated><id>http://localhost:4000/clustering/clustering-with-l2-norm</id><content type="html" xml:base="http://localhost:4000/clustering/clustering-with-l2-norm/">&lt;p&gt;&lt;b&gt; Caglar Aytekin, Xingyang Ni, Francesco Cricri and Emre Aksu (Nokia) 2017 &lt;/b&gt;&lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Recently, there are many works on learning deep unsupervised representations for clustering analysis.&lt;/li&gt;
  &lt;li&gt;Works rely on variants of auto-encoders and use encoder outputs as representation/features for cluster.&lt;/li&gt;
  &lt;li&gt;In this paper, l&lt;sub&gt;2&lt;/sub&gt; normalization constraint during auto-encoder training makes the representations more separable and compact in the Euclidean space.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;related-work&quot;&gt;Related Work&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;DEC : First, dense auto-encoder is trained with minimizing reconstruction error. Then, as clustering optimization state, minimizing the KL divergence between auto-encoder representation and an auxiliary target distribution.
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://github.com/yjucho1/articles/blob/master/DEC/readme.md&quot;&gt;DEC paper&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;IDEC : proposes to jointly optimize the clustering loss and reconstruction loss of the auto-encoder&lt;/li&gt;
  &lt;li&gt;DCEC : adopts a convolutional auto-encoder&lt;/li&gt;
  &lt;li&gt;GMVAE : adopts variational auto-encoder&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;proposed-method&quot;&gt;Proposed Method&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Clustering on l&lt;sub&gt;2&lt;/sub&gt; normalized deep auto-encoder representations&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L = \frac{1}{|J|} \sum_{j \in J} (I_j - D(E_c(I_j)))^2, \\
E_c(I) = \frac{E(I)}{\parallel E(I) \parallel _2}&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;after training auto-encoder with loss function, the clustering is simply performed by k-means algorithm.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Unsupervised Anomaly Detection using l&lt;sub&gt;2&lt;/sub&gt; normalized deep auto-encoder representations&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;v_i = max_j (E_c(I_i) \cdot \frac{C_j}{\parallel C_j \parallel _2} )&lt;/script&gt;

&lt;h2 id=&quot;experimental-result&quot;&gt;Experimental result&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;clustering : evaluation metrics - accuracy&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-11-22/dense-AE.png&quot; width=&quot;600&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-11-22/conv-AE.png&quot; width=&quot;600&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-11-22/comparison-norm.png&quot; width=&quot;600&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;comparision of normalization method : neither batch nor layer normalization provides a noticeable accuracy increase over CAE + k-means. Moreover in MNIST dataset, layer and batch normalization results into a significant accuracy decrease.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This is an important indicator showing that the performance upgrade of our method is not a result of a input conditioning, but it is a result of the specific normalization type that is more fit for clustering in Euclidean space.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;anomaly detection : evaluation metrics - AUC&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-11-22/anomaly-detection.png&quot; width=&quot;600&quot; /&gt;&lt;/p&gt;</content><author><name>yjucho</name></author><summary type="html">Caglar Aytekin, Xingyang Ni, Francesco Cricri and Emre Aksu (Nokia) 2017</summary></entry><entry><title type="html">Attention is All You Need</title><link href="http://localhost:4000/attention/attention-is-all-you-need/" rel="alternate" type="text/html" title="Attention is All You Need" /><published>2018-10-30T00:00:00+09:00</published><updated>2018-10-30T00:00:00+09:00</updated><id>http://localhost:4000/attention/attention-is-all-you-need</id><content type="html" xml:base="http://localhost:4000/attention/attention-is-all-you-need/">&lt;p&gt;&lt;b&gt; Ashish Vaswani et al. (Google Brain), 2017 &lt;/b&gt;&lt;/p&gt;

&lt;p&gt;Tensorflow implementtation :&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py&quot;&gt;https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/Kyubyong/transformer&quot;&gt;https://github.com/Kyubyong/transformer&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;PyTorch implementation : 
&lt;a href=&quot;https://github.com/jadore801120/attention-is-all-you-need-pytorch&quot;&gt;https://github.com/jadore801120/attention-is-all-you-need-pytorch&lt;/a&gt;
[guide annotating the paper with PyTorch implementation]
(http://nlp.seas.harvard.edu/2018/04/03/attention.html)&lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;언어 모델링과 기계 번역과 같은 시퀀스 모델링에서 RNN, LSTM, GRU는 최신 기법으로 확고하게 자리잡고 있습니다. 인코더-디코더 구조를 활용하는 등 향상된 성능을 얻기 위해 많은 시도들이 있어왔습니다.&lt;/p&gt;

&lt;p&gt;recurrent models에서 hidden state &lt;script type=&quot;math/tex&quot;&gt;h_t&lt;/script&gt;는 previus hidden state &lt;script type=&quot;math/tex&quot;&gt;h_{t-1}&lt;/script&gt;과 &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt;번째 입력값의 함수로 계선됩니다. 이러한 순차적 계산방식은 병렬처리가 어렵습니다. 입력 시퀀스가 길어질수록 이러한 제약 사항은 중요한 이슈가 됩니다.&lt;/p&gt;

&lt;p&gt;어텐션 메커니즘은 시퀀스 모델링에서 인풋과 아웃풋 시퀀스간 거리에 상관없이 의존성을 모델링할수 있는 방법으로 사용되었습니다. 하지만 기존 연구들은 recurrent netowork의 보완역할로만 어텐션 메커니즘을 사용하여 여전히 순차적 계산방식에 대한 제약이 남아있습니다.&lt;/p&gt;

&lt;p&gt;이 연구에서는 &lt;code class=&quot;highlighter-rouge&quot;&gt;&quot;Transformer&quot;&lt;/code&gt;라는 새로운 구조를 제안합니다. recurrence 구조를 탈피하고, 인풋과 아웃풋간의 글로벌 의존성을 모델링하는 어텐션 메커니즘만을 사용합니다. 이로 인해 병렬처리가 가능하고, P100 GPU 8장으로 12시간동안 학습시키는 것만으로도 state of art 수준의 번역 품질을 달성할수 있었습니다.&lt;/p&gt;

&lt;h2 id=&quot;backgroud&quot;&gt;Backgroud&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1610.08613&quot;&gt;Extended Neural GPU&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/1610.10099&quot;&gt;ByteNet&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/1705.03122&quot;&gt;ConvS2S&lt;/a&gt; 등은 컨볼루션 뉴럴 네트워크를 사용하여 병렬처리가 가능하도록 시퀀스 모델을 제안하였습니다. 하지만 인풋과 아웃풋 포지션의 거리가 멀어질수록 계산량도 이에 따라 증가하고, 멀리 떨어진 포지션 사이의 의존성을 학습하기가 더 어려워집니다.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;self-attention&lt;/code&gt;, 혹은 &lt;code class=&quot;highlighter-rouge&quot;&gt;intra-attention&lt;/code&gt;은 포지션을 고려하여 시퀀스의 representation을 계산하며, 이는 독해나 요약 등에서 다양하게 사용되고 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1503.08895&quot;&gt;End-to-end memory network&lt;/a&gt;는 시퀀스를 순차적으로 다루는 것이 아니라, 어텐션을 순차적으로 다루는 메커니즘으로 간단한 문답과 같은 언어 모델링에서 잘 작동하는 것으로 알려져 있습니다.&lt;/p&gt;

&lt;p&gt;(우리가 아는 한도에서는) Transformer는 RNN이나 CNN을 사용하지 않으면서도 인풋과 아웃풋의 표현력을 셀프-어텐션만으로 계산하는 최초의 접근법입니다.&lt;/p&gt;

&lt;h2 id=&quot;model-architecture&quot;&gt;Model Architecture&lt;/h2&gt;

&lt;p&gt;대부분의 시퀀스 변환 모델은 인코더-디코더 구조를 사용합니다. 인코더는 인풋 시퀀스 &lt;script type=&quot;math/tex&quot;&gt;(x_1, \cdots, x_n)&lt;/script&gt;를 &lt;script type=&quot;math/tex&quot;&gt;z=(z_1, \cdots, z_n)&lt;/script&gt;로 맵핑합니다. z가 주어지면, 디코더는 아웃풋 시퀀스 &lt;script type=&quot;math/tex&quot;&gt;(y_1 \cdots, y_m)&lt;/script&gt;를 한번에 하나씩 생성합니다. 매스텝마다 모델은 auto-regressive하게 다음단어를 생성할때마다 입력값과 이전에 생성된 심볼을 사용합니다.&lt;/p&gt;

&lt;p&gt;Transformer는 여러개의 셀프-어텐션을 쌓고, point-wise하게 계산하며, fully connected layer를 사용합니다.&lt;/p&gt;

&lt;h3 id=&quot;encoder-and-decoder-stacks&quot;&gt;Encoder and Decoder Stacks&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-10-30/fig1.png&quot; width=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small&gt;&lt;em&gt;그림1. The Transformer - model architecture&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Encoder&lt;/b&gt; : 인코더는 6개의 동일한 레이어로 구성됩니다. 각 레이어는 2개의 서브레이어를 갖습니다. 첫번째는 &lt;code class=&quot;highlighter-rouge&quot;&gt;multi-head self-attention mechanism&lt;/code&gt;, 두번째는 간단한 &lt;code class=&quot;highlighter-rouge&quot;&gt;position-wise fully connected feed-forward network&lt;/code&gt;입니다. 두 서브레이어 사이에는 &lt;code class=&quot;highlighter-rouge&quot;&gt;residual connection&lt;/code&gt;과 &lt;code class=&quot;highlighter-rouge&quot;&gt;layer normalization&lt;/code&gt;을 사용하였습니다. 즉 서브레이어의 아웃풋은 &lt;script type=&quot;math/tex&quot;&gt;LayerNorm(x+Sublayer(x))&lt;/script&gt;입니다. residual connection이 가능하도록 모델의 모든 레이어의 아웃풋 디멘전은 &lt;script type=&quot;math/tex&quot;&gt;d_{model}=512&lt;/script&gt;로 설정하였습니다.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Decoder&lt;/b&gt; : 디코더도 6개의 동일한 레이어로 구성됩니다. 마찬가지로 각 레이어는 위에서 언급한 2개의 서브레이어를 갖지만, 추가로 한가지가 더 있습니다. multi-head attention을 수정하여 생성하고자 하는 &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;번째 시퀀스가 후속 시퀀스에는 영향을 받지 않고, &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;보다 작은 위치의 아웃풋에만 의존할수 있도록 하였습니다(&lt;code class=&quot;highlighter-rouge&quot;&gt;Masked Multi-Head Attention&lt;/code&gt;).&lt;/p&gt;

&lt;h3 id=&quot;attention&quot;&gt;Attention&lt;/h3&gt;

&lt;p&gt;어텐션 함수는 Query, Key-Value 쌍을 아웃풋에 맵핑시키는 것입니다. query, key, valut, output은 모두 벡터입니다. 아웃풋은 value의 가중합이고, 이 때 가중치는 query와 값에 대응되는 key로 계산됩니다.&lt;/p&gt;

&lt;h4 id=&quot;scaled-dot-product-attention&quot;&gt;Scaled Dot-Product Attention&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-10-30/fig2.png&quot; width=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small&gt;&lt;em&gt;그림2. (왼쪽) Scaled Dot-Product Attention (오른쪽) Multi-head Attention은 병렬로 계산되는 여러개의 어텐션 레이어로 이뤄집니다.&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;여기서 사용한 어텐션 방식을 &lt;code class=&quot;highlighter-rouge&quot;&gt;&quot;Scaled Dot-Product Attention&quot;&lt;/code&gt;이라고 하겠습니다. 인풋은 &lt;script type=&quot;math/tex&quot;&gt;d_k&lt;/script&gt;차원의 query와 key, &lt;script type=&quot;math/tex&quot;&gt;d_v&lt;/script&gt;차원의 value입니다. query와 key를 dot-products한 후 &lt;script type=&quot;math/tex&quot;&gt;\sqrt{d_k}&lt;/script&gt;로 나누고, 소프트맥스함수를 취해 가중치를 구합니다.&lt;/p&gt;

&lt;p&gt;실제로는 여러개의 query 집합을 메트릭스 Q로 묶어 계산합니다. key와 value도 각 각 메트릭스 K와 V로 나타내어 메트릭스 형태의 아웃풋을 얻습니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Attention(Q, K, V) = softmax({QK^T\over{\sqrt{d_k}}})V&lt;/script&gt;

&lt;p&gt;널리 알려진 어텐션 함수는 &lt;code class=&quot;highlighter-rouge&quot;&gt;additive attention&lt;/code&gt;과 &lt;code class=&quot;highlighter-rouge&quot;&gt;dot-product(multiplicative) attention&lt;/code&gt;입니다. 여기서 사용한 알고리즘은 dot-product attention과 동일하지만, 스케일링을 위해 &lt;script type=&quot;math/tex&quot;&gt;1\over{\sqrt{d_k}}&lt;/script&gt;로 나눠주는 것을 추가하었습니다. additive attention은 싱글 히든 레이어로 이루어진 피드-포워드 네트워크를 사용하는 방식입니다. 두가지 모두 이론적인 계산복잡도는 유사하지만, dot-product attention이 최적화된 메트릭스 멀티플리케이션 코드으로 구현할수 있기때문에 더 빠르고 효율적입니다.&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;d_k&lt;/script&gt;가 작을 때는 두 메커니즘은 유사한 성능을 보이지만, &lt;script type=&quot;math/tex&quot;&gt;d_k&lt;/script&gt;가 클때는 additive attention이 스케일링이 없는 dot-product attention보다 더 우수한 성능을 보입니다. 우리는 &lt;script type=&quot;math/tex&quot;&gt;d_k&lt;/script&gt;가 클 때는 dot-product의 유효구간이 커지고, 이는 소프트맥스함수에서 그래디어트가 아주 작은 영역으로 가까워지게 하기 때문이라고 생각합니다.&lt;sup&gt;(*)&lt;/sup&gt; 이러한 효과를 줄이기 위해서 &lt;script type=&quot;math/tex&quot;&gt;1\over{\sqrt{d_k}}&lt;/script&gt;로 나눠주었습니다.&lt;/p&gt;

&lt;p&gt;&lt;small&gt;(*) dot-products의 유효구간이 커진다는 것을 설명하기 위해, q와 k가 평균 0이고 분산이 1인 독립적인 변수를 생각보겠습니다. q와 k의 dot-product는 &lt;script type=&quot;math/tex&quot;&gt;q \cdot k = \sum_{i=1}^{d_k}q_ik_i&lt;/script&gt;이고, 이는 평균이 0이고 분산은 &lt;script type=&quot;math/tex&quot;&gt;d_k&lt;/script&gt; 가 됩니다. &lt;/small&gt;&lt;/p&gt;

&lt;h4 id=&quot;multi-head-attention&quot;&gt;Multi-Head Attention&lt;/h4&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;d_{model}&lt;/script&gt; 차원의 keys, values, queries로 싱글 어텐션을 학습할수 있지만, 우리는 선형 프로젝션을 통해 h개의 어텐션을 이용하는 것이 더 효과적이라는 것을 발견하였습니다. 각 각의 프로젝션을 병렬로 계산하여, &lt;script type=&quot;math/tex&quot;&gt;d_v&lt;/script&gt;-차원의 아웃풋값을 얻고, h개의 아웃풋 값들은 concatenate한 후 다시 프로젝션하여 최종 값을 계산합니다.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;multi-head attention&lt;/code&gt;은 서로 다른 represenation subspace에서의 정보를 결합하여 사용하는 것입니다. 싱글 어텐션은 평균값으로 인해 이러한 정보들이 없어져버립니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;MultiHead(Q, K, V)=Concat(head_1, \cdots, head_h)W^O \\
where \ head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)&lt;/script&gt;

&lt;p&gt;프로젝션을 위한 파라미터는 &lt;script type=&quot;math/tex&quot;&gt;W_i^Q \in \mathbb{R}^{d_{model}\times d_k}&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;W_i^K \in \mathbb{R}^{d_{model}\times d_k}&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;W_i^V \in \mathbb{R}^{d_{model}\times d_v}&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;W_i^O \in \mathbb{R}^{hd_v\times d_{model}}&lt;/script&gt;입니다.&lt;/p&gt;

&lt;p&gt;실험에서 &lt;script type=&quot;math/tex&quot;&gt;h = 8&lt;/script&gt;의 병렬 어텐션 레이어를 사용하였습니다. 또한 &lt;script type=&quot;math/tex&quot;&gt;d_k = d_v =d_{model}/h = 64&lt;/script&gt; 사용하였습니다. 각 헤드의 차원이 줄어들었지만, 전체적인 계산비용은 full dimensionality와 유사합니다.&lt;/p&gt;

&lt;h4 id=&quot;application-of-attention-in-our-model&quot;&gt;Application of Attention in our Model&lt;/h4&gt;

&lt;p&gt;Transformer는 multi-head attention을 3가지 방식으로 사용합니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;인코더-디코더 어텐션 레이어에서 queries는 이전 디코더 레이어로부터 입력되고, keys-valus는 인코더 아웃풋으로부터 입력됩니다. 따라서 디코더가 시퀀스의 매 포지션을 생성할때마다, 인풋 시퀀스의 모든 포지션 정보를 이용할수 있습니다. 이는 sequence-to-sequence모델에서 인코더-디코더 어텐션 메커니즘을 그대로 차용한 것입니다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;인코더는 셀프-어텐션 레이어를 포함합니다. 셀프 어텐션 레이어의 keys, values, queries는 이전 레이어의 아웃풋입니다. 레이어의 포지션 정보는 이전 레이어가 생성한 모든 포지션 정보를 다 이용합니다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;유사하게 디코더도 셀프-어텐션 레이어를 포함하며, 디코더가 시퀀스의 매 포지션를 생성할때마다 그 위치까지의 모든 디코더 정보를 이용할수 있습니다. 다만 auto-regressive 속성을 유지하기 위해 소프트맥스의 인풋값 중 후속 포지션에 해당하는 값들은 모두 1로 마스킹합니다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;position-wise-feed-forward-networks&quot;&gt;Position-wise Feed-Forward Networks&lt;/h3&gt;

&lt;p&gt;어텐션 서브-레이어 이외에 인코더와 디코더 모두 &lt;code class=&quot;highlighter-rouge&quot;&gt;fully connected feed-forward network&lt;/code&gt;를 서브레이어로 갖고 있습니다. 각 포지션별로 동일하게 적용되는 네트워크로 ReLU활성함수과 선형변환으로 구성됩니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;FFN(x) = max(0, xW_1 + b_1)W_2 +b_2&lt;/script&gt;

&lt;p&gt;선형변화는 다른 포지션이더라도 동일한 선형 변환을 하지만, 레이어간에는 서로 다른 파라미터를 사용합니다. 이는 커널 사이즈가 1인 두개의 컨볼루션 오퍼레이션이라고도 생각할수 있습니다. 인풋과 아웃풋의 차원은 &lt;script type=&quot;math/tex&quot;&gt;d_{model}=512&lt;/script&gt;이고, 레이어 내부(W)는 &lt;script type=&quot;math/tex&quot;&gt;d_{ff} = 2048&lt;/script&gt;입니다.&lt;/p&gt;

&lt;h3 id=&quot;embedding-and-softmax&quot;&gt;Embedding and Softmax&lt;/h3&gt;

&lt;p&gt;다른 시퀀스 변환 모델과 유사하게, 인풋과 아웃풋 토큰을 &lt;script type=&quot;math/tex&quot;&gt;d_{model}&lt;/script&gt;차원의 벡터로 변환하는 임베딩을 학습합니다. 또한 디코더 아웃풋을 아웃풋 토큰 예측확률값으로 변환하기 위해 선형변환와 소프트맥스 함수를 사용하였습니다. 여기서는 두 임베딩 레이어와 소프트맥스앞의 선형변환에 대해서 모두 동일한 가중치 메트릭스를 사용하였습니다. 임베딩레이어에서는 가중치에 &lt;script type=&quot;math/tex&quot;&gt;\sqrt{d_{model}}&lt;/script&gt;를 곱하여 사용하였습니다.&lt;/p&gt;

&lt;h3 id=&quot;positional-encoding&quot;&gt;Positional Encoding&lt;/h3&gt;

&lt;p&gt;Transformer는 recurrence나 convolution을 사용하지 않기때문에, 시퀀스의 순서(order) 정보를 사용하기 위해서 시퀀스에서 토큰의 절대적인 위치나 상대적인 위치 정보를 강제로 입력해주어야 합니다. 따라서 포지셔널 인코딩을 인코더와 디코더의 가장 아래에 있는 인풋 임베딩 레이어에 추가하였습니다. 포지셔널 인코딩은 &lt;script type=&quot;math/tex&quot;&gt;d_{model}&lt;/script&gt;과 동일한 차원으로 임베딩 벡터와 sum할수 있도록 하였습니다. 여러 종류의 포지셔널 인코딩이 있습니다만, 여기서는 서로 다른 주기의 &lt;code class=&quot;highlighter-rouge&quot;&gt;sine&lt;/code&gt;과 &lt;code class=&quot;highlighter-rouge&quot;&gt;cosine&lt;/code&gt;함수를 사용하였습니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;PE_{(pos, 2i)} = sin(pos/10000^{2i/d_{model}}) \\
PE_{(pos, 2i+1)} = cos(pos/10000^{2i/d_{model}})&lt;/script&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;pos&lt;/script&gt;는 포지션이고, &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;는 디멘전입니다. 즉 포지셔널 인코딩의 각 차원은 sinusoid(사인모양의 파동)와 대응됩니다. 파장은 &lt;script type=&quot;math/tex&quot;&gt;2i&lt;/script&gt;에서 &lt;script type=&quot;math/tex&quot;&gt;10000\cdot 2i&lt;/script&gt;까지의 기하학적 진행을 의미합니다. 이 함수를 선택한 이유는 고정된 오프셋 &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt;에 대해서 &lt;script type=&quot;math/tex&quot;&gt;PE_{pos+k}&lt;/script&gt;는 &lt;script type=&quot;math/tex&quot;&gt;PE_{pos}&lt;/script&gt;의 선형변환으로 쉽게 표현될수 있기때문에 모델이 상대적인 포지션 정보를 쉽게 학습할수 있을것이라 가정했기 때문입니다.&lt;/p&gt;

&lt;p&gt;sinusoid 형태 외에 포지셔널 인코딩을 별도의 네트워크로 두고 학습하는 형태를 비교 실험해보았습니다. 그 결과, 두 버전 모두 거의 유사한 성능을 보였습니다(테이블3의 (E)). 최종적으로는 학습과정에서 마주친 것보다 더 긴 시퀀스에 대해서 extrapolate(외삽)할 수 있다는 점 때문에 sinusoid버전을 선택하였습니다.&lt;/p&gt;

&lt;h2 id=&quot;why-self-attention&quot;&gt;Why Self-Attention&lt;/h2&gt;

&lt;p&gt;이 섹션에서는 self-attention layer를 recurrent나 convolutional layer와 비교하여 설명하겠습니다. 모두 &lt;script type=&quot;math/tex&quot;&gt;(x_1, \cdots, x_n)&lt;/script&gt;의 시퀀스를 다른 시퀀스인 &lt;script type=&quot;math/tex&quot;&gt;(z_1, \cdots, z_n) \ with \ x_i, z_i \in \mathbb{R^d}&lt;/script&gt;로 맵핑하는데 일반적으로 사용되는 레이어들입니다. 이 논문에서 셀프-어텐션을 사용한 이유는 세가지 관점 때문입니다.&lt;/p&gt;

&lt;p&gt;첫번째는 레이어 당 계산 복잡도를 고려했기 때문이고, 두번째는 병렬계산할수 있는 총 계산량으로 필요한 순차적 오퍼레이션의 최소 수를 이용해 정량화하였습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-10-30/table1.png&quot; width=&quot;600&quot; /&gt;&lt;/p&gt;

&lt;p&gt;세번째는 네트워크에서 long-range dependencies간의 거리입니다. 멀리떨어진 단어들간의 의존성을 학습하는 것은 시퀀스 변환 문제에서 아주 중요한 이슈입니다. 이 이슈는 포워드와 백워드 시그널들이 네트워크에서 얼마나 이동할수 있는지에 영향을 받습니다. 인풋과 아웃풋 시퀀스들의 포지션 결합이 짧으면 짧을수록 장기 의존성은 쉽게 학습할수 있습니다. 따라서 우리는 네트워크 상에서 인풋과 아웃풋 포지션간의 최대 경로 길이를 측정하여 서로 다른 구조의 레이어를 비교 평가하였습니다.&lt;/p&gt;

&lt;p&gt;테이블1에서 볼수 있듯이 셀프 어텐션 레이어는 고정된 횟수만큼의 순차적인 오퍼레이션를 통해서 모든 포지션을 연결할수 있지만, recurrent layer는 &lt;script type=&quot;math/tex&quot;&gt;O(n)&lt;/script&gt;만큼의 순차적 오퍼레이션이 필요합니다. 계산복잡도 측면에서 시퀀스의 길이 &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt;이 representation 차원인 &lt;script type=&quot;math/tex&quot;&gt;d&lt;/script&gt;보다 작을 때 (word-piece, byte-pair와 같은 일반적인 기계번역에서 SOTA모델이 사용하는 방식) 셀프어텐션 레이어가 recurrent layer보다 더 빠릅니다. 아주 긴 시퀀스를 다룰 때 계산복잡도를 개선하기 위해 셀프어텐션은 아웃풋 포지션 주위로 r개의 인접 포지션만으로 제약하여 고려하도록 할수 있습니다. (in future work)&lt;/p&gt;

&lt;p&gt;커널 사이즈 &lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
k &lt; n %]]&gt;&lt;/script&gt;인 단일 컨볼루션 레이어는 인풋과 아웃풋 포지션의 모든 쌍을 연결하지 못합니다. 인풋과 아웃풋의 모든 포지션을 연결하기 위해서는 contiguous kernels을 이용하여 &lt;script type=&quot;math/tex&quot;&gt;O(n/k)&lt;/script&gt;만큼의 콘볼루션 레이어를 쌓거나, dilated convlutions을 이용하여 &lt;script type=&quot;math/tex&quot;&gt;O(log_k(n))&lt;/script&gt;를 쌓아야합니다. 이는 위에서 말한 경로 길이를 더 늘리는 것입니다. 컨볼루션 레이어는 일반적으로 recurrent layer보다 계산 비용이 더 큽니다. seperable convolution은 &lt;script type=&quot;math/tex&quot;&gt;O(k \cdot n \cdot d + n \cdot d^2)&lt;/script&gt;만큼 복잡도를 줄일수 있습니다. &lt;script type=&quot;math/tex&quot;&gt;k=n&lt;/script&gt;일때 seperable convolution는 이 논문에서 제안한 셀프-어텐션과 point-wise feed-forward layer를 결합한 것과 동일한 계산복잡도를 갖습니다.&lt;/p&gt;

&lt;p&gt;추가적으로 셀프-어텐션은 조금더 해석가능한 모델을 학습합니다. 어텐션 분포를 살펴보고 토의한 예제가 어펜딕스에 있습니다. 각각의 head가 명확하게 다른 작업을 수행하는 것을 학습할뿐만 아니라, 여러 head가 문장의 구조 및 의미 구조와 관련된 행동을 하는 것을 확인하였습니다.&lt;/p&gt;

&lt;h2 id=&quot;training&quot;&gt;Training&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;WMT 2014 English-German dataset and WMT 2014 English-French dataset&lt;/li&gt;
  &lt;li&gt;8 NVIDIA P100 GPUs&lt;/li&gt;
  &lt;li&gt;100,000 steps or 12 hours (each traini step took about 0.4 seconds)
    &lt;ul&gt;
      &lt;li&gt;for big model(bottom line of table 3), step time : 1.0 sec, total 300,000 steps(3.5 days)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Adam optimizer, &lt;script type=&quot;math/tex&quot;&gt;\beta_1&lt;/script&gt; = 0.9, &lt;script type=&quot;math/tex&quot;&gt;\beta_2&lt;/script&gt; = 0.98, &lt;script type=&quot;math/tex&quot;&gt;\epsilon_1&lt;/script&gt; = &lt;script type=&quot;math/tex&quot;&gt;10^{-9}&lt;/script&gt;
    &lt;ul&gt;
      &lt;li&gt;learning rate : increasing linearly for the first warmup_steps, decreasing it thereafter proportionally the the inverse square root fo the step number 
  &lt;script type=&quot;math/tex&quot;&gt;warmup\_steps&lt;/script&gt; = 4000
  &lt;script type=&quot;math/tex&quot;&gt;lrate = d_{model}^{-0.5} \cdot min(step\_num^{-0.5}, step\_num \cdot warmup\_steps^{-1.5})&lt;/script&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;three types of regularization
    &lt;ul&gt;
      &lt;li&gt;residual dropout : dropout to the output of each sub-layer, dropout to the sums of the embeddings and the positional encodings, &lt;script type=&quot;math/tex&quot;&gt;P_{drop} = 0.1&lt;/script&gt;&lt;/li&gt;
      &lt;li&gt;Label Smoothing : label smoothing of value &lt;script type=&quot;math/tex&quot;&gt;\epsilon_{ls} = 0.1&lt;/script&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-10-30/table2.png&quot; width=&quot;600&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;WMT 2014 English-to-German : new state of the art BLEU 28.4&lt;/li&gt;
  &lt;li&gt;WMT 2014 English-to-French : outperforming all previous single models with 1/4 traing cost of previous model&lt;/li&gt;
  &lt;li&gt;single model obtained by averaging the last 5 checkpoints(for big modle, 20 checkpoints)&lt;/li&gt;
  &lt;li&gt;beam searching with a beam size of 4 and length penalty &lt;script type=&quot;math/tex&quot;&gt;\alpha = 0.6&lt;/script&gt;
    &lt;ul&gt;
      &lt;li&gt;beam searching? &lt;a href=&quot;https://ratsgo.github.io/deep%20learning/2017/06/26/beamsearch/&quot;&gt;뭐냐&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;set the maximum output length during inferece to input length + 50&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-10-30/table3.png&quot; width=&quot;600&quot; /&gt;&lt;/p&gt;

&lt;p&gt;표3은 Transformer의 여러 요소들의 중요도를 평가하기 위해서 베이스 모델을 변형하면서 실험데이터(English-to-German translation on the development set, newstest2013)에 대한 성능 변화를 확인한 내용입니다.&lt;/p&gt;

&lt;p&gt;(A) : 멀티헤드의 개수 - 싱글-헤드 어텐션은 최적모델 대비 0.9만큼 낮은 BLEU를 보였습니다. 너무 많은 헤드일때도 성능이 떨어집니다. &lt;br /&gt;
(B) : key의 차원(&lt;script type=&quot;math/tex&quot;&gt;d_k&lt;/script&gt;)을 줄이면 성능이 떨어집니다. compatibility를 결정하는 것은 쉽지 않고, dot-product보다 더 정교한 함수가 유용할지 모른다는 것을 의미합니다. &lt;br /&gt;
(C) &amp;amp; (D) : bigger models are better. dropout is very helpful in avoiding over-fitting &lt;br /&gt;
(E) : sinusoidal positional encoding 대신에 learned positional embedding으로 변경한 결과, 거의 유사한 결과를 얻었습니다. (0.1 낮음)&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;conclusion&lt;/h2&gt;
&lt;p&gt;이 논문은 어텐션만 사용하여 시퀀스 모델을 학습한 최초의 접근인 Transformer를 제안하였습니다. 향후에는 텍스트 이외에 이미지, 오디오, 비디오와 같은 인풋과 아웃풋 문제로 확장하고, 로컬로 제약된 어텐션 메커니즘을 연결할 계획입니다. 또한 덜 순차적인 방식으로 일반화시키는 것이 또 다른 목표입니다.&lt;/p&gt;</content><author><name>yjucho</name></author><summary type="html">Ashish Vaswani et al. (Google Brain), 2017</summary></entry><entry><title type="html">recommender systems 2</title><link href="http://localhost:4000/recommender%20systems/recommendation2/" rel="alternate" type="text/html" title="recommender systems 2" /><published>2018-10-28T00:00:00+09:00</published><updated>2018-10-28T00:00:00+09:00</updated><id>http://localhost:4000/recommender%20systems/recommendation2</id><content type="html" xml:base="http://localhost:4000/recommender%20systems/recommendation2/">&lt;p&gt;추천시스템에 대해서 알아보자! - 지난 1편에서는 앤드류 응의 강의를 통해서 추천시스템의 전반적인 내용에 대해 알아보았습니다. 이번에는 Collaboratvie Filtering에 대해서 더 자세히 알아보고자 합니다.&lt;/p&gt;

&lt;p&gt;Collaborative filtering을 이용해 상품을 추천하는 방법은 크게 2가지 접근 방식이 있습니다. &lt;code class=&quot;highlighter-rouge&quot;&gt;neighborhood method&lt;/code&gt;와 &lt;code class=&quot;highlighter-rouge&quot;&gt;latent factor models&lt;/code&gt; 입니다.&lt;/p&gt;

&lt;h2 id=&quot;neighborhood-method&quot;&gt;Neighborhood method&lt;/h2&gt;
&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;neighborhood method&lt;/code&gt;는 아이템간 혹은 유저간 관계를 계산하는 것에 중점을 둡니다.&lt;/p&gt;

&lt;p&gt;유저 기반의 방법은 해당 유저와 유사한 다른 유저를 찾은 후, 비슷한 유저가 좋아하는 아이템을 추천하는 방식입니다. 그림1에서처럼, 세가지 영화를 좋아하는 Joe를 위해서, 세가지 영화를 동일하게 좋아하는 비슷한 유저를 찾습니다. 이들이 좋아하는 영화 중에서 가장 인기있는 영화인 Saving Private Ryan(라이언 일병 구하기, denoted #1)를 Joe에게 추천할 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-10-28/user-based-CF.png&quot; width=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small&gt;&lt;em&gt;그림1. user-oriented neighborhood method (Image source: Fig 1 in &lt;a href=&quot;https://datajobs.com/data-science-repo/Recommender-Systems-[Netflix].pdf&quot;&gt;Yehuda Koren et al., 2009&lt;/a&gt;)&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;아이템 기반의 방법은 해당 유저가 좋아하는 아이템과 유사한 아이템을 추천하는 방식입니다. 유사한 아이템은 해당 유저에게 동일한 평가를 받을 가능성이 크기 때문입니다. 예를 들어, Saving Private Ryan와 유사한 영화는 전쟁 영화거나, 톰행크스가 나오거나, 스필버그 감동의 다른 영화일 수 있습니다. 만약 누군가가 Saving Private Ryan를 어떻게 평가할지 궁금하다면, 그 사람이 실제로 본 영화 중에서 Saving Private Ryan와 유사한 영화를 어떻게 평가했는지 찾는 것과 같은 맥락입니다.&lt;/p&gt;

&lt;h3 id=&quot;similarity&quot;&gt;similarity&lt;/h3&gt;
&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;neighborhood method&lt;/code&gt;는 두 유저간 혹은 아이템간 유사도를 계산해야합니다. 유사도를 정량적으로 평가하기 위해서 일반적으로 2가지 measure를 사용합니다.&lt;/p&gt;

&lt;p&gt;&lt;b&gt; (1) pearson correlation coefficient&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;유저 &lt;script type=&quot;math/tex&quot;&gt;u&lt;/script&gt;와 유저 &lt;script type=&quot;math/tex&quot;&gt;v&lt;/script&gt;의 유사도를 &lt;script type=&quot;math/tex&quot;&gt;s(u, v)&lt;/script&gt;로 나타내면,&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;s(u, v) = {
{\sum_{i \in I_{uv}} (r_{ui} - \mu_u) \cdot (r_{vi} - \mu_v)}\over{\sqrt{\sum_{i \in I_{uv}} {(r_{ui} - \mu_u)}^2} \cdot \sqrt{\sum_{i \in I_{uv}} {(r_{vi} - \mu_v)}^2}}}&lt;/script&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;r_{ui}&lt;/script&gt;는 사용자 u가 아이템 i에 대해서 평가한 평점&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;I_{uv}&lt;/script&gt;는 유저 &lt;script type=&quot;math/tex&quot;&gt;u&lt;/script&gt; 와 유저 &lt;script type=&quot;math/tex&quot;&gt;v&lt;/script&gt; 모두에 의해 평가된 아이템의 집합&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;\mu_u&lt;/script&gt;는 유저 &lt;script type=&quot;math/tex&quot;&gt;u&lt;/script&gt;의 평균 평점과 &lt;script type=&quot;math/tex&quot;&gt;\mu_v&lt;/script&gt;는 유저 &lt;script type=&quot;math/tex&quot;&gt;v&lt;/script&gt;의 평균 평점&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;마찬가지로 아이템 &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;와 아이템 &lt;script type=&quot;math/tex&quot;&gt;j&lt;/script&gt;의 유사도를 &lt;script type=&quot;math/tex&quot;&gt;s(i, j)&lt;/script&gt;로 나타내면,&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;s(i, j) = {
{\sum_{u \in U_{ij}} (r_{ui} - \mu_i) \cdot (r_{uj} - \mu_j)}\over{\sqrt{\sum_{u \in U_{ij}} {(r_{ui} - \mu_i)}^2} \cdot \sqrt{\sum_{u \in U_{ij}} {(r_{uj} - \mu_j)}^2}}}&lt;/script&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;U_{ij}&lt;/script&gt;는 아이템 &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; 와 아이템 &lt;script type=&quot;math/tex&quot;&gt;j&lt;/script&gt;를 모두 평가한 유저들의 집합&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;\mu_i&lt;/script&gt;는 아이템 &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;의 평균 평점과 &lt;script type=&quot;math/tex&quot;&gt;\mu_j&lt;/script&gt;는 아이템 &lt;script type=&quot;math/tex&quot;&gt;j&lt;/script&gt;의 평균 평점&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;의미적으로는 유저 &lt;script type=&quot;math/tex&quot;&gt;u&lt;/script&gt;(혹은 아이템 &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;) 평점이 1점 증가할때, 유저 &lt;script type=&quot;math/tex&quot;&gt;v&lt;/script&gt;(혹은 아이템 &lt;script type=&quot;math/tex&quot;&gt;j&lt;/script&gt;) 평점은 어느정도 증가/감소하는지를 평가하는 것입니다. 상관계수는 상관관계가 클수록 &lt;script type=&quot;math/tex&quot;&gt;\pm1&lt;/script&gt;에 가까운 값을 갖고, 0일 경우 상관관계가 거의 없는 것을 의미합니다.&lt;/p&gt;

&lt;p&gt;&lt;b&gt; (2) Cosine Similarity &lt;/b&gt;&lt;/p&gt;

&lt;p&gt;코사인 유사도는 두 non-zero vector간의 코사인 각을 측정하는 것입니다. 
두 벡터 A와 B의 코사인 유사도는 두 벡터의 내적을 이용해 정의됩니다. 각도 &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;가 0도이면 코사인 유사도는 1이 되고, &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;가 90도이면 코사인 유사도는 0이 됩니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
A \cdot B &amp; = |A| |B| cos \theta \\ \\
similarity  =  cos \theta &amp; = \frac{A \cdot B}{|A||B|} \\
&amp; =  \frac{\sum_{i=1}^{n}A_i \cdot B_i}{\sqrt{\sum_{i=1}^{n}(A_i)^2}{\sqrt{\sum_{i=1}^{n}(B_i)^2}}}
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;유저 &lt;script type=&quot;math/tex&quot;&gt;u&lt;/script&gt;와 유저 &lt;script type=&quot;math/tex&quot;&gt;v&lt;/script&gt;의 유사도를 &lt;script type=&quot;math/tex&quot;&gt;s(u, v)&lt;/script&gt;로 나타내면,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;s(u, v) = \frac{\sum_{i \in I_{uv}} r_{ui} \cdot r_{vi} }{ \sqrt{\sum_{i \in I_{uv}}r_{ui}^2} \sqrt{\sum_{i \in I_{uv}}r_{vi}^2} }&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;r_{ui}&lt;/script&gt;는 사용자 u가 아이템 i에 대해서 평가한 평점&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;I_{uv}&lt;/script&gt;는 유저 &lt;script type=&quot;math/tex&quot;&gt;u&lt;/script&gt; 와 유저 &lt;script type=&quot;math/tex&quot;&gt;v&lt;/script&gt; 모두에 의해 평가된 아이템의 집합&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;마찬가지로 아이템 &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;와 아이템 &lt;script type=&quot;math/tex&quot;&gt;j&lt;/script&gt;의 유사도를 &lt;script type=&quot;math/tex&quot;&gt;s(i, j)&lt;/script&gt;로 나타내면,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;s(i, j) = \frac{\sum_{u \in U_{ij}} r_{ui} \cdot r_{uj} }{ \sqrt{\sum_{u \in U_{ij}}r_{ui}^2} \sqrt{\sum_{u \in U_{ij}}r_{uj}^2} }&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;U_{ij}&lt;/script&gt;는 아이템 &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; 와 아이템 &lt;script type=&quot;math/tex&quot;&gt;j&lt;/script&gt;를 모두 평가한 유저들의 집합&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;pearson correlation coefficient와 비교하여 평균 평점에 대한 교정이 포함되지 않은 것을 볼수 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;matrix-factorization&quot;&gt;Matrix Factorization&lt;/h2&gt;

&lt;h3 id=&quot;sgd-vs-als&quot;&gt;SGD vs. ALS&lt;/h3&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;

&lt;p&gt;https://en.wikipedia.org/wiki/Collaborative_filtering&lt;/p&gt;

&lt;p&gt;https://medium.com/@cfpinela/recommender-systems-user-based-and-item-based-collaborative-filtering-5d5f375a127f&lt;/p&gt;

&lt;p&gt;https://datajobs.com/data-science-repo/Recommender-Systems-[Netflix].pdf&lt;/p&gt;

&lt;p&gt;http://jeongchul.tistory.com/553&lt;/p&gt;

&lt;p&gt;http://nicolas-hug.com/blog/matrix_facto_2&lt;/p&gt;

&lt;p&gt;https://datascienceschool.net/view-notebook/fcd3550f11ac4537acec8d18136f2066/&lt;/p&gt;</content><author><name>yjucho</name></author><summary type="html">추천시스템에 대해서 알아보자! - 지난 1편에서는 앤드류 응의 강의를 통해서 추천시스템의 전반적인 내용에 대해 알아보았습니다. 이번에는 Collaboratvie Filtering에 대해서 더 자세히 알아보고자 합니다.</summary></entry><entry><title type="html">Big Data Analysis with Scala and Spark</title><link href="http://localhost:4000/spark/scala/spark-with-scala/" rel="alternate" type="text/html" title="Big Data Analysis with Scala and Spark " /><published>2018-10-21T00:00:00+09:00</published><updated>2018-10-21T00:00:00+09:00</updated><id>http://localhost:4000/spark/scala/spark-with-scala</id><content type="html" xml:base="http://localhost:4000/spark/scala/spark-with-scala/">&lt;p&gt;https://www.coursera.org/learn/scala-spark-big-data/home/welcome&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Shared Memory Data Parallelism (SDP)와 Distributed Data Parallelism (DDP)의 공통점과 차이점을 얘기해주세요.&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;공통점 : 데이터를 나눠서, 병렬로 데이터를 처리한 후 결과를 합침(data-parallel programming). Collection abstraction을 처리할 수 있음. &lt;br /&gt;차이점 : SDP의 경우 한 머신 내 메모리 상에서 데이터가 나눠져  처리가 일어나지만, DDP는 여러개의 노드(머신)에서 처리가 됨. DDP는 노드간의 통신이 필요하기 때문에 latency를 고려해야함&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;분산처리 프레임워크 Haddop의 Fault Tolerance는 DDP의 어떤 문제를 해결했나요?&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Computations on unthinkably large data sets to succeed to completion. 수백~수천개의 노드로 확장가능하도록 함. 노드 중 한개라도 failure 발생하더라도 recover할 수 있음&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Spark가 하둡과 달리 데이터를 메모리에 저장하면서 개선한 것 무엇이고, 왜 메모리에 저장하면 그것이 개선이 되나요?&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;latency를 개선함. 같은 job에 대해서 100배 이상의 성능을 보임. Functional programming을 통해 latency를 줄임&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;val ramyons = List(&quot;신라면&quot;, &quot;틈새라면&quot;, &quot;너구리&quot;)&lt;/code&gt; &lt;br /&gt;
&lt;code class=&quot;highlighter-rouge&quot;&gt;val kkodulRamyons = ramyons.map(ramyon =&amp;gt; &quot;꼬들꼬들 &quot; + ramyon)&lt;/code&gt; &lt;br /&gt;
&lt;code class=&quot;highlighter-rouge&quot;&gt;kkodulRamyonsList.map()을 사용하여 ramyons 리스트에서 kkodulRamyon List를 새로 만들었습니다. kkodulRamyons랑 똑같이 생긴 List를 만드는 Scala 코드를 써주세요.&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;val kkodulRamyons = List(“꼬들꼬들 신라면”, “꼬들꼬들 틈새라면”, “꼬들꼬들 너구리”)&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;val noodles = List(List(&quot;신라면&quot;, &quot;틈새라면&quot;, &quot;너구리&quot;), List(&quot;짜파게티&quot;, &quot;짜왕&quot;, &quot;진짜장&quot;))&lt;/code&gt;
&lt;code class=&quot;highlighter-rouge&quot;&gt;val flatNoodles = noodles.flatMap(list =&amp;gt; list)&lt;/code&gt;
&lt;code class=&quot;highlighter-rouge&quot;&gt;flatNoodlesList.flatmap() 을 사용하여 noodles 리스트에서 flatNoodles List를 새로 만들었습니다. flatNoodles랑 똑같이 생긴 List를 만드는 Scala 코드를 써주세요.&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;val flatNoodles = List(“신라면”, “틈새라면”, “너구리”, “짜파게티”, “짜왕”, “진짜장”)&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;val jajangs = flatNoodles.filter(noodle =&amp;gt; noodle.contains(&quot;짜&quot;))&lt;/code&gt;
&lt;code class=&quot;highlighter-rouge&quot;&gt;jajangsList.filter() 를 사용하여 flatNoodles 리스트에서 jajangs List를 새로 만들었습니다. jajangs랑 똑같이 생긴 List를 만드는 Scala 코드를 써주세요.&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;val jajangs = List(“짜파게티”, “짜왕”, “진짜장”)&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;val jajangMenu = jajangs.reduce((first, second) =&amp;gt; first +&quot;,&quot; + second)&lt;/code&gt;
&lt;code class=&quot;highlighter-rouge&quot;&gt;jajangMenuList.reduce()를 사용하여 jajangs 리스트에서 jajangMenu String을 만들었습니다. jajangMenu랑 똑같이 생긴 String을 만드는 Scala 코드를 써주세요.&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;var jajangMenu = “짜파게티,짜왕,진짜장”&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Eager execution와 Lazy execution의 차이점은 무엇인가요?&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Lazy execution은 결과값이 바로 계산되지 않고 eager execution은 결과가 바로 계산됨. spark의 transformation은 lazy execution이라 action이 나타날때까지 실제로는 아무것도 수행되지 않음&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Transformation과 Action의 결과물 (Return Type)은 어떻게 다를까요?&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Transformation은 새로운 RDD를 결과물로 리턴하고, Action은 HDFS같은 외부 저장소에 값을 리턴함&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;RDD.cache()는 어떤 작동을 하고, 언제 쓰는 것이 좋은가?&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Action을 수행할때마다 RDD를 다시 계산하는 것은 시간적 비용이 크기때문에 메모리에 캐시 저장함&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Lazy Execution이 Eager Execution 보다 더 빠를 수 있는 예를 얘기해주세요.&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;logistic regression처럼 Iterative algoritm의 경우, 값(weight)이 업데이트될때마다 데이터를 평가하하는 경우 매 iteration마다 반복 계산되는 것은 계산 비용이 큼. 따라서 RDD를 persist()나 cache()를 이용해 메모리에 저장하는 방법이 효과적임
other’s best answer : RDD에서 transformation 이 수행 된 데이터를 재사용하는 경우 Lazy Execution이 Eager Excution보다 더 빠르다. 예를 들어 로그파일에서 “ERROR”가 포함된 로그만 필터링하기 위해 filter를 예약한 뒤 캐싱 기능을 호출한다. 이후 take를 이용하여 10개의 에러로그를 획득하는데, 액션이 호출되어 필터링이 수행되면서 필터링된 로그를 메모리에 캐싱을 하게 된다. 이처럼 캐싱된 에러 로그는 이후 총 개수를 구한다던가 하는 다른 actino이 호출될 때 메모리에 적재된 데이터를 이용하여 성능이 향상된다. 최초로 호출되는 action은 disk에서 읽어오는 것보다 조금 느릴 수 있다.
참고 - http://knight76.tistory.com/entry/%ED%8E%8C-lazy-evaluation%EB%8A%90%EA%B8%8B%ED%95%9C-%EA%B3%84%EC%82%B0%EB%B2%95%EC%97%90-%EB%8C%80%ED%95%9C-%EC%A2%8B%EC%9D%80-%EC%84%A4%EB%AA%85-%EA%B7%B8%EB%A6%BC-%EC%9E%90%EB%A3%8C&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;foldLft 와 aggregate 둘다 inputType과 outputType이 다른데 왜 aggregate 만 병렬 처리가 가능한지 설명해주세요.&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;foldLeft는 시퀀셜하게 처리되기 때문에 병렬처리가 불가능함. 만약 두개 콜렉션으로 나눠서 병렬처리한다고 했을때, 아웃풋 타입이 바뀌기때문에 두개의 아웃풋을 합치려고 할 때 타입 에러가 나서 더이상 동일한 함수를 적용할수 없음. aggregate는 seqop과 combop 펑션으로 이루어져있어, chunk로 나눠 처리된 결과를 combop함수를 통해 합칠수 있기 때문에 리턴 타입 변환과 병렬처리가 모두 가능함&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;pairRDD는 어떤 데이터 구조에 적합한지 설명해주세요. 또 pairRDD는 어떻게 만드나요?&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Key-value 형태로 구조화된 데이터를 다룰 때 유용함. 이미 존재하는 RDD에서 map을 이용해서 아래와 같이 만들수 있음
val rdd: RDD[WikipediaPage] = … 
val pairRdd = rdd.map(page =&amp;gt; (page.title, page.text))&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;groupByKey()와 mapValues()를 통해 나온 결과를 reduceByKey()를 사용해서도 똑같이 만들 수 있습니다. 그렇지만 reduceByKey를 쓰는 것이 더 효율적인 이유는 무엇일까요?&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;reduceByKey는 싱글머신에 있는 데이터끼리 합친 후 셔플됨. 반면 groupByKey()는 모든 key-value 값들이 셔플되기때문에 네트워크 상에 데이터가 불필요하게 많이 이동하면서 계산됨. 참고 - https://databricks.gitbooks.io/databricks-spark-knowledge-base/content/best_practices/prefer_reducebykey_over_groupbykey.html&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;join 과 leftOuterJoin, rightOuterJoin이 어떻게 다른지 설명하세요. &lt;/code&gt;&lt;/p&gt;

&lt;p&gt;join(inner join) return a new RDD containing combined pairs whose keys are present in both input RDDs —inner join(join)은 두개의 RDD에 모두 포함되는 key만 포함하여 새로운 pair-RDD를 리턴함. Outer join(leftOuterJoin, rightOuterJoin) return a new RDD containing combined pairs whose keys don’t have to be present in both input RDDs — 왼쪽 혹은 오른쪽 RDD 중에서 유지하고 싶은 keys를 중심으로 결합하여 새로운 RDD를 만듦&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Shuffling은 무엇인가요? 이것은 어떤 distributed data paraellism의 성능에 어떤 영향을 가져오나요?&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;we typically have to move data from one node to another to be “grouped with” its key. Doing this is called “shuffling”. 인메모리 대비 노드간 네트워크 통신이 필요함으로 되도록이면 최소화하는 것이 좋음&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;셔플링은 무엇이고 언제 발생하나요?&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;데이터를 키값을 기준으로 그룹핑하여 한 노드에서 다른 노드로 이동시키기는 것. groupByKey()를 수행할 때 발생.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;파티션은 무엇인가요? 파티션의 특징을 2가지 알려주세요.&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;pair-RDD를 키값을 중심으로 여러 노드에 나눠 저장하는 것&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;동일한 파티션에 있는 데이터들은 반드시 같은 머신에 존재한다.&lt;/li&gt;
  &lt;li&gt;클러스터 안에 한개의 머신에는 적어도 하나 이상의 파티션이 존재할수 있다.&lt;/li&gt;
  &lt;li&gt;파티션의 수는 설정할수 있다. 기본적으로는 executor node의 코어 수와 같다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;스파크에서 제공하는 partitioning 의 종류 두가지를 각각 설명해주세요.&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Hash partitioning :튜플(k, v)마다 p=k.hashCode()%numPartitions 해서 p가 같은 데이터끼리 모아 파티셔닝하는 것. &lt;br /&gt;
Range partitionin :ordering이 있는 key일 경우, 범위별로 데이터를 나누는 것&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;파티셔닝은 어떻게 퍼포먼스를 높여주나요?&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;키값을 중심으로 데이터가 어떤 머신에 있는지 알기 때문에 데이터가 셔플링되는 것을 최소화할수 있음&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;rdd 의 toDebugString 의 결과값은 무엇을 보여주나요?&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;RDD’s lineage를 시각적으로 보여줌&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;파티션 된 rdd에 map 을 실행하면 결과물의 파티션은 어떻게 될까요? mapValues의 경우는 어떤가요?&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;map을 실행하면 파티션이 없어짐. map은 키를 바꿀수 있는 오퍼레이션 이기때문임. mapValues는 키값을 유지하기때문에 파티션도 유지됨.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Narrow Dependency 와 Wide Dependency를 설명해주세요. 각 Dependency를 만드는 operation은 무엇이 있을까요?&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Narrow Dependency - 1개의 부모RDD는 최대 1개의 자식RDD에만 영향을 줌. map, filter, union, join with co-partitioned inputs &lt;br /&gt;
Wide Dependency - 1개의 부모RDD가 여러개의 자식RDD에 영향을 줌. groupByKey, join with inputs not co-partitioned&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Lineage 는 어떻게 Fault Tolerance를 가능하게 하나요?&lt;/code&gt;
RDD는 immutable하고, 우리가 higher-order function을 사용하고, 그 function 역시 RDD를 리턴하기 때문에 lineage graphs를 통해서 dependency information을 추적하여 잃어버린 파티션을 다시 계산하여 failure로부터 회복할수 있다.&lt;/p&gt;</content><author><name>yjucho</name></author><summary type="html">https://www.coursera.org/learn/scala-spark-big-data/home/welcome</summary></entry><entry><title type="html">recommender systems</title><link href="http://localhost:4000/recommender%20systems/recommendation/" rel="alternate" type="text/html" title="recommender systems" /><published>2018-10-20T00:00:00+09:00</published><updated>2018-10-20T00:00:00+09:00</updated><id>http://localhost:4000/recommender%20systems/recommendation</id><content type="html" xml:base="http://localhost:4000/recommender%20systems/recommendation/">&lt;p&gt;추천시스템에 대해서 알아보자! 앤드류응의 머신러닝 강의 중 추천시스템 부분에 대해서 정리하였습니다.&lt;/p&gt;

&lt;h2 id=&quot;problem-formulation&quot;&gt;problem formulation&lt;/h2&gt;

&lt;p&gt;아래와 같이 4명의 유저가 5개 영화를 평가한 데이터가 있다고 하겠습니다. 추천시스템은 이와 같은 평점 데이터를 이용해, 유저가 아직 평가하지 않은 영화를 몇점으로 평가할지 예측하는 문제로 생각할 수 있습니다.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Movie&lt;/th&gt;
      &lt;th&gt;Alice(1)&lt;/th&gt;
      &lt;th&gt;Bob(2)&lt;/th&gt;
      &lt;th&gt;Carol(3)&lt;/th&gt;
      &lt;th&gt;Dave(4)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Love at last&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Romance forever&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;&lt;b&gt;?&lt;/b&gt;&lt;/td&gt;
      &lt;td&gt;&lt;b&gt;?&lt;/b&gt;&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Cute puppies of love&lt;/td&gt;
      &lt;td&gt;&lt;b&gt;?&lt;/b&gt;&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;&lt;b&gt;?&lt;/b&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Nonstop car chases&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Swords vs. karete&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;&lt;b&gt;?&lt;/b&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;n_u&lt;/script&gt; = number of users &lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;n_m&lt;/script&gt; = number of movies &lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;r(i, j)&lt;/script&gt; = 1 if user &lt;script type=&quot;math/tex&quot;&gt;j&lt;/script&gt; has rated movie &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; &lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;y^{(i, j)}&lt;/script&gt; = rating given by user &lt;script type=&quot;math/tex&quot;&gt;j&lt;/script&gt; to movie &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; (defined only if &lt;script type=&quot;math/tex&quot;&gt;r(i, j) = 1&lt;/script&gt; )&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;given &lt;script type=&quot;math/tex&quot;&gt;r(i, j)&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;y^{(i, j)}&lt;/script&gt;, we try to predict what these values of the question mark should be&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;content-based-recommendations&quot;&gt;Content Based Recommendations&lt;/h2&gt;

&lt;p&gt;우선 모든 영화가 로맨스인지, 액션인지 평가된 특징 정보가 있다고 가정하겠습니다. 아래 테이블은 각 영화의 특징에 해당하는 정보(피쳐 벡터), &lt;script type=&quot;math/tex&quot;&gt;x_1&lt;/script&gt;과 &lt;script type=&quot;math/tex&quot;&gt;x_2&lt;/script&gt;를 추가한 것입니다. 이 경우, 각 사용자의 평점은 피쳐벡서를 입력으로 하는 회귀 문제가 됩니다. 회귀문제에서 가중치 &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;는 사용자마다 다르며, 이미 평가한 데이터와 예측값의 오차를 최소화하는 방향으로 회귀식의 가중치을 학습할 수 있습니다.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Movie&lt;/th&gt;
      &lt;th&gt;Alice(1)&lt;/th&gt;
      &lt;th&gt;Bob(2)&lt;/th&gt;
      &lt;th&gt;Carol(3)&lt;/th&gt;
      &lt;th&gt;Dave(4)&lt;/th&gt;
      &lt;th&gt;&lt;script type=&quot;math/tex&quot;&gt;x_1 \\ (romance)&lt;/script&gt;&lt;/th&gt;
      &lt;th&gt;&lt;script type=&quot;math/tex&quot;&gt;x_2 \\ (action)&lt;/script&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Love at last&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0.9&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Romance forever&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;&lt;b&gt;?&lt;/b&gt;&lt;/td&gt;
      &lt;td&gt;&lt;b&gt;?&lt;/b&gt;&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;0.01&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Cute puppies of love&lt;/td&gt;
      &lt;td&gt;&lt;b&gt;?&lt;/b&gt;&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;&lt;b&gt;?&lt;/b&gt;&lt;/td&gt;
      &lt;td&gt;0.99&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Nonstop car chases&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;0.1&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Swords vs. karete&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;&lt;b&gt;?&lt;/b&gt;&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0.9&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;For each user &lt;script type=&quot;math/tex&quot;&gt;j&lt;/script&gt;, learn a prarameter &lt;script type=&quot;math/tex&quot;&gt;\theta ^{(j)} \in \mathbb{R}^{n+1}&lt;/script&gt; &lt;br /&gt;
( n is the number of features. In above example, n = 2 because of &lt;script type=&quot;math/tex&quot;&gt;x_1,\ x_2&lt;/script&gt;. By default &lt;script type=&quot;math/tex&quot;&gt;x_0&lt;/script&gt;=0 )&lt;/p&gt;

&lt;p&gt;Predict user &lt;script type=&quot;math/tex&quot;&gt;j&lt;/script&gt; as rating movie &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; with &lt;script type=&quot;math/tex&quot;&gt;(\theta ^{(j)})^T x^{(i)}&lt;/script&gt; &lt;br /&gt;
( &lt;script type=&quot;math/tex&quot;&gt;x^{(i)}&lt;/script&gt; = feature vector of movie &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; )&lt;/p&gt;

&lt;p&gt;&lt;b&gt;&lt;em&gt;example of Alice’s rating of “Cute pupples of love”&lt;/em&gt;&lt;/b&gt; &lt;br /&gt;
&lt;small&gt;(Assumed &lt;script type=&quot;math/tex&quot;&gt;\theta ^{(1)}&lt;/script&gt; learned by model) &lt;/small&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;x^{(3)} = 
\begin{bmatrix}
1  \\
0.99 \\
0 
\end{bmatrix} \ \ \ 
\theta^{(1)} = 
\begin{bmatrix}
0  \\
5 \\
0 
\end{bmatrix}
\\
(\theta ^{(1)})^T x^{(3)} = 5 * 0.99 = 4.95&lt;/script&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\theta ^{(j)}&lt;/script&gt; = parameter vector of user &lt;script type=&quot;math/tex&quot;&gt;j&lt;/script&gt; &lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;x ^{(i)}&lt;/script&gt; = feature vector for movie &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; &lt;br /&gt;
For user &lt;script type=&quot;math/tex&quot;&gt;j&lt;/script&gt;, movie &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;, predicted rating : &lt;script type=&quot;math/tex&quot;&gt;(\theta ^{(j)})^T x^{(i)}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;&lt;b&gt;To learn &lt;script type=&quot;math/tex&quot;&gt;\theta ^{(j)}&lt;/script&gt; (parameter for user &lt;script type=&quot;math/tex&quot;&gt;j&lt;/script&gt;)&lt;/b&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\min_{\theta ^{(j)}} \frac{1}{2} \sum_{i:r(i,j)=1} ((\theta ^{(j)})^T x^{(i)} - y^{(i,j)})^2 + \frac{\lambda}{2} \sum_{k=1}^{n}(\theta_k^{(j)})^2&lt;/script&gt;

&lt;p&gt;&lt;b&gt;To learn  &lt;script type=&quot;math/tex&quot;&gt;\theta ^{(1)},\theta ^{(2)}, ..., \theta ^{(n_u)}&lt;/script&gt; &lt;/b&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\min_{\theta ^{(j)}} \frac{1}{2} \sum_{j=1}^{n_u} \sum_{i:r(i,j)=1} ((\theta ^{(j)})^T x^{(i)} - y^{(i,j)})^2 + \frac{\lambda}{2} \sum_{j=1}^{n_u} \sum_{k=1}^{n}(\theta_k^{(j)})^2&lt;/script&gt;

&lt;p&gt;&lt;b&gt;Gradient descent update&lt;/b&gt; :&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\theta_k^{(j)} := \theta_k^{(j)} -  \alpha \sum_{i:r(i,j)=1} ((\theta ^{(j)})^T x^{(i)} - y^{(i,j)})x_k^{(i)} \ \ (for \ k=0) \\
\theta_k^{(j)} := \theta_k^{(j)} -  \alpha \left( \sum_{i:r(i,j)=1} ((\theta ^{(j)})^T x^{(i)} - y^{(i,j)})x_k^{(i)} + \lambda \theta_k^{(j)} \right) \ \ (for \ k \neq 0)&lt;/script&gt;

&lt;h2 id=&quot;collaborative-filtering&quot;&gt;Collaborative Filtering&lt;/h2&gt;
&lt;p&gt;이제까지는 모든 영화가 로맨스인지, 액션인지 평가된 피쳐벡터가 존재한다는 가정을 하였습니다. 하지만 모든 영화를 보고 특징 정보를 정리하는 것은 매우 비용이 많이 드는 작업입니다. 이를 극복하기 위해서 반대로 사용자에게 로맨스 영화를 얼마나 좋아하는지, 액션 영화를 얼마나 좋아하는지를 조사하고, 이 값을 토대로 피쳐 벡터를 추정하는 방법을 택할 수 있습니다. 사용자 &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;가 응답한 정보를 &lt;script type=&quot;math/tex&quot;&gt;\theta^{(i)}&lt;/script&gt;로 이용하여 아래와 같이 계산할 수 있습니다.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Movie&lt;/th&gt;
      &lt;th&gt;Alice(1)&lt;/th&gt;
      &lt;th&gt;Bob(2)&lt;/th&gt;
      &lt;th&gt;Carol(3)&lt;/th&gt;
      &lt;th&gt;Dave(4)&lt;/th&gt;
      &lt;th&gt;&lt;script type=&quot;math/tex&quot;&gt;x_1 \\ (romance)&lt;/script&gt;&lt;/th&gt;
      &lt;th&gt;&lt;script type=&quot;math/tex&quot;&gt;x_2 \\ (action)&lt;/script&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Love at last&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;&lt;b&gt;?&lt;/b&gt;&lt;/td&gt;
      &lt;td&gt;&lt;b&gt;?&lt;/b&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Romance forever&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;&lt;b&gt;?&lt;/b&gt;&lt;/td&gt;
      &lt;td&gt;&lt;b&gt;?&lt;/b&gt;&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;&lt;b&gt;?&lt;/b&gt;&lt;/td&gt;
      &lt;td&gt;&lt;b&gt;?&lt;/b&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Cute puppies of love&lt;/td&gt;
      &lt;td&gt;&lt;b&gt;?&lt;/b&gt;&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;&lt;b&gt;?&lt;/b&gt;&lt;/td&gt;
      &lt;td&gt;&lt;b&gt;?&lt;/b&gt;&lt;/td&gt;
      &lt;td&gt;&lt;b&gt;?&lt;/b&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Nonstop car chases&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;&lt;b&gt;?&lt;/b&gt;&lt;/td&gt;
      &lt;td&gt;&lt;b&gt;?&lt;/b&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Swords vs. karete&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;&lt;b&gt;?&lt;/b&gt;&lt;/td&gt;
      &lt;td&gt;&lt;b&gt;?&lt;/b&gt;&lt;/td&gt;
      &lt;td&gt;&lt;b&gt;?&lt;/b&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\theta^{(1)} = 
\begin{bmatrix}
0  \\
5 \\
0 
\end{bmatrix} \ \ \ 
\theta^{(2)} = 
\begin{bmatrix}
0  \\
5 \\
0 
\end{bmatrix} \ \ \ 
\theta^{(3)} = 
\begin{bmatrix}
0  \\
0 \\
5 
\end{bmatrix} \ \ \ 
\theta^{(4)} = 
\begin{bmatrix}
0  \\
0 \\
5 
\end{bmatrix} \ \ \&lt;/script&gt;

&lt;p&gt;&lt;b&gt; Given &lt;script type=&quot;math/tex&quot;&gt;\theta ^{(1)},\theta ^{(2)}, ..., \theta ^{(n_u)}&lt;/script&gt;, to learn &lt;script type=&quot;math/tex&quot;&gt;x^{(i)}&lt;/script&gt; &lt;/b&gt; :&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\min_{x^{(i)}} \frac{1}{2} \sum_{j:r(i,j)=1} ((\theta ^{(j)})^T x^{(i)} - y^{(i,j)})^2 + \frac{\lambda}{2} \sum_{k=1}^{n}(x_k^{(i)})^2&lt;/script&gt;

&lt;p&gt;&lt;b&gt; Given &lt;script type=&quot;math/tex&quot;&gt;\theta ^{(1)},\theta ^{(2)}, ..., \theta ^{(n_u)}&lt;/script&gt;, to learn &lt;script type=&quot;math/tex&quot;&gt;x^{(1)}, ..., x^{(n_m)}&lt;/script&gt; &lt;/b&gt; :&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\min_{x^{(1)},...,x^{(n_m)}} \frac{1}{2} \sum_{i=1}^{n_m} \sum_{j:r(i,j)=1} ((\theta ^{(j)})^T x^{(i)} - y^{(i,j)})^2 + \frac{\lambda}{2} \sum_{i=1}^{n_m} \sum_{k=1}^{n}(x_k^{(i)})^2&lt;/script&gt;

&lt;blockquote&gt;
  &lt;p&gt;Collaborative filtering refers to the observation that when you run this algorithm with a large set of users, all of these users are effectively doing collaboratively to get better movie ratings for everyone. because with every user rating, some subset of the movies, every user is helping the algorithm a little bit to learn better features. By rating a few movies myself, I would be hoping the systerm learn better features and then these features can be used by the system to make better movies predictions for everyone else. &lt;u&gt;So there's a sense of collaboration where every user is helping the system learn better features for the common good.&lt;/u&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Content based 방법은 영화별 특징 정보가 존재할 때, 유저별 가중치를 추정하는 것이고, Collaborative filtering은 유저별 가중치가 존재할 때 영화별 특징 정보를 추정하는 것입니다. 따라서 우리는 이 두가지를 결합하여, forward and backward 방식으로 초기값 &lt;script type=&quot;math/tex&quot;&gt;\theta^{(i)}&lt;/script&gt;를 랜덤하게 설정한 후 피쳐벡터를 추정하고, 추정된 피쳐벡터로 다시 유저 가중치를 구할수 있습니다. 더 간단하게는 &lt;script type=&quot;math/tex&quot;&gt;\theta^{(i)}&lt;/script&gt;와 &lt;script type=&quot;math/tex&quot;&gt;x^{j}&lt;/script&gt;를 동시에 추정할 수 있습니다.&lt;/p&gt;

&lt;p&gt;if we are given &lt;script type=&quot;math/tex&quot;&gt;x^{(i)}&lt;/script&gt;, we can estimate &lt;script type=&quot;math/tex&quot;&gt;\theta^{(i)}&lt;/script&gt;. Likewise if we are given &lt;script type=&quot;math/tex&quot;&gt;\theta^{(i)}&lt;/script&gt;, we can estimate &lt;script type=&quot;math/tex&quot;&gt;x^{(i)}&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;So we can initialize &lt;script type=&quot;math/tex&quot;&gt;\theta^{(i)}&lt;/script&gt; randomly, then estimate &lt;script type=&quot;math/tex&quot;&gt;x^{(i)}&lt;/script&gt;. After that, we update  &lt;script type=&quot;math/tex&quot;&gt;\theta^{(i)}&lt;/script&gt; and repeat.&lt;/p&gt;

&lt;p&gt;Putting togeter, we can solve for theta and x simultaneously.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\min_{x^{(1)}, ... , x^{(n_m)}, \theta^{(1)}, ..., \theta ^{(n_u)}} J(x^{(1)}, ... , x^{(n_m)}, \theta^{(1)}, ..., \theta ^{(n_u)}) \\
J(x^{(1)}, ... , x^{(n_m)}, \theta^{(1)}, ..., \theta ^{(n_u)}) = \frac{1}{2} \sum_{(i, j):r(i,j)=1}  ((\theta ^{(j)})^T x^{(i)} - y^{(i,j)})^2 + \frac{\lambda}{2} \sum_{i=1}^{n_m} \sum_{k=1}^{n}(x_k^{(i)})^2 + \frac{\lambda}{2} \sum_{j=1}^{n_u} \sum_{k=1}^{n}(\theta_k^{(j)})^2&lt;/script&gt;

&lt;p&gt;note : in this version, &lt;script type=&quot;math/tex&quot;&gt;x \in \mathbb{R}^{n}, \theta \in \mathbb{R}^{n}&lt;/script&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Initialize &lt;script type=&quot;math/tex&quot;&gt;x^{(1)}, ... , x^{(n_m)}, \theta^{(1)}, ..., \theta ^{(n_u)}&lt;/script&gt; to small random values&lt;/li&gt;
  &lt;li&gt;Minimize &lt;script type=&quot;math/tex&quot;&gt;J(x^{(1)}, ... , x^{(n_m)}, \theta^{(1)}, ..., \theta ^{(n_u)})&lt;/script&gt; using gradient descent(or an advanced optimization algorithm). E.g. for every &lt;script type=&quot;math/tex&quot;&gt;j=1, ..., n_u, i=1,...,n_m&lt;/script&gt; :
&lt;script type=&quot;math/tex&quot;&gt;x_k^{(i)} := x_k^{(i)} -  \alpha \left( \sum_{j:r(i,j)=1} ((\theta ^{(j)})^T x^{(i)} - y^{(i,j)})\theta_k^{(j)} + \lambda x_k^{(i)} \right) \\
\theta_k^{(j)} := \theta_k^{(j)} -  \alpha \left( \sum_{i:r(i,j)=1} ((\theta ^{(j)})^T x^{(i)} - y^{(i,j)})x_k^{(i)} + \lambda \theta_k^{(j)} \right)&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;For a user with parameters &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; and a movie with (learned) features &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt;, predict a star rating of &lt;script type=&quot;math/tex&quot;&gt;\theta^Tx&lt;/script&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;vectorization--low-rank-matrix-factorization&quot;&gt;Vectorization : Low Rank Matrix Factorization&lt;/h2&gt;
&lt;p&gt;위에서 설명한 것을 각 element별로 구하는 것이 아니라 matrix 형태로 vectorization하여 계산할 수도 있습니다.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Movie&lt;/th&gt;
      &lt;th&gt;Alice(1)&lt;/th&gt;
      &lt;th&gt;Bob(2)&lt;/th&gt;
      &lt;th&gt;Carol(3)&lt;/th&gt;
      &lt;th&gt;Dave(4)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Love at last&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Romance forever&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;&lt;b&gt;?&lt;/b&gt;&lt;/td&gt;
      &lt;td&gt;&lt;b&gt;?&lt;/b&gt;&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Cute puppies of love&lt;/td&gt;
      &lt;td&gt;&lt;b&gt;?&lt;/b&gt;&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;&lt;b&gt;?&lt;/b&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Nonstop car chases&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Swords vs. karete&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;&lt;b&gt;?&lt;/b&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
Y = 
\begin{bmatrix}
5 &amp; 5 &amp; 0 &amp; 0 \\
5 &amp; ? &amp; ? &amp; 0 \\
? &amp; 4 &amp; 0 &amp; ? \\
0 &amp; 0 &amp; 5 &amp; 4 \\
0 &amp; 0 &amp; 5 &amp; 0 \\
\end{bmatrix} \ \ \ \

predicted \ ratings : 
\begin{bmatrix}
(\theta ^{(1)})^T x^{(1)} &amp; (\theta ^{(2)})^T x^{(1)} &amp; \cdots &amp; (\theta ^{(n_u)})^T x^{(1)} \\
(\theta ^{(1)})^T x^{(2)} &amp; (\theta ^{(2)})^T x^{(2)} &amp; \cdots &amp; (\theta ^{(n_u)})^T x^{(2)} \\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots \\
(\theta ^{(1)})^T x^{(n_m)} &amp; (\theta ^{(2)})^T x^{(n_m)} &amp; \cdots &amp; (\theta ^{(n_u)})^T x^{(n_m)} 
\end{bmatrix} %]]&gt;&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
X = 
\begin{bmatrix}
-(x^{(1)})^T- \\
-(x^{(2)})^T- \\
\cdots\\
-(x^{(n_m)})^T- \\
\end{bmatrix} \ \ \ 

\Theta = 
\begin{bmatrix}
| &amp; | &amp; \ &amp; | \\
(\Theta^{(1)})^T &amp; (\Theta^{(2)})^T &amp; \cdots &amp;(\Theta^{(n_u)})^T \\
| &amp; | &amp; \ &amp; | \\
\end{bmatrix} 
\\
then, \\
predicted \ ratings = X\Theta %]]&gt;&lt;/script&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;X\Theta&lt;/script&gt; has mathematical property of low rank matrix&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;b&gt; How to find movies &lt;script type=&quot;math/tex&quot;&gt;j&lt;/script&gt; related to movie &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; &lt;/b&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;small &lt;script type=&quot;math/tex&quot;&gt;\parallel x^{(i)} - x^{(j)} \parallel&lt;/script&gt; \to moving &lt;script type=&quot;math/tex&quot;&gt;j&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; are “similar”&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;implementation-detail-mean-normalization&quot;&gt;Implementation Detail Mean Normalization&lt;/h2&gt;
&lt;p&gt;지금까지 설명한 collaborative filtering은 평점 데이터가 없은 유저에 대해서는 항상 0값을 예측한다는 단점이 있습니다.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Movie&lt;/th&gt;
      &lt;th&gt;Alice(1)&lt;/th&gt;
      &lt;th&gt;Bob(2)&lt;/th&gt;
      &lt;th&gt;Carol(3)&lt;/th&gt;
      &lt;th&gt;Dave(4)&lt;/th&gt;
      &lt;th&gt;Eve(5)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Love at last&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;&lt;b&gt;?&lt;/b&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Romance forever&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;&lt;b&gt;?&lt;/b&gt;&lt;/td&gt;
      &lt;td&gt;&lt;b&gt;?&lt;/b&gt;&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;&lt;b&gt;?&lt;/b&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Cute puppies of love&lt;/td&gt;
      &lt;td&gt;&lt;b&gt;?&lt;/b&gt;&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;&lt;b&gt;?&lt;/b&gt;&lt;/td&gt;
      &lt;td&gt;&lt;b&gt;?&lt;/b&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Nonstop car chases&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;&lt;b&gt;?&lt;/b&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Swords vs. karete&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;&lt;b&gt;?&lt;/b&gt;&lt;/td&gt;
      &lt;td&gt;&lt;b&gt;?&lt;/b&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;b&gt;For Eve&lt;/b&gt;, compute &lt;script type=&quot;math/tex&quot;&gt;\theta^{(5)}&lt;/script&gt; : 
let’s say that n is equal to 2.&lt;/p&gt;

&lt;p&gt;Since Eve rated no movies, there are no movies for which r(i, j) is equal to one. So the first term of the objective plays no role at all in determining &lt;script type=&quot;math/tex&quot;&gt;\theta^{(5)}&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;the only term that affects  &lt;script type=&quot;math/tex&quot;&gt;\theta^{(5)}&lt;/script&gt; is the last term&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\min \frac{\lambda}{2} \sum_{j=1}^{n_u} \sum_{k=1}^{n}(\theta_k^{(j)})^2 = \frac{\lambda}{2} 
\begin{bmatrix}
(\Theta_1^{(5)})^2 + (\Theta_2^{(5)})^2
\end{bmatrix}&lt;/script&gt;

&lt;p&gt;Minimizing above term, we’re going to end up with &lt;script type=&quot;math/tex&quot;&gt;(\theta^{(5)})^T =[0 \ \ 0]&lt;/script&gt;. So all predicted ratings for Eve(&lt;script type=&quot;math/tex&quot;&gt;(\theta^{(5)})^T x^{(i)}&lt;/script&gt;) is equal to zero.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;This approach is not useful. The idea of mean normalization will let us fix this problem.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;이를 해결하기 위해 일반적으로 mean normalization이라는 전처리 과정을 추가합니다. 직관적으로 평점데이터가 없는 유저의 경우, 영화별 평균 평점으로 예측하도록 하는 방법입니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
Y = 
\begin{bmatrix}
5 &amp; 5 &amp; 0 &amp; 0 &amp; ? \\
5 &amp; ? &amp; ? &amp; 0 &amp; ? \\
? &amp; 4 &amp; 0 &amp; ? &amp; ? \\
0 &amp; 0 &amp; 5 &amp; 4 &amp; ? \\
0 &amp; 0 &amp; 5 &amp; 0 &amp; ? \\
\end{bmatrix} \ \ \ \

\mu = 
\begin{bmatrix}
2.5\\
2.5\\
2\\
2.25\\
1.25\\
\end{bmatrix} \ \ \ \
\to
Y = 
\begin{bmatrix}
2.5 &amp; 2.5 &amp; -2.5 &amp; -2.5 &amp; ? \\
2.5 &amp; ? &amp; ? &amp; -2.5 &amp; ? \\
? &amp; 2 &amp; -2 &amp; ? &amp; ? \\
-2.25 &amp; -2.25 &amp; 2.75 &amp; 1.75 &amp; ? \\
-1.25 &amp; -1.25 &amp; 3.75 &amp; -1.25 &amp; ? \\
\end{bmatrix} \ \ \ \ %]]&gt;&lt;/script&gt;

&lt;p&gt;&lt;b&gt;For user &lt;script type=&quot;math/tex&quot;&gt;j&lt;/script&gt;, on movie &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; predict :
&lt;script type=&quot;math/tex&quot;&gt;(\theta^{(j)})^T(x^{i}) + \mu_i&lt;/script&gt;&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;&lt;b&gt;For Eve :
&lt;script type=&quot;math/tex&quot;&gt;(\theta^{(j)})^T(x^{i}) + \mu_i = 0 + \mu_i = \mu_i&lt;/script&gt;&lt;/b&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Mean normalization as a solid pre-processing step for collaborative filtering.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;감사합니다!&lt;/p&gt;

&lt;h2 id=&quot;reference-&quot;&gt;Reference :&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=giIXNoiqO_U&quot;&gt;Recommender Systems - Problem Formulation&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=9siFuMMHNIA&quot;&gt;Recommender Systems - Content Based Recommendations&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=9AP-DgFBNP4&quot;&gt;Recommender Systems - Collaborative Filtering_1&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=YW2b8La2ICo&quot;&gt;Recommender Systems - Collaborative Filtering_2&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=5R1xOJOFRzs&quot;&gt;Recommender Systems - Vectorization Low Rank Matrix Factorization&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=Am9fhp2Q91o&quot;&gt;Recommender Systems - Implementational Detail Mean Normalization&lt;/a&gt;&lt;/p&gt;</content><author><name>yjucho</name></author><summary type="html">추천시스템에 대해서 알아보자! 앤드류응의 머신러닝 강의 중 추천시스템 부분에 대해서 정리하였습니다.</summary></entry><entry><title type="html">[번역] Attention? Attention!</title><link href="http://localhost:4000/attention/attention/" rel="alternate" type="text/html" title="[번역] Attention? Attention!" /><published>2018-10-13T00:00:00+09:00</published><updated>2018-10-13T00:00:00+09:00</updated><id>http://localhost:4000/attention/attention</id><content type="html" xml:base="http://localhost:4000/attention/attention/">&lt;blockquote&gt;
  &lt;p&gt;이 글은 &lt;a href=&quot;https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html&quot;&gt;lilianweng의 Attention? Attention! 포스팅&lt;/a&gt;을 번역한 글입니다.&lt;br /&gt;&lt;br /&gt;Attention은 최근 딥러닝 커뮤니티에서 자주 언급되는 유용한 툴입니다. 이 포스트에서는 어떻게 어텐션 개념과 다양한 어텐션 메커니즘을 설명하고 transformer와 SNAIL과 같은 모델들에 대해서 알아보고자 합니다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#whats-wrong-with-seq2seq-model&quot;&gt;What’s Wrong with Seq2Seq Model?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#born-for-translation&quot;&gt;Born for Translation&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#definition&quot;&gt;Definition&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#a-family-of-attention-mechanisms&quot;&gt;A Family of Attention Mechanisms&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#summary&quot;&gt;Summary&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#self-attention&quot;&gt;Self-Attention&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#soft-vs-hard-attention&quot;&gt;Soft vs Hard Attention&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#global-vs-local-attention&quot;&gt;Global vs Local Attention&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#transformer&quot;&gt;Transformer&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#key-value-and-query&quot;&gt;Key, Value and Query&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#multi-head-self-attention&quot;&gt;Multi-Head Self-Attention&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#encoder&quot;&gt;Encoder&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#decoder&quot;&gt;Decoder&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#full-architecture&quot;&gt;Full Architecture&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#snail&quot;&gt;SNAIL&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#self-attention-gan&quot;&gt;Self-Attention GAN&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#references&quot;&gt;References&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Attention은 우리가 이미지에서 어떤 영역을 주목하는지, 한 문장에서 연관된 단어는 무엇인지를 찾는데서 유래하였습니다. 그림1에 있는 시바견을 살펴보세요.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-10-13/shiba-example-attention.png&quot; width=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small&gt;&lt;em&gt;그림1. 사람옷을 입은 시바견. 이미지의 모든 권리는 인스타그램 &lt;a href=&quot;https://www.instagram.com/mensweardog/?hl=en&quot;&gt;@mensweardog&lt;/a&gt;에 있습니다.&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;인간은 이미지의 특정 부분을 고해상도로(노란 박스안에 뽀족한 귀) 집중하는 반면, 주변 부분들은 저해상도((눈이 쌓인 배경과 복장)로 인식하고 이후 초점영역을 조정하여 그에 따른 추론을 합니다. 이미지의 작은 패치가 가려져있을때, 나머지 영역의 픽셀들은 그 영역에 어떤 것이 들어가야 하는지를 알려주는 힌트가 됩니다. 우리는 노란 박스 안은 뽀족한 귀가 있어야 하는 것을 알고 있습니다. 왜냐하면 개의 코, 오른쪽의 다른 귀, 시바견의 몽롱한 눈(빨란 박스안에 것들)를 이미 봤기 때문입니다. 반면 이 추론을 하는데 아래쪽에 있는 스웨터나 담요는 별 도움이 되지 못합니다.&lt;/p&gt;

&lt;p&gt;마찬가지로, 한 문장이나 가까운 문맥 상에서 단어들간의 관계를 설명할수 있습니다. “eating”이라는 단어를 보았을때, 음식 종류에 해당하는 단어가 가까이 위치에 있을 것을 예상할수 있습니다. 그림2에서 “green”은 eating과 더 가까이 위치해있지만 직접적으로 관련있는 단어는 아닙니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-10-13/sentence-example-attention.png&quot; width=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small&gt;&lt;em&gt;그림2. 한 단어는 같은 문장의 단어들에 서로 다른 방식으로 주목하게 만듭니다.&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;간단히 말해, 딥러닝에서 어텐션은 weights의 중요도 벡터로 설명할수 있습니다. 이미지의 픽셀값이나 문장에서 단어 등 어떤 요소를 예측하거나 추정하기 위해, 다른 요소들과 얼마나 강하게 연관되어 있는지 확인하고(많은 논문들에서 읽은 것처럼) 이것들과 어텐션 백터로 가중 합산된 값의 합계를 타겟값으로 추정할 수 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;whats-wrong-with-seq2seq-model&quot;&gt;What’s Wrong with Seq2Seq Model?&lt;/h2&gt;

&lt;p&gt;seq2seq 모델은 언어 모델링에서 유래되었습니다. 간단히 말해서 입력 시퀀스를 새로운 시퀀스로 변형하는 것을 목적으로 하며, 이때 입력값이나 결과값 모두 임의 길이를 갖습니다. seq2seq의 예로는 기계번역, 질의응답 생성, 문장을 문법 트리로 구문 분석하는 작업 등이 있습니다.&lt;/p&gt;

&lt;p&gt;seq2seq 모델은 보통 인코더-디코더 구조로 이루어져있습니다 :&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;인코더는 입력 시퀀스를 처리하여 고정된 길이의 컨텍스트 벡터(context vector, sentence embedding 또는 thought vector로도 알려진)로 정보를 압축합니다. 이러한 차원 축소된 벡터 표현은 소스 시퀀스의 문맥적인 요약 정보로 간주할수 있습니다.&lt;/li&gt;
  &lt;li&gt;디코더는 컨텍스트 벡터를 다시 처리하여 결과값을 만들어 냅니다. 인코더 네트워크의 결과값을 입력으로 받아 변형을 수행합니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;인코더와 디코더 모두 &lt;a href=&quot;http://colah.github.io/posts/2015-08-Understanding-LSTMs/&quot;&gt;LSTM이나 GRU&lt;/a&gt; 같은 Recurrent Neural Networks 구조를 사용합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-10-13/encoder-decoder-example.png&quot; width=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small&gt;&lt;em&gt;그림3. 인코더-디코더 모델, she is eating a green apple 이란 문장을 중국어로 변형함. 순차적인 방식으로 풀어서 시각화함&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;고정된 길이의 컨텍스트 벡터로 디자인하는 것의 문제점은 아주 긴 문장의 경우, 모든 정보를 다 기억하지 못한다 것입니다. 일단 전체 문장을 모두 처리하고 나면 종종 앞 부분을 잊어버리곤 합니다. 어텐션 메커니즘은 이 문제점을 해결하기 위해 제안되었습니다. (&lt;a href=&quot;https://arxiv.org/pdf/1409.0473.pdf&quot;&gt;Bahdanau et al., 2015&lt;/a&gt;)&lt;/p&gt;

&lt;h2 id=&quot;born-for-translation&quot;&gt;Born for Translation&lt;/h2&gt;
&lt;p&gt;어텐션 메커니즘은 딥러닝 기반의 기계번역(&lt;a href=&quot;https://arxiv.org/pdf/1409.0473.pdf&quot;&gt;NMT&lt;/a&gt;)에서 긴 소스 문장을 기억하기 위해서 만들어졌습니다. 인코더의 마지막 히든 스테이트의 컨텍스트 벡터뿐만아니라, 어텐션을 이용해 컨텍스트 벡터와 전체 소스 문장 사이에 지름길(shortcuts)을 만들어 사용하는 것입니다. 이 지름길의 가중치들은 각 아웃풋 요소들에 맞게 정의할 수 있습니다.&lt;/p&gt;

&lt;p&gt;컨텍스트 벡터는 전체 입력 시퀀스에 접근할수 있고, 잊어 버릴 염려가 없습니다. 소스와 타겟 간의 관계은 컨텍스트 벡터에 의해 학습되고 제어됩니다. 기본적으로 컨텍스트 벡터는 세가지 정보를 사용합니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;인코더 히든 스테이트&lt;/li&gt;
  &lt;li&gt;디코더 히든 스테이트&lt;/li&gt;
  &lt;li&gt;소스와 타겟 사이의 순차적 정보(alignment)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-10-13/encoder-decoder-attention.png&quot; width=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small&gt;&lt;em&gt;그림4. additive attention mechanism이 있는 인코더-디코더 모델 &lt;a href=&quot;https://arxiv.org/pdf/1409.0473.pdf&quot;&gt;Bahdanau et al., 2015&lt;/a&gt;&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;h3 id=&quot;definition&quot;&gt;Definition&lt;/h3&gt;
&lt;p&gt;NMT에서 사용되는 어텐션 메커니즘을 과학적으로 정의해보도록 하겠습니다. 우리는 길이가 &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt;인 소스 문장 &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt;를 이용해 길이가 &lt;script type=&quot;math/tex&quot;&gt;m&lt;/script&gt;인 타겟 문장 &lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt;을 만든다고 해봅시다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbf{x} = [x_1, x_2, ..., x_n] \\
\mathbf{y} = [y_1, y_2, ..., y_m]&lt;/script&gt;

&lt;p&gt;(볼드 표시된 변수는 벡터를 의미합니다. 이하의 모든 내용에 적용됩니다)&lt;/p&gt;

&lt;p&gt;인코더는 &lt;a href=&quot;https://www.coursera.org/lecture/nlp-sequence-models/bidirectional-rnn-fyXnn&quot;&gt;bidirectional RNN&lt;/a&gt;(또는 다른 구조의 RNN를 갖을 수 있습니다)로 히든 스테이트 &lt;script type=&quot;math/tex&quot;&gt;\mathbf{\overrightarrow{h_i}}&lt;/script&gt; 와 반대방향 히든 스테이트 &lt;script type=&quot;math/tex&quot;&gt;\mathbf{\overleftarrow{h_i}}&lt;/script&gt;를 갖습니다. 두 표현식을 간단히 연결(concatenation)하여 인코더의 히든 스테이트를 나타냅니다. 이렇게 하여 한 단어의 앞 뒷 단어를 표시할수 있습니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbf{h_i} = [\mathbf{\overrightarrow{h_i}}^\top; \mathbf{\overrightarrow{h_i}}^\top]^\top, \ i=1, ..., n&lt;/script&gt;

&lt;p&gt;디코더의 히든 스테이트는 t번째 아웃풋 단어를 만들기 위해 &lt;script type=&quot;math/tex&quot;&gt;s_t = f(s_{t-1}, y_{t-1}, c_t)&lt;/script&gt; 로 정의됩니다. 이때, &lt;script type=&quot;math/tex&quot;&gt;c_t&lt;/script&gt;(context vector)는 어라인먼트 스코어를 가중치로 갖는 인코더 히든스테이트의 가중 합계입니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
\mathbf{c_t} &amp; = \sum_{i=1}^{n}\alpha_{t, i} \mathbf{h_i} &amp; ; \ Context \ vector \ for \ output \ y_t \\\\
\alpha_{t,i} &amp; = align(y_t, x_i) &amp; ; \ How \ well \ two \ words \ y_t \ and \ x_i \ are \ aligned. \\\\
 &amp; = \frac{score(s_{t-1}, \mathbf{h_{i^{'}}})}{\sum_{i=1}^{n} score(s_{t-1},\mathbf{h_{i^{'}}})} &amp; ; \ Softmax \ of \ some \ predefined \ alignment \ score. &amp;
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;alignment model은 i번째 입력과 t번째 결과값이 얼마나 잘 매치되는지 확인 한 후  스코어 &lt;script type=&quot;math/tex&quot;&gt;\alpha_{t, i}&lt;/script&gt;를 이 쌍 &lt;script type=&quot;math/tex&quot;&gt;(y_t, x_i)&lt;/script&gt;에 할당합니다. &lt;script type=&quot;math/tex&quot;&gt;{\alpha_{t,i}}&lt;/script&gt;의 집합은 각 소스의 히든 스테이트가 결과값에 어느정도 연관되어 있는지를 정의하는 가중치 입니다. Bahdanau의 논문은 alignment score &lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt;는 한개의 히든 레이어를 가진 &lt;b&gt;feed-forward network&lt;/b&gt;로 파라미터라이즈됩니다. 그리고 이 네트워크는 모델의 다른 부분들과 함께 학습됩니다. 스코어 함수는 아래와 같은 형태이고, tanh는 비선형 활성함수로 사용되었습니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;score(\mathbf{s_t}, \mathbf{h_i}) = \mathbf{v_a^\top} tanh(\mathbf{W_a}[\mathbf{s_t} ; \mathbf{h_i}])&lt;/script&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\mathbf{v_a}&lt;/script&gt; 와 &lt;script type=&quot;math/tex&quot;&gt;\mathbf{W_a}&lt;/script&gt;는 alignment model에서 학습되는 가중치 메트릭스입니다.&lt;/p&gt;

&lt;p&gt;alignment score를 메트릭스로 표시하여 시각적으로 소스 단어와 타겟 단어 사이의 상관관계를 명시적으로 확인할수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-10-13/bahdanau-fig3.png&quot; width=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small&gt;&lt;em&gt;그림5. 프랑스어 “L’accord sur l’Espace économique européen a été signé en août 1992”와 영어 “The agreement on the European Economic Area was signed in August 1992”의 기계번역 모델의 Alignment matrix입니다. (출저 : Fig 3 in &lt;a href=&quot;https://arxiv.org/pdf/1409.0473.pdf&quot;&gt;Bahdanau et al., 2015&lt;/a&gt;)&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;구현 방법은 텐서플로우팀의 &lt;a href=&quot;https://www.tensorflow.org/versions/master/tutorials/seq2seq&quot;&gt;튜토리얼&lt;/a&gt;을 확인하세요.&lt;/p&gt;

&lt;h2 id=&quot;a-family-of-attention-mechanisms&quot;&gt;A Family of Attention Mechanisms&lt;/h2&gt;

&lt;p&gt;어텐션으로 인해서 소스와 타겟 시퀀스간의 의존성은 더이상 둘 간의 거리에 의해 제한되지 않습니다. 어텐션은 기계 번역에서 큰 성과를 보였고, 곧 컴퓨터 비전 분야로 확대되었으며(&lt;a href=&quot;http://proceedings.mlr.press/v37/xuc15.pdf&quot;&gt;Xu et al. 2015&lt;/a&gt;) 다양한 어텐션 메커니즘이 연구되기 시작했습니다.(&lt;a href=&quot;https://arxiv.org/pdf/1508.04025.pdf&quot;&gt;Luong, et al., 2015&lt;/a&gt;;&lt;a href=&quot;https://arxiv.org/abs/1703.03906&quot;&gt;Britz et al., 2017&lt;/a&gt;;&lt;a href=&quot;http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf&quot;&gt;Vaswani, et al., 2017&lt;/a&gt;)&lt;/p&gt;

&lt;h3 id=&quot;summary&quot;&gt;Summary&lt;/h3&gt;

&lt;p&gt;아래는 대표적인 어텐션 메커니즘의 요약 정보입니다(또는 어텐션 메커니즘의 대략적인 분류).&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Name&lt;/th&gt;
      &lt;th&gt;Aligment socre function&lt;/th&gt;
      &lt;th&gt;citation&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Additive(*)&lt;/td&gt;
      &lt;td&gt;&lt;script type=&quot;math/tex&quot;&gt;score(\mathbf{s}_t&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;\mathbf{h}_i&lt;/script&gt;) = &lt;script type=&quot;math/tex&quot;&gt;\mathbf{v}_a^\top tanh(\mathbf{W}_a[\mathbf{s}_t; \mathbf{h}_i]&lt;/script&gt;)&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://arxiv.org/pdf/1409.0473.pdf&quot;&gt;Bahdanau2015&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Location-Base&lt;/td&gt;
      &lt;td&gt;&lt;script type=&quot;math/tex&quot;&gt;\alpha_{t,i} = softmax(\mathbf{W}_a \mathbf{s}_t)&lt;/script&gt; &lt;br /&gt; Note : This simplifies the softmax alignment max to only depend on the target position.&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://arxiv.org/pdf/1508.04025.pdf&quot;&gt;Luong2015&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;General&lt;/td&gt;
      &lt;td&gt;&lt;script type=&quot;math/tex&quot;&gt;score(\mathbf{s}_t, \mathbf{h}_i)=\mathbf{s}_t^\top \mathbf{W}_a \mathbf{h}_i&lt;/script&gt; &lt;br /&gt; where &lt;script type=&quot;math/tex&quot;&gt;\mathbf{W}_a&lt;/script&gt; is a trainable weight matrix in the attention layer.&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://arxiv.org/pdf/1508.04025.pdf&quot;&gt;Luong2015&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Dot-Product&lt;/td&gt;
      &lt;td&gt;&lt;script type=&quot;math/tex&quot;&gt;score(\mathbf{s}_t, \mathbf{h}_i) = \mathbf{s}_t^\top \mathbf{h}_i&lt;/script&gt;&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://arxiv.org/pdf/1508.04025.pdf&quot;&gt;Luong2015&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Scaled Dot-Product(^)&lt;/td&gt;
      &lt;td&gt;&lt;script type=&quot;math/tex&quot;&gt;score(\mathbf{s}_t, \mathbf{h}_i) =&lt;/script&gt;  &lt;script type=&quot;math/tex&quot;&gt;{\mathbf{s}_t^\top \mathbf{h}_i}\over{\sqrt{n}}&lt;/script&gt; &lt;br /&gt; Note: very similar to dot-product attention except for a scaling factor; where n is the dimension of the source hidden state.&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf&quot;&gt;Vaswani2017&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Self-Attention(&amp;amp;)&lt;/td&gt;
      &lt;td&gt;Retating different position of the same input sequence. Theoretically the self-attention can adopt any score functions above, but just replace the target sequence with the same input sequence.&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://arxiv.org/pdf/1601.06733.pdf&quot;&gt;Cheng2016&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Global/Soft&lt;/td&gt;
      &lt;td&gt;Attending to the entire input state space.&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;http://proceedings.mlr.press/v37/xuc15.pdf&quot;&gt;Xu2015&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Local/Hard&lt;/td&gt;
      &lt;td&gt;Attending to the part of input state space; i.e. a patch of the input image.&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;http://proceedings.mlr.press/v37/xuc15.pdf&quot;&gt;Xu2015&lt;/a&gt;;&lt;a href=&quot;https://arxiv.org/pdf/1508.04025.pdf&quot;&gt;Luong2015&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;(*) 이 방식은 Luong, et al., 2015 에서는 “concat”이라고 언급되었으며, Vaswani, et al., 2017에서는 “additive attention”이라고 언급되었습니다.&lt;/p&gt;

&lt;p&gt;(^)인풋이 매우 길어서 소프트맥스 함수의 그래디언트가 아주 작아져 학습이 어려운 경우를 보완하기 위해서 스케일링 펙터, &lt;script type=&quot;math/tex&quot;&gt;1/\sqrt{n}&lt;/script&gt;,가 더해진 것입니다.&lt;/p&gt;

&lt;p&gt;(&amp;amp;) Cheng et al., 2016 등 다른 논문들에서는 intra-attention이라고도 불리웁니다.&lt;/p&gt;

&lt;h3 id=&quot;self-attention&quot;&gt;Self-Attention&lt;/h3&gt;

&lt;p&gt;&lt;b&gt;Self-attetion&lt;/b&gt;, 또는 &lt;b&gt;intra-attention &lt;/b&gt;으로 알려진 어텐션 메커니즘은 시퀀스의 representation을 계산하기 위해 시퀀스의 서로 다른 포지션과 연관된 방법입니다. 기계 판독, 추상 요약 또는 이미지 설명 생성에 매우 유용합니다.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1601.06733.pdf&quot;&gt;long short-term memory network&lt;/a&gt; 논문에서 기계판독 문제를 해결하기위해 셀프어텐션 기법을 사용하였습니다. 아래 예제와 같이 셀프 어텐션 메커니즘을 통해 현재 단어와 이전 단어들간의 상관관계를 학습할수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-10-13/cheng2016-fig1.png&quot; width=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small&gt;&lt;em&gt;그림6. 현재 단어는 빨간색으로 표시하였고, 파란색 그림자의 크기는 엑티베이션 정도를 나타남(출저 : &lt;a href=&quot;https://arxiv.org/pdf/1601.06733.pdf&quot;&gt;Cheng et al., 2016&lt;/a&gt;)&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://proceedings.mlr.press/v37/xuc15.pdf&quot;&gt;show, attend and tell&lt;/a&gt; 논문에서는 셀프어텐션을 이미지에 적용하여 적절한 설명 문구을 생성하였습니다. 이미지는 먼저 컨볼루션 뉴럴 넷을 이용해 인코딩되었고, 인코딩된 피쳐 멥을 인풋으로하는 리커런트 네트워크(셀프 어텐션이 적용된)를 이용해 묘사하는 단어를 하나 하나 생성하였습니다. 어텐션 가중치를 시각화한 결과, 모델이 특정 단어를 생성할 때 이미지에서 어떤 영역을 중점으로 반영하는지 확인할 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-10-13/xu2015-fig6b.png&quot; width=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small&gt;&lt;em&gt;그림7. “A woman is throwing a frisbee in a park.” (Image source: Fig. 6(b) in &lt;a href=&quot;http://proceedings.mlr.press/v37/xuc15.pdf&quot;&gt;Xu et al. 2015&lt;/a&gt;)&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;h3 id=&quot;soft-vs-hard-attention&quot;&gt;Soft vs Hard Attention&lt;/h3&gt;
&lt;p&gt;어텐션의 또 다른 정의 방식은 soft와 hard 어텐션입니다. 기본적인 아이디어는 &lt;a href=&quot;http://proceedings.mlr.press/v37/xuc15.pdf&quot;&gt;show, attend and tell&lt;/a&gt; 논문에서 제안되었습니다. 어텐션이 전체 이미지를 대상으로하는지 혹은 일부 패치 영역을 대상으로 하는지에 따라 :&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;soft attention : 가중치가 학습되어, 소스 이미지의 모든 패치에 “소프트하게” 맵핑됨; 근본적으로 &lt;a href=&quot;https://arxiv.org/pdf/1409.0473.pdf&quot;&gt;Bahdanau et al., 2015&lt;/a&gt;와 유사함
    &lt;ul&gt;
      &lt;li&gt;장점 : 모델이 스무스하고 미분가능함&lt;/li&gt;
      &lt;li&gt;단점 : 소스 이미지가 클 때 계산비용이 큼&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;hard attention : 이미지의 일부 패치영역이 한번에 하나씩 선택되는 방식
    &lt;ul&gt;
      &lt;li&gt;장점 : 인퍼런스에서 더 적은 계산 비용&lt;/li&gt;
      &lt;li&gt;단점 : 모델이 미분불가능하고, 학습 시 variance reduction이나 reinforcement learning같은 더 복잡한 기법들이 필요함 (&lt;a href=&quot;https://arxiv.org/pdf/1508.04025.pdf&quot;&gt;Luong et al., 2015&lt;/a&gt;)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;global-vs-local-attention&quot;&gt;Global vs Local Attention&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1508.04025.pdf&quot;&gt;Luong et al., 2015&lt;/a&gt;)에서는 global과 local 어텐션을 제안하였습니다. 글로벌 어텐션은 소프트 어텐션과 유사하고, 로컬 어텐션은 하드와 소프트 개념이 모두 이용해 미분가능하도록 만든 하드 어텐션이라고 생각할수 있습니다. 현재 타겟 단어를 위해 한개의 포지션을 예측하고 소스 포지션 주위로 센터된 윈도우을 이용해 컨텍스트 벡터를 계산합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-10-13/luong2015-fig2-3.png&quot; width=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small&gt;&lt;em&gt;그림8. “글로벌 vs 로컬 어텐션” (Image source: Fig 2 &amp;amp; 3 in &lt;a href=&quot;https://arxiv.org/pdf/1508.04025.pdf&quot;&gt;Luong et al., 2015&lt;/a&gt;)&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;h2 id=&quot;transformer&quot;&gt;Transformer&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf&quot;&gt;“Attention is All you Need”&lt;/a&gt;(Vaswani, et al., 2017), 는 2017년 논문중에서 가장 임팩트있고 흥미로운 논문입니다. 기존 소프트 어텐션 방식을 대폭 개선시키고 &lt;em&gt;recurrent network units없이&lt;/em&gt; seq2seq를 모델링할수 있다는 것을 보였습니다. &lt;b&gt;transformer&lt;/b&gt;라는 것을 제안하여 순차적인 계산 구조 없이 셀프 어텐션 메커니즘을 구현할수 있습니다.&lt;/p&gt;

&lt;p&gt;핵심은 바로 모델 구조에 있습니다.&lt;/p&gt;

&lt;h3 id=&quot;key-value-and-query&quot;&gt;key, Value and Query&lt;/h3&gt;
&lt;p&gt;가장 중요한 부분은 &lt;em&gt;multi-head self-attention mechanism&lt;/em&gt;입니다. 트랜스포머는 인풋의 인코딩된 representation을 &lt;b&gt;key-value&lt;/b&gt; 쌍, &lt;script type=&quot;math/tex&quot;&gt;(\mathbf{K, V})&lt;/script&gt;의 집합체로 보았습니다; 둘다 n(인풋 시퀀스 길이)차원 벡터로 인코더의 히든 스테이트에 해당. 디코더에서 이전 결과값들은 &lt;b&gt;query&lt;/b&gt;(&lt;script type=&quot;math/tex&quot;&gt;\mathbf{Q}&lt;/script&gt; of dimension m)로 압축되고, 다음 아웃풋은 이 쿼리와 키-벨류 셋트를 맵핑함으로써 계산됩니다.&lt;/p&gt;

&lt;p&gt;트렌스포머는 &lt;a href=&quot;&quot;&gt;scaled dot-product attention&lt;/a&gt;을 사용하였습니다: 아웃풋은 가중합산된 값이고, 가중치들은 쿼리와 키값들의 dot-product로 결정됩니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Attention(\mathbf{Q, K, V}) = softmax( {\mathbf{Q}\mathbf{K}^\top \over {\sqrt{n}}} )\mathbf{V}&lt;/script&gt;

&lt;h3 id=&quot;multi-head-self-attention&quot;&gt;multi-Head Self-Attention&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-10-13/multi-head-attention.png&quot; width=&quot;300&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small&gt;&lt;em&gt;그림9. 멀티-헤드 스케일드 닷-프로덕트 어텐션 메커니즘 (Image source: Fig 2 in &lt;a href=&quot;http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf&quot;&gt;Vaswani, et al., 2017&lt;/a&gt;)&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;어텐션을 한번만 계산하는 것보다 멀티-헤드 메커니즘은 스케일 닷-프로덕트 어텐션을 병렬로 여러번 계산된다. 독립적인 어텐션 아웃풋은 단순히 concatenated되며, 선형으로 예상되는 차원으로 변형됩니다. 이렇게 하는 이유는 앙상블은 항상 도움이 되기 때문이 아닐까요? 논문에 따르면 “multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this (멀티-헤드 어텐션은 서로 다른 representation 공간에 있는 포지션 정보를 결합하여 이용할수 있게 해줍니다. 싱글 어텐션 헤드를 이용하면 이런 정보들이 서로 평균화되어 버립니다.)&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;MultiHead(\mathbf{Q, K, V}) = [head_1; ... ; head_h]\mathbf{W}^O \\
where \ head_i = Attenton(\mathbf{QW}_i^Q, \mathbf{KW}_i^K, \mathbf{VW}_i^V)&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;\mathbf{W}_i^Q, \mathbf{W}_i^K, \mathbf{W}_i^V&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\mathbf{W}^O&lt;/script&gt; are parameter matrics to be learned.&lt;/p&gt;

&lt;h3 id=&quot;encoder&quot;&gt;Encoder&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-10-13/transformer-encoder.png&quot; width=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small&gt;&lt;em&gt;그림10. 트랜스포머의 인코더 (Image source: &lt;a href=&quot;http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf&quot;&gt;Vaswani, et al., 2017&lt;/a&gt;)&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;인코더는 무한히 클수있는 문백에서 특정 정보 조각을 찾을수 있도록 어텐션 기반의 representation을 생성합니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;동일한 6개의 레이어를 쌓습니다.&lt;/li&gt;
  &lt;li&gt;각 레이어는 멀티-헤드 셀프어텐션 레이어와 포지션-와이즈 풀리 커넥티드 피드-포워드 네트워크를 서브 레이어로 갖습니다.&lt;/li&gt;
  &lt;li&gt;각 서브 레이어는 &lt;a href=&quot;https://arxiv.org/pdf/1512.03385.pdf&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;residual&lt;/code&gt;&lt;/a&gt; 커넥션과 &lt;code class=&quot;highlighter-rouge&quot;&gt;layer normalization&lt;/code&gt; 이 적용됩니다. 모든 서브 레이어는 &lt;script type=&quot;math/tex&quot;&gt;d_{model}=512&lt;/script&gt;로 동일한 차원의 아웃풋을 갖습니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;decoder&quot;&gt;Decoder&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-10-13/transformer-decoder.png&quot; width=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small&gt;&lt;em&gt;그림11. 트랜스포머의 디코더 (Image source: &lt;a href=&quot;http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf&quot;&gt;Vaswani, et al., 2017&lt;/a&gt;)&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;디코더는 인코딩된 representation으로부터 정보를 다시 되돌리는 역할을 합니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;동일한 6개의 레이어를 쌓습니다.&lt;/li&gt;
  &lt;li&gt;각 레이어는 멀티-헤드 셀프어텐션 레이어와 포지션-와이즈 풀리 커넥티드 피드-포워드 네트워크를 서브 레이어로 갖습니다.&lt;/li&gt;
  &lt;li&gt;인코더와 유사하게 각 서브 레이어는 residual 커넥션과 레이어 노말리제이션이 적용됩니다.&lt;/li&gt;
  &lt;li&gt;첫번째 서브레이어의 멀티-헤드 어텐션은 타겟 시퀀스의 미래을 보는 것은 관심이 없으므로, 현재 위치 이후의 포지션 정보는 이용하지 않도록 변형됩니다. (현재 포지션의 이전 정보만 이용하도록)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;full-architecture&quot;&gt;Full Architecture&lt;/h3&gt;

&lt;p&gt;트렌스포머의 전체적인 구조는 다음과 같습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;먼저 소스와 타겟 시퀀스 모두 동일한 디멘션 &lt;script type=&quot;math/tex&quot;&gt;d_{model} = 512&lt;/script&gt;을 갖도록 임베딩 레이어를 거칩니다.&lt;/li&gt;
  &lt;li&gt;포지션 정보를 유지하기 위해 sinusoid-wave-based positional encoding을 적용한 후 임베딩 아웃풋과 합칩니다.&lt;/li&gt;
  &lt;li&gt;마지막 디코더 아웃풋에 소프트맥스와 선형 레이어가 추가됩니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-10-13/transformer.png&quot; width=&quot;600&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small&gt;&lt;em&gt;그림12. 트랜스포머의 전체 모델 구조 (Image source: Fig 1&amp;amp; 2 in &lt;a href=&quot;http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf&quot;&gt;Vaswani, et al., 2017&lt;/a&gt;)&lt;/em&gt;&lt;/small&gt;&amp;gt;&lt;/p&gt;

&lt;h2 id=&quot;snail&quot;&gt;SNAIL&lt;/h2&gt;

&lt;p&gt;트랜스포머는 리커런트 또는 컨볼루션 구조를 사용하지 않고, 임베딩 벡터에 포지션 인코딩이 더해지긴 하지만 시퀀스의 순서는 약하게 통합되는 수준입니다. &lt;a href=&quot;https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html&quot;&gt;강화 학습&lt;/a&gt;과 같이 위치 종속성에 민감한 경우, 큰 문제가 될 수 있습니다. 
&lt;b&gt;Simple Neural Attention &lt;a href=&quot;http://bair.berkeley.edu/blog/2017/07/18/learning-to-learn/&quot;&gt;Meta-Learner&lt;/a&gt;(SNAIL)&lt;/b&gt;&lt;a href=&quot;http://metalearning.ml/papers/metalearn17_mishra.pdf&quot;&gt;Mishra et al., 2017&lt;/a&gt;는 트랜스포머의 셀프-어텐션 메커니즘과 &lt;a href=&quot;https://deepmind.com/blog/wavenet-generative-model-raw-audio/&quot;&gt;시간적 컨볼루션&lt;/a&gt;을 결합하여 &lt;a href=&quot;#full-architecture&quot;&gt;포지션 문제&lt;/a&gt;를 부분적으로 개선하기 위해 제안되었습니다. SNAIL은 지도학습과 강화학습 모두에서 좋은 결과를 보입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-10-13/snail.png&quot; width=&quot;600&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small&gt;&lt;em&gt;그림13. SNAIL 모델 구조 (Image source: &lt;a href=&quot;http://metalearning.ml/papers/metalearn17_mishra.pdf&quot;&gt;Mishra et al., 2017&lt;/a&gt;)&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;SNAIL은 그 자체만으로도 중요한 토픽인 메타-러닝 분야에서 최초 제안되었습니다. 간단히 말해서 메타 러닝 모델은 비슷한 분포에서 nevel, unseen tasks들에 일반화할수 있습니다. 더 자세한 정보는 &lt;a href=&quot;http://bair.berkeley.edu/blog/2017/07/18/learning-to-learn/&quot;&gt;이 글&lt;/a&gt;을 확인하세요.&lt;/p&gt;

&lt;h2 id=&quot;self-attention-gan&quot;&gt;Self-Attention GAN&lt;/h2&gt;
&lt;p&gt;마지막으로 &lt;a href=&quot;https://lilianweng.github.io/lil-log/2017/08/20/from-GAN-to-WGAN.html&quot;&gt;Generative Adversarial Network (GAN)&lt;/a&gt;타입의 모델인, `self-attention GAN(SAGAN; &lt;a href=&quot;https://arxiv.org/pdf/1805.08318.pdf&quot;&gt;Zhang et al., 2018&lt;/a&gt;)을 통해서 어텐션이 생성이미지의 퀄리티를 향상시키는지 설명하도록 하겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1511.06434&quot;&gt;DCGAN&lt;/a&gt;(Deep Convolutional GAN)에서 discriminator와 generator은 멀티-레이어 컨볼루션 네트워크입니다. 하지만 하나의 픽셀은 작은 로컬 영역으로 제한되기 때무네, 네트워크의 representation capacity는 필터 사이즈에 의해 제한됩니다. 멀리 떨어진 영역을 연결하기 위해서 피쳐들이 컨볼루션 오퍼레이션을 통해 희석되어야하여 종속성이 유지되는 것이 보장되지 않습니다.&lt;/p&gt;

&lt;p&gt;비전 컨텍스트에서 (소프트) 셀프-어텐션은 한 픽셀과 다른 포지션의 픽셀들간에 관계를 명시적으로 학습하도록 설계되어 있습니다. 멀리 떨어진 영역이더라도 쉽게 글로벌 디펜던시를 학습할수 있습니다. 따라서 셀프-어텐션이 적용된 GAN은 디테일한 정보를 더 잘 처리할수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-10-13/conv-vs-self-attention.png&quot; width=&quot;600&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small&gt;*그림14. 컨볼루션 오퍼레이션과 셀프-어텐션은 서로 다른 사이즈의 영역을 다룹니다. *&lt;/small&gt;&amp;gt;&lt;/p&gt;

&lt;p&gt;SAGAN은 어텐션 계산을 위해서 &lt;a href=&quot;https://arxiv.org/pdf/1711.07971.pdf&quot;&gt;non-local neural network&lt;/a&gt;를 도입하였습니다. 컨볼루셔널 이미지 피쳐맵 &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt;는 3개로 복제되어 나눠지며, 이는 트랜스포머에서 각 각 &lt;a href=&quot;#key-value-and-query&quot;&gt;key, value, and query&lt;/a&gt; 개념에 대응됩니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Key : &lt;script type=&quot;math/tex&quot;&gt;f(x)=W_fx&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;Query : &lt;script type=&quot;math/tex&quot;&gt;g(x)=W_gx&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;Value : &lt;script type=&quot;math/tex&quot;&gt;h(x)=W_hx&lt;/script&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;그리고 나서 dot-product 어텐션을 셀프-어텐션 피쳐맵에 적용합니다 :&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\alpha_{i, j} = softmax(f(\mathbf{x}_i)^{\top}g(\mathbf{x}_j))\\
\mathbf{o}_j = \sum_{i=1}^{N} \alpha_{i,j}h(\mathbf{x}_i)&lt;/script&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-10-13/self-attention-gan-network.png&quot; width=&quot;600&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small&gt;*그림15. SAGAN에서 셀프-어텐션 메커니즘 (Image source : Fig 2 in &lt;a href=&quot;https://arxiv.org/pdf/1805.08318.pdf&quot;&gt;Zhang et al., 2018&lt;/a&gt;) *&lt;/small&gt;&amp;gt;&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\alpha_{i,j}&lt;/script&gt;는 j번째 위치를 합성할 때 모델이 i번째 위치에 얼마나 많은 주의를 기울여야하는지를 나타내는 어텐션 맵의 엔트리입니다. &lt;script type=&quot;math/tex&quot;&gt;\mathbf{W}_f, \mathbf{W}_g, \mathbf{W}_h&lt;/script&gt;는 1x1 컨볼루션 필터입니다. 만약 1x1 conv가 이상하다고 생각되면(단순히 피쳐맵 전체 값에 한개 값을 곱하는 것 아니냐?라고 생각한다면) 앤드류 응의 &lt;a href=&quot;https://www.youtube.com/watch?v=9EZVpLTPGz8&quot;&gt;튜토리얼&lt;/a&gt;을 보세요. 아웃풋 &lt;script type=&quot;math/tex&quot;&gt;\mathbf{o}_j&lt;/script&gt;는 마지막 아웃풋 &lt;script type=&quot;math/tex&quot;&gt;\mathbf{o} = (\mathbf{o}_1, \mathbf{o}_2, ..., \mathbf{o}_j, ..., \mathbf{o}_N)&lt;/script&gt;의 컬럼 벡터입니다.&lt;/p&gt;

&lt;p&gt;추가로 어텐션 레이어의 아웃풋에 스케일 파라미터를 곱하고, 오리지날 인풋 피쳐맵을 더해줍니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbf{y} = \mathbf{x}_i + \rho \mathbf{o}_i&lt;/script&gt;

&lt;p&gt;스케일링 파라미터 &lt;script type=&quot;math/tex&quot;&gt;\rho&lt;/script&gt;는 학습과정에서 0에서 점차 증가하고, 네트워크는 처음에는 로컬 영역에만 의존하다가 점차 멀리있는 영역에 더 많은 가중치를 주는 방법을 배우도록 구성됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-10-13/SAGAN-examples.png&quot; width=&quot;600&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small&gt;*그림16. SAGAN에 의해 생성된 이미지(128x128) 예들 (Image source : partial Fig 6 in &lt;a href=&quot;https://arxiv.org/pdf/1805.08318.pdf&quot;&gt;Zhang et al., 2018&lt;/a&gt;) *&lt;/small&gt;&amp;gt;&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;p&gt;[0] &lt;a href=&quot;https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html&quot;&gt;lilianweng의 Attention? Attention!&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[1] &lt;a href=&quot;http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/&quot;&gt;“Attention and Memory in Deep Learning and NLP.”&lt;/a&gt; - Jan 3, 2016 by Denny Britz&lt;/p&gt;

&lt;p&gt;[2] &lt;a href=&quot;https://www.tensorflow.org/versions/master/tutorials/seq2seq&quot;&gt;“Neural Machine Translation (seq2seq) Tutorial”&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[3] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. &lt;a href=&quot;https://arxiv.org/pdf/1409.0473.pdf&quot;&gt;“Neural machine translation by jointly learning to align and translate.”&lt;/a&gt; ICLR 2015.&lt;/p&gt;

&lt;p&gt;[4] Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich Zemel, and Yoshua Bengio. &lt;a href=&quot;http://proceedings.mlr.press/v37/xuc15.pdf&quot;&gt;“Show, attend and tell: Neural image caption generation with visual attention.”&lt;/a&gt; ICML, 2015.&lt;/p&gt;

&lt;p&gt;[5] Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. &lt;a href=&quot;https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf&quot;&gt;“Sequence to sequence learning with neural networks.”&lt;/a&gt;NIPS 2014.&lt;/p&gt;

&lt;p&gt;[6] Thang Luong, Hieu Pham, Christopher D. Manning. &lt;a href=&quot;https://arxiv.org/pdf/1508.04025.pdf&quot;&gt;“Effective Approaches to Attention-based Neural Machine Translation.”&lt;/a&gt; EMNLP 2015.&lt;/p&gt;

&lt;p&gt;[7] Denny Britz, Anna Goldie, Thang Luong, and Quoc Le. &lt;a href=&quot;https://arxiv.org/abs/1703.03906&quot;&gt;“Massive exploration of neural machine translation architectures.”&lt;/a&gt; ACL 2017.&lt;/p&gt;

&lt;p&gt;[8] Ashish Vaswani, et al. “Attention is all you need.” NIPS 2017. http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf&lt;/p&gt;

&lt;p&gt;[9] Jianpeng Cheng, Li Dong, and Mirella Lapata. &lt;a href=&quot;https://arxiv.org/pdf/1601.06733.pdf&quot;&gt;“Long short-term memory-networks for machine reading.”&lt;/a&gt; EMNLP 2016.&lt;/p&gt;

&lt;p&gt;[10] Xiaolong Wang, et al. &lt;a href=&quot;https://arxiv.org/pdf/1711.07971.pdf&quot;&gt;“Non-local Neural Networks.”&lt;/a&gt; CVPR 2018&lt;/p&gt;

&lt;p&gt;[11] Han Zhang, Ian Goodfellow, Dimitris Metaxas, and Augustus Odena. &lt;a href=&quot;https://arxiv.org/pdf/1805.08318.pdf&quot;&gt;“Self-Attention Generative Adversarial Networks.”&lt;/a&gt; arXiv preprint arXiv:1805.08318 (2018).&lt;/p&gt;

&lt;p&gt;[12] Nikhil Mishra, Mostafa Rohaninejad, Xi Chen, and Pieter Abbeel. &lt;a href=&quot;http://metalearning.ml/papers/metalearn17_mishra.pdf&quot;&gt;“A simple neural attentive meta-learner.”&lt;/a&gt; NIPS Workshop on Meta-Learning. 2017.&lt;/p&gt;

&lt;p&gt;[13] &lt;a href=&quot;https://deepmind.com/blog/wavenet-generative-model-raw-audio/&quot;&gt;“WaveNet: A Generative Model for Raw Audio”&lt;/a&gt; - Sep 8, 2016 by DeepMind.&lt;/p&gt;</content><author><name>yjucho</name></author><summary type="html">이 글은 lilianweng의 Attention? Attention! 포스팅을 번역한 글입니다.Attention은 최근 딥러닝 커뮤니티에서 자주 언급되는 유용한 툴입니다. 이 포스트에서는 어떻게 어텐션 개념과 다양한 어텐션 메커니즘을 설명하고 transformer와 SNAIL과 같은 모델들에 대해서 알아보고자 합니다.</summary></entry><entry><title type="html">cs231n - 이해하기 2</title><link href="http://localhost:4000/cs231n/cs231n-2/" rel="alternate" type="text/html" title="cs231n - 이해하기 2" /><published>2018-10-11T00:00:00+09:00</published><updated>2018-10-10T00:00:00+09:00</updated><id>http://localhost:4000/cs231n/cs231n-2</id><content type="html" xml:base="http://localhost:4000/cs231n/cs231n-2/">&lt;p&gt;cs231n&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;http://cs231n.stanford.edu/&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;이 포스팅은 딥러닝에 대한 기본 지식을 상세히 전달하기보다는 간략한 핵심과 실제 모델 개발에 유용한 팁을 위주로 정리하였습니다.&lt;/p&gt;

&lt;h2 id=&quot;detection-and-segmentation&quot;&gt;Detection and Segmentation&lt;/h2&gt;

&lt;p&gt;1) semantic segmentation :&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;sliding window&lt;/li&gt;
  &lt;li&gt;Fully convolutional : labeling class per every pixel
    &lt;ul&gt;
      &lt;li&gt;downsampling and upsampling : how to upsampling(unpooling)
        &lt;ul&gt;
          &lt;li&gt;nearest neighbor&lt;/li&gt;
          &lt;li&gt;bed of nails&lt;/li&gt;
          &lt;li&gt;max unpooling(remember which element was max)&lt;/li&gt;
          &lt;li&gt;Transpose Convolution&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-10-11/cs231n-01.png&quot; width=&quot;300&quot; /&gt;
&lt;img src=&quot;/assets/img/2018-10-11/cs231n-02.png&quot; width=&quot;300&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-10-11/cs231n-03.png&quot; width=&quot;300&quot; /&gt;
&lt;img src=&quot;/assets/img/2018-10-11/cs231n-04.png&quot; width=&quot;300&quot; /&gt;&lt;/p&gt;

&lt;p&gt;2) classification + localization :&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;class score : softmax loss&lt;/li&gt;
  &lt;li&gt;box coordiantes(x, y, w, h) : L2 loss
    &lt;ul&gt;
      &lt;li&gt;treat localization as a regression problem&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;multimodal - how to determine weight of two different loss function?
loss값 외에 다른 지표를 참고&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;aside : Human pose estimation&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;3) Object Detection&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;fixed set of categories and draw box location&lt;/li&gt;
  &lt;li&gt;benchmark dataset : PASCAL VOC&lt;/li&gt;
  &lt;li&gt;each image needs a different number of outputs - not easy to solve with regression&lt;/li&gt;
  &lt;li&gt;sliding window : crops of the image, CNN classifies each crop as object or background
    &lt;ul&gt;
      &lt;li&gt;how to choose crop? need to apply CNN to huge number of locations and scales very expensive&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;region proposals : selective search gives 1000 region proposal -&amp;gt; brute force but high recall&lt;/li&gt;
  &lt;li&gt;R-CNN
    &lt;ul&gt;
      &lt;li&gt;region of interest(RoI) from a proposal method (~2k)&lt;/li&gt;
      &lt;li&gt;Warped image regions&lt;/li&gt;
      &lt;li&gt;forward each region through convNet&lt;/li&gt;
      &lt;li&gt;classify regions with SVMs&lt;/li&gt;
      &lt;li&gt;Box regression
        &lt;blockquote&gt;
          &lt;p&gt;slow train and inference&lt;/p&gt;
        &lt;/blockquote&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;fast R-CNN
    &lt;ul&gt;
      &lt;li&gt;Forward whole image through ConvNet&lt;/li&gt;
      &lt;li&gt;RoIs from proposal method on convnet feature map of image&lt;/li&gt;
      &lt;li&gt;RoI pooling layer&lt;/li&gt;
      &lt;li&gt;fully connected&lt;/li&gt;
      &lt;li&gt;classification and regression&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;faster R-CNN
    &lt;ul&gt;
      &lt;li&gt;make CNN do proposals&lt;/li&gt;
      &lt;li&gt;insert &lt;em&gt;region proposal network(RPN)&lt;/em&gt; to predict proposals from features&lt;/li&gt;
      &lt;li&gt;jointly train with 4 lossess&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-10-11/cs231n-05.png&quot; width=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;detection without proposals : YOLO / SSD&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-10-11/cs231n-06.png&quot; width=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;p&gt;4) Instance Segmentation&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Mask R-CNN - similar to faster R-CNN
    &lt;ul&gt;
      &lt;li&gt;can also does pose : add joint coordinates&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;bechmark data : microsoft coco data&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;visualizing-and-understanding&quot;&gt;Visualizing and Understanding&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;what’s going on inside ConvNets?
What are the intermediate features looking for?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;visualize the filters : raw weights
    &lt;ul&gt;
      &lt;li&gt;not that interesting&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Last Layer :
    &lt;ul&gt;
      &lt;li&gt;check Nearest Neighbors in faeture space(last fc layer)&lt;/li&gt;
      &lt;li&gt;dimensionality reduction : PCA, t-SNE&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Occlusion Experiments : 부분적으로 마스크함
    &lt;ul&gt;
      &lt;li&gt;mask한 영역으로 인해 확률이 극격히 변화면 해당 영역은 크리티컬하다고 가정&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;saliency maps : 이미지의 각 픽셀들에 대해서 클래스 스코어의 그래디언트를 구함. compute gradient of class score with respect to image pixels
    &lt;ul&gt;
      &lt;li&gt;intermediate feature via guided backprop : which part of image impact to intermediate activation value&lt;/li&gt;
      &lt;li&gt;relu : positive gradient만 이용하면 더 나이스 이미지를 얻을수 있다&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;gradient ascent : 지금까지는 보통의 백프로파게이션을 통해 이미지의 어떤 부분이 뉴련에 영향을 주는지 알아봤다면(고정된 입력 이미지 값), 그래디언트 어센트는 뉴런의 액티베이션을 최대화하는 방향으로 이미지를 만들어내는 것임(입력 이미지 값을 생성하는 것)
    &lt;ul&gt;
      &lt;li&gt;generate a synthetic image that maximally activates a neuron&lt;/li&gt;
      &lt;li&gt;better regrularizer (image prior regualarization)&lt;/li&gt;
      &lt;li&gt;optimize in FC6 latent space instead of pixel space&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Fooling Image
    &lt;ul&gt;
      &lt;li&gt;엘리퍼튼 이미지를 고르고&lt;/li&gt;
      &lt;li&gt;코알라 클래스 스코어를 골라&lt;/li&gt;
      &lt;li&gt;코알라 클래스 스코어를 최대화하도록 이미지를 모디파이&lt;/li&gt;
      &lt;li&gt;네트워크가 코알라로 분류할때까지 반복&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-10-11/cs231n-07.png&quot; width=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;DeepDream
    &lt;ul&gt;
      &lt;li&gt;choose an image and a layer in a CNN : repeat:
        &lt;ul&gt;
          &lt;li&gt;Forward : compute activations at chosen layer&lt;/li&gt;
          &lt;li&gt;set gradient of chosen layer equal to its activation&lt;/li&gt;
          &lt;li&gt;backward : compute gradient on image&lt;/li&gt;
          &lt;li&gt;update image&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Feature Inversion : 피쳐벡터를 뽑고, 그 피쳐벡터에 매칭되는 다른 입풋 이미지를 만들어냄&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-10-11/cs231n-08.png&quot; width=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Texture sythesis : given a sample patch of some texture, can we genrate a bigger image of the same texture?
    &lt;ul&gt;
      &lt;li&gt;classical approch : nearest&lt;/li&gt;
      &lt;li&gt;neural texture synthesis : gram matrix&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Style Transfer&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-10-11/cs231n-09.png&quot; width=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;slow  : train another model to transfer style&lt;/li&gt;
  &lt;li&gt;fast style transfer&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-10-11/cs231n-10.png&quot; width=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;generative-models&quot;&gt;Generative Models&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;addresses density estimation&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;generative models of time-series data can be used for simulation and planning&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Fully visible belief network&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-10-11/cs231n-11.png&quot; width=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;픽셀의 오더를 어떻게 결정하지? –&amp;gt; pixelRNN&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;pixelRNN
    &lt;ul&gt;
      &lt;li&gt;코너에 있는 픽셀부터 다이어고날 방향으로 시퀄셜로 학습 using RNN (LSTM)&lt;/li&gt;
      &lt;li&gt;sequtional is slow&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;pixelCNN
    &lt;ul&gt;
      &lt;li&gt;코너에 있는 픽샐부터 시작하는 것은 같으나&lt;/li&gt;
      &lt;li&gt;context region(previous pixels)으로부터 모델링되는 것&lt;/li&gt;
      &lt;li&gt;training is faster but generation must still process sequentially&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Variational auto-encoder
    &lt;ul&gt;
      &lt;li&gt;intractible to compute p(x|z) for every z&lt;/li&gt;
      &lt;li&gt;in addition to decoder p&lt;sub&gt;θ&lt;/sub&gt;(x|z), define additional encoder q&lt;sub&gt;φ&lt;/sub&gt;(z|x) that approximates p&lt;sub&gt;θ&lt;/sub&gt;(z|x)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-10-11/cs231n-12.png&quot; width=&quot;300&quot; /&gt; 
&lt;img src=&quot;/assets/img/2018-10-11/cs231n-13.png&quot; width=&quot;300&quot; /&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
log p_\theta(x^{(i)}) &amp; =  E_z[log p_\theta(x^{(i)}|z)] - D_{KL}(q_\phi(z|x^{(i)}||p_\theta(z))) + D_{KL}(q_\phi(z|x^{(i)}||p_\theta(z|x^{(i)}))) \\ \\
E_z[log p_\theta(x^{(i)}|z)] &amp; \  reconstruct \ the \ input \ data \\ \\
D_{KL}(q_\phi(z|x^{(i)}||p_\theta(z))) &amp; \  make \ approximate \ posterior \ distribution \ close \ to \ prior \\
\end{align} %]]&gt;&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;GAN&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-10-11/cs231n-14.png&quot; width=&quot;400&quot; /&gt; &lt;img src=&quot;/assets/img/2018-10-11/cs231n-15.png&quot; width=&quot;300&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-10-11/cs231n-16.png&quot; width=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;reinforce-learning&quot;&gt;reinforce learning&lt;/h2&gt;</content><author><name>yjucho</name></author><summary type="html">cs231n http://cs231n.stanford.edu/</summary></entry><entry><title type="html">Quick drawing - dogs and cats</title><link href="http://localhost:4000/keras/visualizing%20filters/quick-drawing-exec/" rel="alternate" type="text/html" title="Quick drawing - dogs and cats" /><published>2018-10-09T00:00:00+09:00</published><updated>2018-10-09T00:00:00+09:00</updated><id>http://localhost:4000/keras/visualizing%20filters/quick-drawing-exec</id><content type="html" xml:base="http://localhost:4000/keras/visualizing%20filters/quick-drawing-exec/">&lt;h3 id=&quot;개와-고양이는-어떻게-구분되는가&quot;&gt;개와 고양이는 어떻게 구분되는가&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;quick drawing은 구글에서 공개하는 오픈소스 데이터셋입니다.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;345개 종류의 5백만장의 그림으로 이루어져있습니다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;이 포스팅에서는 그 중 개와 고양이 그림을 이용해 개와 고양이 그림을 구분하는 모델을 학습하고, 모델이 그림을 어떻게 인식하는지 시각화해보았습니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;## Quick! drawing dataset&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;## https://quickdraw.withgoogle.com/data&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;## https://github.com/googlecreativelab/quickdraw-dataset&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;## download : https://console.cloud.google.com/storage/browser/quickdraw_dataset/full/numpy_bitmap&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;dogs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;load&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'full&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%2&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Fnumpy_bitmap&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%2&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Fdog.npy'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;cats&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;load&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'full&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%2&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Fnumpy_bitmap&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%2&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Fcat.npy'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;121&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imshow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dogs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;28&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;28&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_cmap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Greys'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vmin&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vmax&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;255&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'dog'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;122&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imshow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cats&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;28&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;28&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_cmap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Greys'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vmin&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vmax&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;255&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'cat'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;suptitle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Wow! so cute!'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-10-09/output_1_0.png&quot; /&gt;&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.model_selection&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_test_split&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;keras.utils&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;to_categorical&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dogs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cats&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;x_train&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dogs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;28&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;28&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dogs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x_train&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;concatenate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cats&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;28&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;28&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;concatenate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ones&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cats&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))))&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;x_train&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x_train&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;255.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;to_categorical&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;x_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_test&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_test_split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;test_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x_test&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_test&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;(152159, 784) (123202, 784)
(192752, 28, 28, 1) (192752, 2) (82609, 28, 28, 1) (82609, 2)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;keras&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;keras.models&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Sequential&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;keras.layers&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dropout&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Flatten&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;keras.layers&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Conv2D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MaxPooling2D&lt;/span&gt;


&lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;128&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;epochs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;12&lt;/span&gt;


&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Sequential&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Conv2D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kernel_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
                 &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'relu'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                 &lt;span class=&quot;n&quot;&gt;input_shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;28&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;28&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Conv2D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'relu'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MaxPooling2D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pool_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dropout&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.25&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Flatten&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;128&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'relu'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dropout&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'softmax'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;compile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;keras&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;losses&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;categorical_crossentropy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
              &lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;keras&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optimizers&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Adadelta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt;
              &lt;span class=&quot;n&quot;&gt;metrics&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'accuracy'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;epochs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;epochs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;verbose&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;validation_data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;score&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;evaluate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;verbose&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Test loss:'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Test accuracy:'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Train on 192752 samples, validate on 82609 samples
Epoch 1/12
192752/192752 [==============================] - 310s 2ms/step - loss: 0.2997 - acc: 0.8717 - val_loss: 0.2500 - val_acc: 0.8949
Epoch 2/12
192752/192752 [==============================] - 317s 2ms/step - loss: 0.2490 - acc: 0.8966 - val_loss: 0.2328 - val_acc: 0.9034
Epoch 3/12
192752/192752 [==============================] - 308s 2ms/step - loss: 0.2336 - acc: 0.9036 - val_loss: 0.2295 - val_acc: 0.9059
Epoch 4/12
192752/192752 [==============================] - 302s 2ms/step - loss: 0.2242 - acc: 0.9087 - val_loss: 0.2333 - val_acc: 0.9058
Epoch 5/12
192752/192752 [==============================] - 306s 2ms/step - loss: 0.2158 - acc: 0.9122 - val_loss: 0.2195 - val_acc: 0.9099
Epoch 6/12
192752/192752 [==============================] - 306s 2ms/step - loss: 0.2102 - acc: 0.9152 - val_loss: 0.2195 - val_acc: 0.9085
Epoch 7/12
192752/192752 [==============================] - 308s 2ms/step - loss: 0.2058 - acc: 0.9174 - val_loss: 0.2246 - val_acc: 0.9118
Epoch 8/12
192752/192752 [==============================] - 321s 2ms/step - loss: 0.2015 - acc: 0.9190 - val_loss: 0.2151 - val_acc: 0.9126
Epoch 9/12
192752/192752 [==============================] - 316s 2ms/step - loss: 0.1972 - acc: 0.9211 - val_loss: 0.2160 - val_acc: 0.9132
Epoch 10/12
192752/192752 [==============================] - 320s 2ms/step - loss: 0.1945 - acc: 0.9226 - val_loss: 0.2274 - val_acc: 0.9126
Epoch 11/12
192752/192752 [==============================] - 320s 2ms/step - loss: 0.1908 - acc: 0.9244 - val_loss: 0.2327 - val_acc: 0.9122
Epoch 12/12
192752/192752 [==============================] - 304s 2ms/step - loss: 0.1881 - acc: 0.9253 - val_loss: 0.2281 - val_acc: 0.9135
Test loss: 0.22813269033429004
Test accuracy: 0.9135203186107955
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;summary&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_8 (Conv2D)            (None, 26, 26, 32)        320       
_________________________________________________________________
conv2d_9 (Conv2D)            (None, 24, 24, 64)        18496     
_________________________________________________________________
max_pooling2d_4 (MaxPooling2 (None, 12, 12, 64)        0         
_________________________________________________________________
dropout_7 (Dropout)          (None, 12, 12, 64)        0         
_________________________________________________________________
flatten_4 (Flatten)          (None, 9216)              0         
_________________________________________________________________
dense_7 (Dense)              (None, 128)               1179776   
_________________________________________________________________
dropout_8 (Dropout)          (None, 128)               0         
_________________________________________________________________
dense_8 (Dense)              (None, 2)                 258       
=================================================================
Total params: 1,198,850
Trainable params: 1,198,850
Non-trainable params: 0
_________________________________________________________________
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;save&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'quick_drawing_model.h5'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.image&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mpimg&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;a_dog&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mpimg&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imread&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'articles/quick-drawing/yjucho-dog.png'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;a_cat&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mpimg&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imread&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'articles/quick-drawing/yjucho-cat.png'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;121&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imshow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a_dog&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_cmap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Greys'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vmin&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vmax&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;255&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'dog'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;122&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imshow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a_cat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_cmap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Greys'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vmin&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vmax&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;255&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'cat'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;suptitle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'yjucho&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\'&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s drawing'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-10-09/output_6_0.png&quot; /&gt;&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;tmp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a_dog&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imshow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tmp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_cmap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'gray'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tmp&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;28&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;28&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-10-09/output_7_0.png&quot; /&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;array([[0.5746763 , 0.42532378]], dtype=float32)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;tmp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a_cat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imshow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tmp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_cmap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'gray'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tmp&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;28&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;28&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-10-09/output_8_0.png&quot; /&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;array([[0.5847676 , 0.41523245]], dtype=float32)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;keras.backend&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;K&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;keras.models&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;load_model&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;load_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'quick_drawing_model.h5'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;layer_dict&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;layer&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;deprocess_image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# normalize tensor: center on 0., ensure std is 0.1&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1e-5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;

    &lt;span class=&quot;c&quot;&gt;# clip to [0, 1]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;clip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;c&quot;&gt;# convert to RGB array&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;255&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;#x = x.transpose((1, 2, 0))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;clip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;255&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;astype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'uint8'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;vis_img_in_filter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;img&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;13&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;28&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;28&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;astype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; 
                      &lt;span class=&quot;n&quot;&gt;layer_name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'conv2d_8'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;layer_output&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;layer_dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layer_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;output&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;img_ascs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;filter_index&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layer_output&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]):&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;# build a loss function that maximizes the activation&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;# of the nth filter of the layer considered&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;K&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layer_output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;filter_index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

        &lt;span class=&quot;c&quot;&gt;# compute the gradient of the input picture wrt this loss&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;grads&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;K&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gradients&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

        &lt;span class=&quot;c&quot;&gt;# normalization trick: we normalize the gradient&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;grads&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;K&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;K&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;K&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;square&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1e-5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;c&quot;&gt;# this function returns the loss and grads given the input picture&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;iterate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;K&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

        &lt;span class=&quot;c&quot;&gt;# step size for gradient ascent&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;step&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;5.&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;img_asc&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;img&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;# run gradient ascent for 20 steps&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;loss_value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grads_value&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;iterate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;img_asc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;img_asc&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grads_value&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;img_asc&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;img_asc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;img_ascs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;deprocess_image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;img_asc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;28&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;28&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
        
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;layer_output&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;35&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;plot_x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plot_y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;elif&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;layer_output&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;23&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;plot_x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plot_y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;elif&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;layer_output&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;11&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;plot_x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plot_y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;plot_x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plot_y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subplots&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot_x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plot_y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;figsize&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;12&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;12&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imshow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;img&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;28&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;28&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cmap&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'gray'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Input image'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;suptitle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Input image and &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s filters'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layer_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tight_layout&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pad&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rect&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot_x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot_y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;continue&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imshow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;img_ascs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plot_y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cmap&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'gray'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'filter &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;d'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plot_y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;vis_img_in_filter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;img src=&quot;/assets/img/2018-10-09/output_9_0.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;reference : https://www.kaggle.com/ernie55ernie/mnist-with-keras-visualization-and-saliency-map&lt;/p&gt;</content><author><name>yjucho</name></author><summary type="html">개와 고양이는 어떻게 구분되는가</summary></entry><entry><title type="html">cs231n - 이해하기</title><link href="http://localhost:4000/cs231n/cs231n/" rel="alternate" type="text/html" title="cs231n - 이해하기" /><published>2018-10-09T00:00:00+09:00</published><updated>2018-10-09T00:00:00+09:00</updated><id>http://localhost:4000/cs231n/cs231n</id><content type="html" xml:base="http://localhost:4000/cs231n/cs231n/">&lt;p&gt;cs231n&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;http://cs231n.stanford.edu/&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;이 포스팅은 딥러닝에 대한 기본 지식을 상세히 전달하기보다는 
간략한 핵심과 실제 모델 개발에 유용한 팁을 위주로 정리하였습니다.&lt;/p&gt;

&lt;h3 id=&quot;activation-functions&quot;&gt;activation functions&lt;/h3&gt;
&lt;p&gt;1) sigmoid&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;saturated neurons kill the gradient&lt;/li&gt;
  &lt;li&gt;sigmoid outputs are not zero-centered&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;2) tanh&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;zero-cented but staturated neurons kill the gradient&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;3) relu&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;doest not saturate&lt;/li&gt;
  &lt;li&gt;computationally efficient&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;4) leaky relu&lt;/p&gt;

&lt;p&gt;5) exponential Linear Units&lt;/p&gt;

&lt;h3 id=&quot;sigmoid-outputs-are-not-zero-centered-why-is-it-problem&quot;&gt;Sigmoid outputs are not zero-centered. why is it problem?&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-10-09/cs231n-01.png&quot; width=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Sigmoid outputs are not zero-centered. This is undesirable since neurons in later layers of processing in a Neural Network (more on this soon) would be receiving data that is not zero-centered. This has implications on the dynamics during gradient descent, because if the data coming into a neuron is always positive (e.g. x&amp;gt;0 elementwise in f=wTx+b)), then the gradient on the weights w will during backpropagation become either all be positive, or all negative (depending on the gradient of the whole expression f). This could introduce undesirable zig-zagging dynamics in the gradient updates for the weights. However, notice that once these gradients are added up across a batch of data the final update for the weights can have variable signs, somewhat mitigating this issue. Therefore, this is an inconvenience but it has less severe consequences compared to the saturated activation problem above.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{dL}{dw} =  \frac{dL}{df}\frac{df}{dw}\\
\frac{df}{dw} =  x \ and \ x \ are \ all \ positive,\\ 
the \ gradient \frac{dL}{dw} \ always \ has \ the \ same \ sign \ as \frac{dL}{df} \ (all \ positive \ or \ all \ negative )&lt;/script&gt;

&lt;h3 id=&quot;nesterov-momentum&quot;&gt;Nesterov Momentum&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-10-09/cs231n-04.png&quot; width=&quot;250&quot; /&gt;
&lt;img src=&quot;/assets/img/2018-10-09/cs231n-03.png&quot; width=&quot;250&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://tensorflowkorea.files.wordpress.com/2017/03/ec8aa4ed81aceba6b0ec83b7-2017-03-22-ec98a4eca084-11-40-58.png&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;현재 위치의 그래디언트 g(&lt;script type=&quot;math/tex&quot;&gt;\theta_t&lt;/script&gt;) 를 이용하는 것이 아니고 현재 위치에서 속도 &lt;script type=&quot;math/tex&quot;&gt;\mu v_t&lt;/script&gt;만큼 전진한 후의 그래디언트 g(&lt;script type=&quot;math/tex&quot;&gt;\theta_t + \mu v_t&lt;/script&gt;) 를 이용합니다. 사람들은 이를 가리켜 선험적으로 혹은 모험적으로 먼저 진행한 후 에러를 교정한다라고 표현합니다. &lt;small&gt; (ref : https://tensorflow.blog/2017/03/22/momentum-nesterov-momentum/) &lt;/small&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;optimizer&quot;&gt;optimizer&lt;/h3&gt;

&lt;p&gt;1) SGD&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;while True : 
  dx = compute_gradient(x)
  x += -learning_rate * dx
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;2) SGD + Momentum&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;vx = 0 
while True :
  dx = compute_gradient(x)
  vx = rho * vx + dx 
  x += -learning_rate * vx
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;3) Nesterov Accelerated Gradient(NAG)&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;vx = 0
while True :
  dx = compute_gradient(x)
  old_vx = vx
  vx = rho * vx - learning_rate * dx
  x += -rho * old_vx + (1 + rho) * vx
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;4) AdaGrad&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;grad_squared = 0 
while True :
  dx = compute_gradient(x)
  grad_squared += dx * dx
  x += -learning_rate * dx / (np.sqrt(grad_squared) + 1e-7)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;5) RMSProp&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;grad_squared = 0 
while True :
  dx = compute_gradient(x)
  grad_squared += decay_rate * grad_squared + (1-dacay_rate) * dx * dx
  x += -learning_rate * dx / (np.sqrt(grad_squared) + 1e-7)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;6) Adam&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;first_moment = 0
second_moment = 0
for t in range(num_iterations):
  dx = compute_gradient(x)
  first_moment = beta1 * first_moment + (1-beta1) * dx
  second_moment = beta2 * second_moment + (1-beta2) * dx * dx
  ## bias correction for the fact that first and second momentum estimates start at zero
  first_unbias = first_moment / (1-beta1 ** t)
  second_unbias = second_moment / (1-beta2 ** t)
  x -= learning_rate * first_moment / (np.sqrt(second_moment) + 1e-7)
#### Tips!!
## Adam with bete1 = 0.9 and beta2 = 0.999 and 
## learning_rate = 1e-3 or 5e-4 is a great starting point 
## for many models!
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;옵티마이저에 상관없이 모두 learning rate 하이퍼파라미터가 필요함. 어떻게 조절하지? decay over time!&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;SGD with Momentum에서는 흔히 사용. 하지만 Adam에서는 잘 사용안함.&lt;/li&gt;
  &lt;li&gt;second order hyperprameter임. First, try no decay and see what happen.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;model-ensembel-trick&quot;&gt;Model ensembel trick&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Enjoy 2% extra performance&lt;/li&gt;
  &lt;li&gt;독립적인 모델을 여러개 만드는 것보다 한가지 모델의 여러 스냅샷을 앙상블하는게 효과적.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-10-09/cs231n-05.png&quot; width=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;regularization&quot;&gt;regularization&lt;/h3&gt;
&lt;p&gt;common pattern : add random noise in train, marginalize over the noise in test&lt;/p&gt;

&lt;p&gt;1) Dropout
2) Batch Normalization
3) Data Augmentation
– below is not common in practice, but cool ideas
4) DropConnect - 랜덤하게 activation값을 제로로 만드는 dropout과 비슷. 하지만 activation이 아니라 weight를 제로로 만드는 것
5) Fractional max pooling - 풀링레이어에서 풀링 영역을 랜덤하게 선택
6) stochastic depth - 전체 레이어 중 랜덤하게 선택한 레이어만 학습. 테스트할때는 averaging하여 전체를 사용&lt;/p&gt;

&lt;h3 id=&quot;transfer-learning&quot;&gt;transfer learning&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;It’s the norm, not the exception&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-10-09/cs231n-06.png&quot; width=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt; &lt;/th&gt;
      &lt;th&gt;very similar dataset&lt;/th&gt;
      &lt;th&gt;very different dataset&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;b&gt;very little data&lt;/b&gt;&lt;/td&gt;
      &lt;td&gt;Use Linear Classifier on top layer&lt;/td&gt;
      &lt;td&gt;you’re in trouble.. Try linear classifier from different stages&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;b&gt;quite a lot of data&lt;/b&gt;&lt;/td&gt;
      &lt;td&gt;Finetune a few layers&lt;/td&gt;
      &lt;td&gt;Finetune a larger number of layers&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;cnn-architectures&quot;&gt;CNN architectures&lt;/h3&gt;

&lt;p&gt;1) Lenet-5&lt;/p&gt;

&lt;p&gt;2) AlexNet - 7x7 filter size&lt;/p&gt;

&lt;p&gt;3) VGG - 3x3 filter size&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;why use smaller filters? (3x3 conv)
    &lt;ul&gt;
      &lt;li&gt;stack of three 3x3 conv (stride 1) layer has same effective receptive field as one 7x7 layer&lt;/li&gt;
      &lt;li&gt;But deeper, more non-linearities&lt;/li&gt;
      &lt;li&gt;And fewer parameters: 3 * (3&lt;sup&gt;2&lt;/sup&gt;C&lt;sup&gt;2&lt;/sup&gt;) vs. 7&lt;sup&gt;2&lt;/sup&gt;C&lt;sup&gt;2&lt;/sup&gt; for C channels per layer&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;4) GoogLeNet&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Inception module : 1x1 conv, 3x3 conv, 5x5 conv and 3x3 pooling in parallel  –&amp;gt; concatenate outputs
    &lt;ul&gt;
      &lt;li&gt;computational expensive&lt;/li&gt;
      &lt;li&gt;adding 1x1 conv(64 filter) as bottlenecks&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;auxiliary classification outputs to inject additional gradient at lower layers&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;5) ResNet&lt;/p&gt;

&lt;p&gt;– below others to know&lt;/p&gt;

&lt;p&gt;6) Network in Network (NiN)&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;philosophical inspiration for googLeNet&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;7) Wide Residul Networks&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;residuals are the important factor, not depth&lt;/li&gt;
  &lt;li&gt;Use wider residual blocks (F x k filters instead of F filters in each layer)&lt;/li&gt;
  &lt;li&gt;50-layer wide resnet outperforms 152-layers original resnet&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;8) deep networks with stochastic depth&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;randomly drop a subset of layers during each training pass&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;9) FractalNet&lt;/p&gt;

&lt;p&gt;10) Densely connected conv net&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;each layer is connected to every other layer in feedforward fashion&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;rnn&quot;&gt;RNN&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;one to one - vanilla NN&lt;/li&gt;
  &lt;li&gt;one to many - image captioning&lt;/li&gt;
  &lt;li&gt;many to one - sentiment classification&lt;/li&gt;
  &lt;li&gt;many to many - machine translation&lt;/li&gt;
  &lt;li&gt;many to many - video classfiction on frame level&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;backpropagation through time is super slow! &lt;br /&gt;
Truncated Backpropagation through time &lt;br /&gt;
minibatch별로 나눠서 그래디언트 업데이트 &lt;br /&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;1) Image captioning&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;input : image -&amp;gt; ConvNet -&amp;gt; FC 4096&lt;/li&gt;
  &lt;li&gt;take FC 4096 as first hidden state vector&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;2) Image captioning with Attention&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;이미지의 location 정보를 이용하도록 함&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;3) Visual Question Answering : RNN with Attention&lt;/p&gt;

&lt;h3 id=&quot;vanilla-rnn-gradient-flow&quot;&gt;vanilla RNN gradient flow&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-10-09/cs231n-07.png&quot; width=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;p&gt;computing gradient of h&lt;sub&gt;0&lt;/sub&gt; involves many factors of W&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;exploding gradients -&amp;gt; gradient clipping&lt;/li&gt;
  &lt;li&gt;vanishing gradients -&amp;gt; change RNN architecture (LSTM)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;lstm&quot;&gt;LSTM&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-10-09/cs231n-08.png&quot; width=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;i, f, o, g gate&lt;/li&gt;
  &lt;li&gt;forget gate : whether to erase cell&lt;/li&gt;
  &lt;li&gt;input gate : whether to wirte to cell&lt;/li&gt;
  &lt;li&gt;gate gate : how much to wirte to cell&lt;/li&gt;
  &lt;li&gt;output gate : how much to reveal cell&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-10-09/cs231n-09.png&quot; width=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;p&gt;backpropagation from c&lt;sub&gt;t&lt;/sub&gt; to c&lt;sub&gt;t-1&lt;/sub&gt; only elementwise multiplication by f, no matrix multiply by W&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-10-09/cs231n-10.png&quot; width=&quot;500&quot; /&gt;&lt;/p&gt;</content><author><name>yjucho</name></author><summary type="html">cs231n http://cs231n.stanford.edu/</summary></entry></feed>