<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2023-09-14T17:05:41+09:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">yjucho’s blog</title><subtitle>DATA, Deep Learning, AI</subtitle><author><name>yjucho</name></author><entry><title type="html">Twin neural network regression is a semi-supervised regression algorithm</title><link href="http://localhost:4000/time-series/consistency%20injection/twin-nn-regression/" rel="alternate" type="text/html" title="Twin neural network regression is a semi-supervised regression algorithm" /><published>2023-09-14T00:00:00+09:00</published><updated>2023-09-14T00:00:00+09:00</updated><id>http://localhost:4000/time-series/consistency%20injection/twin-nn-regression</id><content type="html" xml:base="http://localhost:4000/time-series/consistency%20injection/twin-nn-regression/">&lt;p&gt;&lt;b&gt;Wetzel, Sebastian J., Roger G. Melko, and Isaac Tamblyn. “Twin neural network regression is a semi-supervised regression algorithm.” Machine Learning: Science and Technology 3.4 (2022): 045007.&lt;/b&gt;&lt;/p&gt;

\[F(x_i, x_j) = y_i -y_j\]

\[\begin{align}
F(x_1, x_2) + F(x_2, x_3) + f(x_3, x_1) &amp;amp;= (y_1 - y_2) + (y_2 -y_3) + (y_3 - y_1) \\
&amp;amp;= 0
\end{align}\]

\[loss = \frac{1}{n^2}\sum_{ij}(F(x_i, x_j)-(y_i-y_j))^2 + \lambda \frac{1}{n^3} \sum_{ijk}(F(x_i, x_j) + F(x_j, x_k) + f(x_k, x_i))^2\]</content><author><name>yjucho</name></author><category term="Time-series" /><category term="consistency injection" /><summary type="html">Wetzel, Sebastian J., Roger G. Melko, and Isaac Tamblyn. “Twin neural network regression is a semi-supervised regression algorithm.” Machine Learning: Science and Technology 3.4 (2022): 045007.</summary></entry><entry><title type="html">Multi-Granularity Residual Learning with Confidence Estimation for Time Series Prediction</title><link href="http://localhost:4000/time-series/consistency%20injection/granularity/" rel="alternate" type="text/html" title="Multi-Granularity Residual Learning with Confidence Estimation for Time Series Prediction" /><published>2023-08-30T00:00:00+09:00</published><updated>2023-08-30T00:00:00+09:00</updated><id>http://localhost:4000/time-series/consistency%20injection/granularity</id><content type="html" xml:base="http://localhost:4000/time-series/consistency%20injection/granularity/">&lt;p&gt;&lt;b&gt;Hou, Min, et al. “Multi-Granularity Residual Learning with Confidence Estimation for Time Series Prediction.” Proceedings of the ACM Web Conference 2022. 2022. &lt;/b&gt;&lt;/p&gt;

\[\mathcal{L} = \sum^S_{s=1}||y^s - \hat{y}^s||^2 + \lambda_1 \sum_{s=1}^{S} \mathcal{L}_{Rec} + \frac{\lambda_{\theta}}{2}||\Theta||_F^2\]

\[\mathcal{L}_{Rec} = \sum_{m=1,2,..m} ||F^m - G^{m-1}||^2_F\]</content><author><name>yjucho</name></author><category term="Time-series" /><category term="consistency injection" /><summary type="html">Hou, Min, et al. “Multi-Granularity Residual Learning with Confidence Estimation for Time Series Prediction.” Proceedings of the ACM Web Conference 2022. 2022.</summary></entry><entry><title type="html">time series representation learning with consistency</title><link href="http://localhost:4000/time-series/consistency%20injection/consistency/" rel="alternate" type="text/html" title="time series representation learning with consistency" /><published>2023-08-28T00:00:00+09:00</published><updated>2023-08-28T00:00:00+09:00</updated><id>http://localhost:4000/time-series/consistency%20injection/consistency</id><content type="html" xml:base="http://localhost:4000/time-series/consistency%20injection/consistency/">&lt;h2 id=&quot;temporal-consistency&quot;&gt;Temporal consistency&lt;/h2&gt;
&lt;p&gt;Tonekaboni, Sana, Danny Eytan, and Anna Goldenberg. “Unsupervised Representation Learning for Time Series with Temporal Neighborhood Coding.” International Conference on Learning Representations. 2020&lt;/p&gt;

&lt;h2 id=&quot;subseries-consistency&quot;&gt;Subseries consistency&lt;/h2&gt;
&lt;p&gt;Franceschi, Jean-Yves, Aymeric Dieuleveut, and Martin Jaggi. “Unsupervised scalable representation learning for multivariate time series.” Advances in neural information processing systems 32 (2019)&lt;/p&gt;

&lt;h2 id=&quot;transformation-consistency&quot;&gt;Transformation consistency&lt;/h2&gt;
&lt;p&gt;Eldele, Emadeldeen, et al. “Time-series representation learning via temporal and contextual contrasting.” arXiv preprint arXiv:2106.14112 (IJCAI 2021).&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;weak augmentation : jitter-and scale&lt;/li&gt;
  &lt;li&gt;strong augmentation : permutation-and-jitter&lt;/li&gt;
  &lt;li&gt;a tough cross-view prediction task by using the context of the strong
augmentation \(c^s_t\) to predict the future timesteps of the weak augmentation \(z^w
_{t+k}\) and vice versa  (temporal contrasting module)&lt;/li&gt;
  &lt;li&gt;\(c_t^{i+}\) as the positive sample of \(c_t^{i}\) that comes from the other augmented view of the same inut (contextual contrasting module)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2023-08-28/fig_transf.png&quot; width=&quot;500&quot; /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;contextual-consistency&quot;&gt;Contextual consistency&lt;/h2&gt;
&lt;p&gt;Yue, Zhihan, et al. “Ts2vec: Towards universal representation of time series.” Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 36. No. 8. 2022.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;random cropping : randomly samples two overlapping time segments [a1, b1], [a2, b2] a1&amp;lt;=a2&amp;lt;=b1&amp;lt;=b2 (the overlapped segment [a2, b1] should be consistent for two context views)&lt;/li&gt;
  &lt;li&gt;timestamp masking : randomly mask the latent vector \(z_i\)&lt;/li&gt;
  &lt;li&gt;hierarchical constrative loss encapsulate representations in different levels of granularity&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2023-08-28/fig_hierarchical.png&quot; /&gt;&lt;br /&gt;
&lt;img src=&quot;/assets/img/2023-08-28/fig_contextual.png&quot; /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;frequency-consistency&quot;&gt;Frequency consistency&lt;/h2&gt;
&lt;p&gt;Zhang, Xiang, et al. “Self-supervised contrastive pre-training for time series via time-frequency consistency.” Advances in Neural Information Processing Systems 35 (2022): 3988-4003.&lt;/p&gt;

&lt;p&gt;TBU&lt;/p&gt;</content><author><name>yjucho</name></author><category term="Time-series" /><category term="consistency injection" /><summary type="html">Temporal consistency Tonekaboni, Sana, Danny Eytan, and Anna Goldenberg. “Unsupervised Representation Learning for Time Series with Temporal Neighborhood Coding.” International Conference on Learning Representations. 2020</summary></entry><entry><title type="html">The Capacity and Robustness Trade-off: Revisiting the Channel Independent Strategy for Multivariate Time Series Forecasting</title><link href="http://localhost:4000/time-series/LTSF-CD-and-CI/" rel="alternate" type="text/html" title="The Capacity and Robustness Trade-off: Revisiting the Channel Independent Strategy for Multivariate Time Series Forecasting" /><published>2023-08-22T00:00:00+09:00</published><updated>2023-08-22T00:00:00+09:00</updated><id>http://localhost:4000/time-series/LTSF-CD-and-CI</id><content type="html" xml:base="http://localhost:4000/time-series/LTSF-CD-and-CI/">&lt;p&gt;&lt;b&gt;Han, Lu, Han-Jia Ye, and De-Chuan Zhan (2023)&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;우왕 2023년ㅎㅎㅎ&lt;/p&gt;

&lt;h2 id=&quot;abstract&quot;&gt;Abstract&lt;/h2&gt;

&lt;h2 id=&quot;analysis&quot;&gt;Analysis&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;CD전략과 CI전략에서의 yule-walker equation 분석과 train과 test데이터셋의 각 채널별 ACF와 전체 채널의 ACF합 비교해 봄&lt;/li&gt;
  &lt;li&gt;학습데이터로부터 파라미터 \(W\)를 추정한 모델 (\(R(\hat{W})\))의 리스크분석과 CD 및 CI 전락으로 학습된 모델에 대해 각 데이터셋뱔 통계값(train error, test error, diff W, generalization error)를 확인함&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;practical-guide&quot;&gt;Practical Guide&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;이전 섹션을 통해서 CD 학습전략이 capacity는 높지만, robustness 측면에서는 떨어지는 것을 확인하였다. 반대로 CI 학습전략은 robustness 측면에서는 강건함을 보였다.&lt;/li&gt;
  &lt;li&gt;이번 섹션은 위와 같은 특성을 고려하여 CD 전략을 개선함으로써 CI 전략의 성능(capacity)보다 더 좋은 성능을 낼수있음을 실험적으로 확인하고자 한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;predict-residuals-with-regularization&quot;&gt;Predict Residuals with Regularization&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/img/2023-08-22/fig7.png&quot; /&gt;&lt;br /&gt;
위에 Fig 7은 실제 값과, CD와 CI 전략간의 예측값을 비교하여 살펴본 결과, CD 학습에 의한 모델이 sharp하며, 강견하지 않은(non-robust) 예측값을 생성하는 것을 예시로 보여주고 있다. 이러한 현상을 개선하기 위해 이 연구에서는 regularization를 이용하며 Residual을 예측하는 objective를 제안하였다.&lt;/p&gt;

\[min_{f} \frac{1}{N} \sum_{i=1}^{N} \ell (f(X^{(i)}-N^{(i)})+N^{(i)}, Y^{(i)}) + \lambda\Omega(f)
\\ where \ N^{(i)}=X_{:,L}^{(i)}\]

&lt;p&gt;\(N^{(i)}\)는 입력값 X의 마지막 값입니다. reqularization term \(\Omega\)는 pytorch의 L2를 사용하였다고 한다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2023-08-22/table4.png&quot; /&gt;&lt;br /&gt;
Table4는 실험 결과를 보여준다. Linear 모델과 Transformer 모델에 제안한 방법(PRReg)를 목적식으로 사용하였을때 결과로, 적절한 \(\lambda\)를 사용할경우 CI전략보다 더 우수한 성능을 보인다. PRReg는 기본적으로 CD전략이고 regularization term의 \(\lambda\)가 너무 크면 언더피팅되고, 너무 작으면 CD와 마찬가지로 오버피팅되어 robustness가 떨어지는 것을 확인할수 있었다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2023-08-22/table5.png&quot; /&gt;&lt;br /&gt;
PatchTST와 비교 (patchtst가 가장 우수하거나, transfomer+PRReg모델모다 더 나은 성능)&lt;/p&gt;</content><author><name>yjucho</name></author><category term="Time-series" /><summary type="html">Han, Lu, Han-Jia Ye, and De-Chuan Zhan (2023)</summary></entry><entry><title type="html">SOM-CPC: Unsupervised Contrastive Learning with Self-Organizing Maps for Structured Representations of High-Rate Time Series</title><link href="http://localhost:4000/unsupervised%20learning/time-series/SOMCPC/" rel="alternate" type="text/html" title="SOM-CPC: Unsupervised Contrastive Learning with Self-Organizing Maps for Structured Representations of High-Rate Time Series" /><published>2023-08-22T00:00:00+09:00</published><updated>2023-08-22T00:00:00+09:00</updated><id>http://localhost:4000/unsupervised%20learning/time-series/SOMCPC</id><content type="html" xml:base="http://localhost:4000/unsupervised%20learning/time-series/SOMCPC/">&lt;p&gt;&lt;b&gt;Huijben, Iris AM, et al. (ICLR poster 2023)&lt;/b&gt;&lt;/p&gt;

&lt;h2 id=&quot;self-organizing-map-som&quot;&gt;self-organizing map (SOM)&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;an unsupervised machine learning technique used to produce a low-dimensional (typically two-dimensional) representation of a higher dimensional data set while &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;preserving the topological structure&lt;/code&gt; of the data&lt;/li&gt;
  &lt;li&gt;K개의 node\(\phi\)를 가정하고 각 데이터 포인트 \(z\)를 1개의 카운터파트 노드(winning node) \(q_{\phi}(z)\)로 할당함&lt;/li&gt;
&lt;/ul&gt;

\[q_{\phi}(z) = \phi[argmin_i(||\phi, z||_2^2)] \\\]

&lt;p&gt;At training, each ϕi is updated as follows&lt;/p&gt;

\[\phi(i)^{(n+1)} = \phi(i)^{(n)} + \eta^{(n)}\mathcal{S}_i(q_{\phi}(z))(z - \phi(i)^{(n)} ) \\
where \ \mathcal{S}_i(q_{\phi}(z)) = \text{exp}(-\frac{d_i^{(n)}}{2(\sigma^{(n)})^2}) \\
d_i^{(n)} = ||\mathcal{P}[q_{\phi}(z)],\mathcal{P}[\phi_i^{(n)}]||_2^{2} \\
\sigma^{(n)} =\sigma^{(0)}\text{exp}(-\frac{n}{\lambda})\]

&lt;ul&gt;
  &lt;li&gt;winning node와 그와 가가운 노드들은 데이터 z와 가까운 쪽으로 업데이트됨&lt;/li&gt;
  &lt;li&gt;\(\eta^{(n)}\) : decreasing learning rate&lt;/li&gt;
  &lt;li&gt;\mathcal{S}&lt;em&gt;i(q&lt;/em&gt;{\phi}(z)) : distance from the winning node&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;som-cpc&quot;&gt;SOM-CPC&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;a representation learning model that learns to map windows of time series data to a structured 2D grid for the purpose of pattern discovery
&lt;img src=&quot;/assets/img/2023-08-22/fig1.png&quot; /&gt;&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;TBU&lt;/code&gt;&lt;/p&gt;</content><author><name>yjucho</name></author><category term="unsupervised learning" /><category term="Time-series" /><summary type="html">Huijben, Iris AM, et al. (ICLR poster 2023)</summary></entry><entry><title type="html">Dynamic Time Warping(DTW)</title><link href="http://localhost:4000/time-series/dtw/" rel="alternate" type="text/html" title="Dynamic Time Warping(DTW)" /><published>2019-05-01T00:00:00+09:00</published><updated>2019-05-01T00:00:00+09:00</updated><id>http://localhost:4000/time-series/dtw</id><content type="html" xml:base="http://localhost:4000/time-series/dtw/">&lt;p&gt;두 시계열 데이터간의 유사도를 어떻게 계산할 수 있을까? 두 시계열이 동일한 길이의 시퀀스라면 단순히 상관계수를 구하는 것이 가능하지만, 현실 세계의 시계열 데이터는 그렇지 않은 경우가 많습니다. 예를 들어 아래와 같은 두 시계열 데이터를 살펴보겠습니다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;ts1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ts2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figure&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figsize&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;15&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;121&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Time series 1'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ts1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;122&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Time series 2'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ts2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2019-05-01/fig1.png&quot; width=&quot;550&quot; /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;육안으로 보기엔 두 시계열 모두 두 개의 peak를 가지고 있고 전체적으로 우상향하는 모습이 매우 유사해보입니다. 두 시계열 간의 상관계수를 구해보도록 하겠습니다. 어랏, 두 데이터의 길이가 다르기 때문에 바로 계산되지 않네요.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;corrcoef&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ts1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ts2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;## ValueError: all the input array dimensions except for the concatenation axis must match exactly
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;유사도를 측정하기 위한 가장 간단한 방법은 상대적으로 길이가 짧은 시계열1 데이터를 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;interpolation&lt;/code&gt;하여 길이를 동일하게 맞춘 후, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;np.corrcoef&lt;/code&gt;를 사용하여 상관계수를 계산하는 것입니다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;len_ts1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ts1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;len_ts2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ts2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;interval&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;len_ts2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;len_ts1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;interp_ind&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;len_ts2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;interval&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ts1_interp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;interp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;len_ts2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;interp_ind&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ts1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figure&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figsize&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;15&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;121&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Time series 1'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ts1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;122&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Comparison : ts1_interp vs. ts2'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ts1_interp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ts2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;legend&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Time series 1 - interpolation'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'Time series 2'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;## correlation coefficent
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;corrcoef&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ts1_interp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ts2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;#### output
# array([[ 1.        ,  0.85206492],
#        [ 0.85206492,  1.        ]])
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2019-05-01/fig2.png&quot; width=&quot;550&quot; /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;단순히 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;선형 보간(linear interpolation)&lt;/code&gt; 방법은 기존의 시계열 데이터1이 가지고 있는 모습을 꽤 왜곡시킨는 결과를 낳습니다. 2개의 spike형태의 peak가 사라진 것을 볼 수 있습니다. 실제로 단순히 데이터 포인트를 늘려서 대응방식으로 비교하는 것은 합리적이지 못한 경우가 많습니다.&lt;/p&gt;

&lt;p&gt;이렇게 길이가 서로 다른 두 시계열의 유사도를 계산하는 방법으로 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DTW(Dynamic Time Warping)&lt;/code&gt;를 사용할 수 있습니다. DTW는 시퀀스의 길이를 고려하지 않기 때문에 서로 다른 길이의 시퀀스의 유사도를 바로 계산할 수 있습니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;img src=&quot;https://upload.wikimedia.org/wikipedia/commons/thumb/a/ab/Dynamic_time_warping.png/440px-Dynamic_time_warping.png&quot; width=&quot;200&quot; /&gt; &lt;br /&gt; &lt;small&gt;In time series analysis, dynamic time warping (DTW) is one of the algorithms for measuring similarity between two temporal sequences, &lt;u&gt;which may vary in speed&lt;/u&gt;. For instance, similarities in walking could be detected using DTW, even if one person was walking faster than the other, or if there were accelerations and decelerations during the course of an observation. DTW has been applied to temporal sequences of video, audio, and graphics data — indeed, any data that can be turned into a linear sequence can be analyzed with DTW. A well known application has been automatic speech recognition, to cope with different speaking speeds. Other applications include speaker recognition and online signature recognition. Also it is seen that it can be used in partial shape matching application. &lt;i&gt;- &lt;a href=&quot;https://en.wikipedia.org/wiki/Dynamic_time_warping&quot;&gt;위키피디아&lt;/a&gt;&lt;/i&gt; &lt;/small&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;** 아래는 DTW의 개념을 소개하기 위해 &lt;a href=&quot;https://jsideas.net/bitcoin_dtw/&quot;&gt;jsideas님의 포스팅&lt;/a&gt;를 인용하였습니다.&lt;/p&gt;

&lt;p&gt;n개의 데이터포인트가 있는 시퀀스 X와 m개의 데이터포인트가 있는 시퀀스 Y가 있다고 하겠습니다. 이 두 시퀀스를 각 각 x축과 y축에 늘어놓고 데이터 포인트간의 거리(예를 들어 유클리디언 거리)를 구하면, 그 값둘은 m\(\times\)n의 매트릭스 형태가 됩니다. 이 매트릭스를 cost matrix라고 하도록 하겠습니다. cost matrix를 heatmap형식으로 표현하면 아래 그림처럼, 두 데이터 포인트간 거리가 짧은 곳은 어둡게, 거리가 먼 곳은 흰색으로 표현됩니다. DTW알고리즘은 저 cost matrix 상의 좌하단에서 우상단까지 가는 최적의 경로를 찾는 문제를 푸는 것입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2019-05-01/fig5.png&quot; width=&quot;550&quot; /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;이 최적화문제의 목적식은 좌하단(0,0)에서 우상단(m, n)을 이동하는데 드는 비용을 최소화하는 것이고, 이때 3가지 제약조건이 존재하게 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2019-05-01/fig6.png&quot; width=&quot;550&quot; /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;두 시퀀스의 처음과 끝은 같아야 합니다. 즉 무조건 좌하단에서 시작해서 우하단에서 끝나야합니다.&lt;/li&gt;
  &lt;li&gt;x나 y축, 혹은 그 두 축에서 음의 방향으로 이동하지 않습니다.&lt;/li&gt;
  &lt;li&gt;이동할때 정해진 스텝사이즈 (예를 들어 오른쪽과 위쪽 한칸씩만 이동가능하다던지..(0,1) or (1,0) or (1,1))만큼 이동가능합니다. 가능한 스텝사이즈를 늘릴수록 더 많은 경우 수를 검색하기 때문에 최적에 가까운 경로를 얻을 수 있지만, 그만큼 계산속도가 느려지게 됩니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;DTW는 결국 X와 Y를 늘어놓고 X의 특정 데이터포인트가 Y의 어떤 데이터포인트에 가장 적합한지를 판정하는 로직이므로, X와 Y의 길이가 늘어나면 늘어날수록 검색 비용이 늘어나는 단점이 있습니다.&lt;/p&gt;

&lt;p&gt;또한 앞서 언급했듯이 최적값을 찾기 위해 검색가능한 스텝사이즈를 늘리면 계산 속도가 느려지게 되고, 반대로 스텝사이즈를 줄이면 전후 경로만 보고 기계적으로 두 시퀀스를 정렬시켜버리는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pathological alignment&lt;/code&gt; 문제가 발생할 수 있습니다. 일반적으로 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pathological alignment&lt;/code&gt;문제를 피하기 위해 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Sakoe-Chiba Band&lt;/code&gt;와 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Itakura Parallelogram&lt;/code&gt;방법 등을 사용하기도 합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2019-05-01/fig7.png&quot; width=&quot;550&quot; /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;python에서의-dtw&quot;&gt;python에서의 DTW&lt;/h3&gt;
&lt;p&gt;파이썬에서는 pip 패키지인 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dtw&lt;/code&gt;를 통해서 별도의 구현없이 DTW알고리즘을 쉽게 이용할 수 있습니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;https://pypi.org/project/dtw/ &lt;br /&gt;&lt;br /&gt;
&lt;b&gt;github description&lt;/b&gt; : https://github.com/pierre-rouanet/dtw&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;패키지를 설치한 후 아래와 같이 사용할 수 있습니다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;dtw&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtw&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ts1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ts2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;euclidean_norm&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;abs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cost_matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;acc_cost_matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;path&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtw&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dist&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;euclidean_norm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imshow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;acc_cost_matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;origin&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'lower'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cmap&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'gray'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;interpolation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'nearest'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'w'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;img src=&quot;/assets/img/2019-05-01/fig3.png&quot; width=&quot;150&quot; /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;cost matrix와 최적 path는 위 이미지에 표시된것과 같고, 이를 다시 시계열 차트에서 비교하면 아래와 같습니다. dtw를 통해 warping된 시계열데이터1과 시계열데이터2의 상관계수를 구한 결과, 약 0.92로 단순 선형 보간에 의한 상관계수 0.85보다 더 높은 값이 계산되는 것을 볼 수 있습니다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;ts1_dtw&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ts1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figure&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figsize&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;15&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;121&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Time series 1'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ts1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;122&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Comparison : ts1_dtw vs. ts2'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ts1_dtw&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ts2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;legend&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Time series 1 - Warping'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'Time series 2'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;corrcoef&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ts1_dtw&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ts2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;#### output
# array([[ 1.        ,  0.92247328],
#        [ 0.92247328,  1.        ]])
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2019-05-01/fig4.png&quot; width=&quot;550&quot; /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;긴 글을 읽어주셔서 감사합니다.&lt;/p&gt;

&lt;p&gt;[1] &lt;a href=&quot;https://jsideas.net/bitcoin_dtw/&quot;&gt;jsideas’s blog - Dynamic Time Warping: BitCoin&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[2] &lt;a href=&quot;https://en.wikipedia.org/wiki/Dynamic_time_warping&quot;&gt;wikipedia&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[3] &lt;a href=&quot;https://github.com/pierre-rouanet/dtw&quot;&gt;DTW (Dynamic Time Warping) python module&lt;/a&gt;&lt;/p&gt;</content><author><name>yjucho</name></author><category term="Time-series" /><summary type="html">두 시계열 데이터간의 유사도를 어떻게 계산할 수 있을까? 두 시계열이 동일한 길이의 시퀀스라면 단순히 상관계수를 구하는 것이 가능하지만, 현실 세계의 시계열 데이터는 그렇지 않은 경우가 많습니다. 예를 들어 아래와 같은 두 시계열 데이터를 살펴보겠습니다.</summary></entry><entry><title type="html">Detecting Spacecraft Anomalies Using LSTMs and Nonparametric Dynamic Thresholding</title><link href="http://localhost:4000/deep%20learning%20paper/time-series/Nonparametric-Dynamic-Thresholding/" rel="alternate" type="text/html" title="Detecting Spacecraft Anomalies Using LSTMs and Nonparametric Dynamic Thresholding" /><published>2019-03-15T00:00:00+09:00</published><updated>2019-03-15T00:00:00+09:00</updated><id>http://localhost:4000/deep%20learning%20paper/time-series/Nonparametric-Dynamic-Thresholding</id><content type="html" xml:base="http://localhost:4000/deep%20learning%20paper/time-series/Nonparametric-Dynamic-Thresholding/">&lt;p&gt;&lt;b&gt;Kyle Hundman et al (2018 KDD, NASA)&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;Implementation : &lt;a href=&quot;https://github.com/khundman/telemanom&quot;&gt;https://github.com/khundman/telemanom&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;abstract&quot;&gt;Abstract&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;NASA의 우주선은 많은 양의 원격 데이터를 송신합니다. NASA 연구원들은 우주선이 보내는 데이터들을 이용해 엔지니어의 모니터링 부담과 운영 비용을 줄이기 위해서 어노말리 디텍션 시스템을 구축/개선하고 있습니다.&lt;/li&gt;
  &lt;li&gt;이 논문은 우주선 데이터에 적용가능한 어노말리 디텍션 알고리즘을 제안합니다. expert-labeled 어노말리 데이터가 포함된 Soil Moiture Active Passive(SMAP) satelite와 Mars Science Laboratory(MSL) rover 데이터에 LSTM 모델을 적용한 새로운 어노말리 디텍션 방법을 제안하였습니다.&lt;/li&gt;
  &lt;li&gt;이 방식은 지도학습에 의한 모델 학습이 아니라 unsupervised and nonparametric anomaly thresholding approach에 해당하며, 후반부에는 false positive를 줄이기 위한 방법들을 추가적으로 논의하였습니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;우주선에서는 온도, 방사선, 전력, 소프트웨어의 계산량 등 복잡하고 방대한 양의 데이터들이 수집되며 각 데이터들의 어노말리 디텍션은 매우 중요한 문제입니다.&lt;/li&gt;
  &lt;li&gt;기존의 어노말리 디텍션 방법들은 사전에 정의된 제한값을 벗어나면 발생하는 알람 형식이거나 수작업이 포함된 시각화 방법과 채널별 통계 분석을 이용합니다. 이러한 방식은 적지않은 전문가적 지식을 요구하며 각 규칙을 정의하거나 노말 범위를 업데이트하는데 수기 작업이 필요합니다. 특히 하루에 85테라바이트의 데이터가 생성되는 방대한 양의 데이터를 처리하는 빅데이터 시스템에서는 더욱 문제가 악화됩니다.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;다변량 시계열 데이터의 어려운 이슈이 우주선 데이터를 분석하는데 여전히 유효하고, 라벨링 부족한 상황에서 비지도학습방식이나 세미지도학습방법의 필요성 역시 존재합니다. 또한 대부분의 실제 시계열 데이터가 그러하듯이 non-stationary한 특징과 현재 컨텍스에 매우 종속된 특징을 갖는 것들도 어려운 점입니다. 또한 엔지니어에게 인사이트를 줄수 있도록 interpretability도 필요한 요소입니다. 마지막으로 false positive와 false negative를 최소화하면서 적절한 발런스를 찾는 것 역시 중요합니다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;b&gt; Contributions &lt;/b&gt;
    &lt;ul&gt;
      &lt;li&gt;이 논문은 LSTM을 이용하여 높은 예측력을 얻고, 각 채널별 예측모델을 구축하여 전체 시스템의 interpretability를 유지하였습니다.&lt;/li&gt;
      &lt;li&gt;일단 모델의 예측값을 이용해 실제 값과의 오차(residual)을 이용해 어노말리인지 판단하게 됩니다. 이때 nonparametric, dynamic, and unsupervised thresholding approach를 사용합니다. 이 방식을 이용해 신호의 다양성, 비정상성과 노이즈에 대해서 논의하고 이후 사용자 피드백과 과거 데이터를 이용해 시스템을 향상시키는 방법도 함께 논의하였습니다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;background-and-related-work&quot;&gt;Background and Related Work&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;일반적으로 3가지 종류의 어노말리가 존재합니다.
    &lt;ul&gt;
      &lt;li&gt;point anomaly : low density regions에 해당하는 싱글 포인트가 발생하는 것을 의미합니다.&lt;/li&gt;
      &lt;li&gt;contextual anomaly : low density region은 아니지만 로컬 값들과 비교했을때는 비정상적인 싱글 포인트가 발생하는 경우입니다.&lt;/li&gt;
      &lt;li&gt;collective anomaly : 여러개의 시퀀스값들이 비정상적일 때를 의미합니다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;가장 기본적인 어노말리 디텍션은 out-of-limit(OOL)입니다.
    &lt;ul&gt;
      &lt;li&gt;그 외 clustering based approaches, nearest neighbors approaches, expert systems, dimensionality reduction approaches 등이 있지만, parameter specification, interpretability, generalizability, or computational expense 등의 단점이 존재합니다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;기존에도 우주선에 적용가능한 어노말리 디텍션 방법들이 다수 연구되었습니다. ISACS-DOC, IMSE, ELMER, Deep Space One spacecrash 등과 같은 프로젝트들이 있었습니다만, 여전히 직관적인 결과를 얻을수 있고 관리가 쉬운 OOL 방식이 사용되고 있습니다.&lt;/li&gt;
  &lt;li&gt;최근 딥러닝이 발전하면서 seq-to-seq 학습에서도 큰 성과를 얻고 있습니다. LSTM과 RNN계열의 모델을 이용해 과거값을 이용해 예측값을 학습할수 있습니다. 정상데이터로 학습된 LSTM을 이용하여 정상적인 상태에서의 시스템을 모니터링할수 있습니다. LSTM은 차원축소를 하지 않아도 다변량 시계열 데이터에 적용가능하고, 특별한 도메인 날리지를 요구하지 않기 때문에 다른 우주선에 일반적으로 적용가능합니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;method&quot;&gt;Method&lt;/h2&gt;

&lt;h3 id=&quot;telemetry-value-prediction-with-lstms&quot;&gt;Telemetry Value Prediction with LSTMs&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;b&gt;single channel models&lt;/b&gt; : 여기서는 각 채널별로 모델을 생성합니다. 싱글 모델의 장점은
    &lt;ul&gt;
      &lt;li&gt;채널 레벨로 추정가능하다는 것&lt;/li&gt;
      &lt;li&gt;로우 레벨의 어노말리를 그룹핑하여 서브시스템 형태로 통합할수 있습니다. 이로인해서 더 세분화된 시스템 관리가 가능합니다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;b&gt;predicting values for a channel &lt;/b&gt;: 주어진 시계열은 \(X=\left\{ x^{(1)}, x^{(2)}, ..., x^{(n)} \right\}\) 이고, \(x^{(t)}\)는 m차원의 벡터를 나타내고 각 element가 채널의 입력값을 나타냅니다.  \(l_s\)는 모델 입력으로 사용한 시퀀스의 길이를 의미합니다. \(l_p\)는 예측할 시퀀스의 길이를 나타내며 이 논문에서는 계산 속도를 위해서 1을 사용하였습니다. 또한 각 채널별 예측을 수행하기 때문에 예측값의 차원 d=1로 설정하였습니다. \(x^{(t)}\) 는 각 채널의 이전 값과 함께 우주선에 전송된 encoded command information이 포함됩니다. 커멘드를 생성한 것과 커멘드를 수신한 정보가 one-hot encoded되어 입력으로 사용됩니다. (Fig3 참고)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;dynamic-error-thresholds&quot;&gt;Dynamic Error Thresholds&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;수천개의 원격 데이터를 자동으로 모니터링하기 위해서는 계산속도가 빠르고, 예측값이 어노말리인지 판단하는 과정이 비지도 학습방식이어야합니다. 이를 위한 일방적인 방식은 과거의 스무딩된 에러들을 가우시안 분포로 가정하여 새로운 에러값과 이전 값들의 compact representation간의 빠른 비교가 되도록 하는 것입니다. 하지만 이 방식은 가우시안 분포라는 가정이 맞지 않을때는 문제가 되기때문에 여기서는 어떠한 가정없이 extreme values를 찾아내는 방식을 제안합니다. distance-based method가 비슷하지만 기존의 distance based method는 각 포인트들을 인근의 k개와 비교하기 때문에 계산량이 많다는 단점이 있습니다.&lt;/li&gt;
  &lt;li&gt;&lt;b&gt;Errors and Smoothing&lt;/b&gt; : 우선 예측값과 실제값 사이의 에러를 계산합니다.&lt;/li&gt;
&lt;/ul&gt;

\[e^{(t)} = \left\vert y^{(t)} - \hat{y}^{(t)} \right\vert \\
\boldsymbol{e}=[e^{(t-h)}, ..,e^{(t-1)}, e^{(t)}]\]

&lt;ul&gt;
  &lt;li&gt;이때 각 에러값들은 스무딩(smoothed)된 값들을 사용합니다. 정상적인 상태라도 값이 급변하여 완벽하게 예측이 되지 않아 스파이크 형태의 에러값이 생기는 경우가 종종 있기 때문입니다. 여기서는 Exponentially-weighted average(EWMA)를 사용하였습니다.&lt;/li&gt;
&lt;/ul&gt;

\[\boldsymbol{e_s}=[e_s^{(t-h)}, ..,e_s^{(t-1)}, e_s^{(t)}]\]

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;값들이 정상인지 판단하기 위해서는 threshold를 설정하여 사용하였습니다. threshold보다 큰 값은 anomalies로 분류됩니다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;b&gt;Threshold Calculation and Anomaly Scoring &lt;/b&gt; : 일반적으로 threshold를 결정하기 위해서 지도학습방식으로 학습을 합니다. 하지만 이 방식은 라벨링된 데이터가 필요하기 때문에 여기서는 비지도학습 형태로 threshold를 결정하는 방법을 제안하였습니다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

\[\boldsymbol{\epsilon} = \mu(\boldsymbol{e_s}) + z\sigma(\boldsymbol{e_s})\]

&lt;ul&gt;
  &lt;li&gt;Where \(\epsilon\) is determined by:&lt;/li&gt;
&lt;/ul&gt;

\[\epsilon = argmax(\boldsymbol{\epsilon}) = \frac{\triangle\mu(\boldsymbol{e_s})/\mu(\boldsymbol{e_s}) + \triangle\sigma(\boldsymbol{e_s})/\sigma(\boldsymbol{e_s})}{\left\vert \boldsymbol{e_a} \right\vert + \left\vert \boldsymbol{E_{seq}} \right\vert^2}\]

&lt;ul&gt;
  &lt;li&gt;such that:&lt;/li&gt;
&lt;/ul&gt;

\[\triangle\mu(\boldsymbol{e_s}) = \mu(\boldsymbol{e_s}) - \mu(\left\{ e_s \in \boldsymbol{e_s} \vert e_s \lt \epsilon \right\}) \\
\triangle\sigma(\boldsymbol{e_s}) = \sigma(\boldsymbol{e_s}) - \sigma(\left\{ e_s \in \boldsymbol{e_s} \vert e_s \lt \epsilon \right\}) \\
\boldsymbol{e_a} = \left\{ e_s \in \boldsymbol{e_s} \vert e_s \gt \epsilon \right\} \\
\boldsymbol{E_{seq}} = \mbox{continuous sequences of }e_a \in \boldsymbol{e_a}\]

&lt;ul&gt;
  &lt;li&gt;anomaly score&lt;/li&gt;
&lt;/ul&gt;

\[s^{(i)} = \frac{max(e^{(i)}_{seq})-argmax({\epsilon})}{\mu(\boldsymbol{e_s}) + \sigma( \boldsymbol{e_s})}\]

&lt;h3 id=&quot;mitigating-false-positives&quot;&gt;Mitigating False Positives&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;b&gt;Pruning Anomalies&lt;/b&gt;
    &lt;ul&gt;
      &lt;li&gt;prediction-based 방식은 과거데이터의 갯수(h)에 영향을 많이 받습니다.&lt;/li&gt;
      &lt;li&gt;너무 많은 과거 데이터를 이용할 경우, 실시간 모니터링 시나리오에서 계산 비용이 너무 크게 됩니다. 너무 적은 과거 데이터를 이용할 경우, 좁은 컨텍스트만 고려하여 판단하기 때문에 false positive가 많아지게 됩니다. 그렇다고 false positive를 너무 줄이다보면 감지되지 못한 어노말리를 찾기위해서 휴먼 인스펙션 부담이 커지게 됩니다. 따라서 false positives를 약화시키기 위해서 pruning procedure를 도입하였습니다.&lt;/li&gt;
      &lt;li&gt;\(\boldsymbol{e_{max}}\) is created containing \(max(\boldsymbol{e_{seq}})\) for all \(\boldsymbol{e_{seq}}\) sorted in descending order. we also add the maximum smoothed error that isn’t anomalous, \(max(\left\{ e_s \in \boldsymbol{e_s} \in \boldsymbol{E_{seq}} \vert e_s \ni \boldsymbol{e_a} \right\} )\), to the end of \(\boldsymbol{e_{max}}\).&lt;/li&gt;
      &lt;li&gt;The sequence is then stepped through incrementally and the the percent decrease \(d^{(i)} = ( e_{max}^{(i-1)} - e_{max}^{(i)}) / e_{max}^{(i-1)}\) at each step \(i\) is calculated where \(i \in \left\{1, 2, ..., (\left\vert \boldsymbol{E_{seq}} \right\vert + 1)\right\}\).&lt;/li&gt;
      &lt;li&gt;If at some step \(i\) a minimum percentage decrease p is exceeded by \(d^{(i)}\), all \(e_{max}^{(j)} \in \boldsymbol{e_{max}} \vert j \lt i\) and their corresponding anomaly sequences remain anomalies.&lt;/li&gt;
      &lt;li&gt;If the minimum decrease p in not met by \(d^{(i)}\) and for all subsequent errors \(d^{(i)}, d^{(i+1)}, ..., d^{(i+\left\vert \boldsymbol{E_{seq}} \right\vert + 1)}\) those smoothed error sequences are reclassified as nominal.&lt;/li&gt;
      &lt;li&gt;이와 같은 pruning 과정은 정상적인 흐름에서의 노이즈가 어노말리로 판단되는 것을 방지합니다. 또한 단순히 값과 값을 여러번 비교하여 판단하는 것보다 잠재가능성이 있는 비정상적인 시퀀스 중에 맥시멈 에러갓을 비교하는 것이 더 효율적이라는 장점이 있습니다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;b&gt;Learning from History&lt;/b&gt;
    &lt;ul&gt;
      &lt;li&gt;false postive를 감소시키는 두번째 전략은 적은양이더라도 과거의 비정상 값들 또는 라벨링된 데이터를 적용시키는 것입니다. 각 채널 데이터에서 일정 비율 이상 수집된 값들을 어노말리로 판단하여 미니멈 값 \(s_{min}\)로 설정합니다. 이후 새로운 값들 중 \(s \lt s_{min}\)조건을 만족하는 경우 정상값으로 분류합니다.  \(s_{min}\)는 precision과 recall사이의 적절한 밸러스가 되도록 설정할수 있습니다.&lt;/li&gt;
      &lt;li&gt;또는 유저가 제공하는 라벨링 정보를 이용하여 \(s_{min}\)를 설정할수도 있습니다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2019-03-15/fig3.png&quot; width=&quot;550&quot; /&gt;&lt;br /&gt;&lt;/p&gt;</content><author><name>yjucho</name></author><category term="Deep Learning paper" /><category term="Time-series" /><summary type="html">Kyle Hundman et al (2018 KDD, NASA)</summary></entry><entry><title type="html">RobustSTL : A Robust Seasonal-Trend Decomposition Algorithm for Long Time Series</title><link href="http://localhost:4000/deep%20learning%20paper/time-series/robustSTL/" rel="alternate" type="text/html" title="RobustSTL : A Robust Seasonal-Trend Decomposition Algorithm for Long Time Series" /><published>2019-02-24T00:00:00+09:00</published><updated>2019-02-24T00:00:00+09:00</updated><id>http://localhost:4000/deep%20learning%20paper/time-series/robustSTL</id><content type="html" xml:base="http://localhost:4000/deep%20learning%20paper/time-series/robustSTL/">&lt;p&gt;&lt;b&gt;Qingsong Wen et al (2018, Alibaba Group)&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;Implementation : &lt;a href=&quot;https://github.com/LeeDoYup/RobustSTL&quot;&gt;https://github.com/LeeDoYup/RobustSTL&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;abstract&quot;&gt;Abstract&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;시계열데이터를 trend, seasonality, and remainder components로 분해하는 것은 어노말리 디텍션이나 예측 모델을 만드는데 중요한 역할을 합니다.&lt;/li&gt;
  &lt;li&gt;기존의 여러가지 성분분해 방식들은
    &lt;ul&gt;
      &lt;li&gt;1) 주기성이 변하거나 이동하는 것, 트렌드나 나머지성분의 갑작스러운 변화를 잘 처리하지 못하며(seasonality fluctuation and shift, and abrupt change in trend and reminder)&lt;/li&gt;
      &lt;li&gt;2) 어노말리 데이터에 대해서 로버스트하지 못하거나&lt;/li&gt;
      &lt;li&gt;3) 주기가 긴 시계열 데이터에 대해서 적용하기 어려운 문제가 있습니다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;본 논문에서는 위와 같은 문제점을 해결할 수 있는 새로운 성분 분해 방식을 제안합니다.
    &lt;ul&gt;
      &lt;li&gt;먼저 sparse regularization와 least absolute deviation loss를 이용해 트렌드를 뽑고&lt;/li&gt;
      &lt;li&gt;Non-local seasonal filter를 사용하여 seasonality 성분을 얻습니다.&lt;/li&gt;
      &lt;li&gt;이 과정을 정확한 디컴포지션을 얻을때까지 반복합니다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;실험데이터와 실제 시계열데이터에 대해서 기존 방법들 대비 더 좋은 성능을 보임을 확인하였습니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;디컴포지션 방법으로 널리 사용되는 방법은 STL(seasonal trend decomposition using Loess), X-13-ARIMA-SEATS, X-11-ARIMA, X-12-ARIMA 등이 있습니다. 하지만 seasonality shift나 fluctuation이 존재할 경우 정확하지 않거나, 빅데이터에 존재하는 long seasonality에는 적합하지 않습니다.
    &lt;ul&gt;
      &lt;li&gt;seasonality fluctuation and shift - 하루가 주기인 시계열 데이터가 있다고 했을때, 오늘 1시에서의 seasonality component는 어제의 12시 30분에 대응되고, 그제의 1시 30분에 대응될수 있음&lt;/li&gt;
      &lt;li&gt;Abrupt change of trend and remainder - local anomaly could be a spike during an idle period (busy day의 높은 값보다는 낮아서 정확히 디텍션하기 어려움&lt;/li&gt;
      &lt;li&gt;Long seasonality - 보통은 quarterly or monthly data임. T 주기의 시즈널리티를 찾기 위해서는 T-1개의 데이터가 필요함. 하루 주기에 1분 간격 데이터의 경우 T=1440개고 이와 같은 long seasonality는 기존 방법들로는 풀기어려움&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;이 논문에서 제안한 방법은 Long seasonality period and high noises 더라도 시즈널리티를 비교적 정확하게 디컴포지션할수 있습니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;robust-stl-decomposition&quot;&gt;Robust STL Decomposition&lt;/h2&gt;
&lt;h3 id=&quot;model-overview&quot;&gt;Model Overview&lt;/h3&gt;

\[\begin{align}
y_t &amp;amp; = \tau_t + s_t + r_t, &amp;amp; t = 1, 2, …, N \\
r_t &amp;amp; = a_t + n_t \\
\end{align}\]

&lt;p&gt;where \(a_t\) denotes spike or dip, and \(n_t\) denotes the white noise.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;시계열 모델은 트렌드(\(\tau_t\)), 시즈널리티(\(s_t\)), 리마인더(\(r_t\))로 구성되어 있고, 리마인더는 스파크 또는 딥과 같은 어노말리(\(a_t\))와 화이트 노이즈(\(n_t\))로 이루어집니다.&lt;/li&gt;
  &lt;li&gt;제안하는 알고리즘은 크게 4-steps 으로 각 성분을 분해합니다.
    &lt;ul&gt;
      &lt;li&gt;Denoise time series by applying bilateral filtering&lt;/li&gt;
      &lt;li&gt;Extract trend robustly by solving a LAD regression with sparse regularization&lt;/li&gt;
      &lt;li&gt;Calculate the seasonality component by applying a non-local seasonal filtering to overcome seasonality fluctuation and shift&lt;/li&gt;
      &lt;li&gt;Adjust extracted components&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;noise-removal&quot;&gt;Noise Removal&lt;/h3&gt;

\[\begin{align}
y^\prime_t &amp;amp; = \sum_{j \in J} w_j^t y_t, &amp;amp; J = t, t \pm 1, …, t \pm H \\
w_j^t &amp;amp; = \frac{1}{z} e^{-\frac{\left\vert j- t \right\vert ^2}{2\delta_d^2}} e^{-\frac{\left\vert y_j - y_t \right\vert ^2}{2\delta_i^2}}
\end{align}\]

&lt;ul&gt;
  &lt;li&gt;J는 필터의 윈도우를 의미하며, 윈도우 사이즈는 2H+1 입니다.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;필터의 가중치는 두개의 가우시안 함수로 구성됩니다. bilateral filter는 &lt;a href=&quot;https://en.wikipedia.org/wiki/Bilateral_filter&quot;&gt;여기&lt;/a&gt;를 참고하세요.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;After denoising,&lt;/li&gt;
&lt;/ul&gt;

\[\begin{align}
y^\prime_t &amp;amp; = \tau_t + s_t + r^\prime_t, &amp;amp; t = 1, 2, …, N \\
r^\prime_t &amp;amp; = a_t + (n_t - \hat{n}_t \\
\end{align}\]

&lt;p&gt;Where the \(\hat{n}_t = y_t - y^\prime_t\) is the filtered noise.&lt;/p&gt;

&lt;h3 id=&quot;trend-extraction&quot;&gt;Trend Extraction&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;시즈널 디퍼런스 오퍼레이터는 같은 주기의 값을 차분하는 것으로 아래와 같이 정의할 수 있습니다.&lt;/li&gt;
&lt;/ul&gt;

\[\begin{align}
g_t &amp;amp; = \nabla_T y^\prime_t = y^\prime_t - y^\prime_{t-T} \\
&amp;amp; = \nabla_T \tau_t + \nabla_T s_t + \nabla_T r^\prime_t \\
&amp;amp; = \sum_{I=0}^{T-1} \nabla \tau_{t-i} + ( \nabla_T s_t + \nabla_T r^\prime_t )
\end{align}\]

&lt;ul&gt;
  &lt;li&gt;마지막 줄의 수식에서 첫번째 항이 \(g_t\)에 가장 많은 기여를 합니다. \(s_t\) and \(r^\prime_t\)에 시즈널 디퍼런스 오퍼레이터를 적용하면 값이 매우 작아진다고 가정하기 때문입니다.&lt;/li&gt;
  &lt;li&gt;\(g_t\)에서 트렌드의 first order differece(\(\nabla \tau_t\))를 구하기 위해서 다음과 같은 최적화 식을 사용합니다.&lt;/li&gt;
&lt;/ul&gt;

\[Minimize \ \sum_{t=T+1}^N \left\vert g_t - \sum_{I=0}^{T-1} \nabla \tau_{t-i} \right\vert + \lambda_1 \sum_{t=2}^N \left\vert \nabla \tau_t \right\vert + \lambda_2 \sum_{t=3}^N \left\vert \nabla^2 \tau_t \right\vert\]

&lt;ul&gt;
  &lt;li&gt;첫번째 항은 LAD를 사용한 emprical error를 의미합니다. sum-of-squares 보다 아웃라이어에 대해서 더 로버스트하기 때문에 LAD를 사용하였습니다.&lt;/li&gt;
  &lt;li&gt;두번째와 세번째 항은 각 각 트렌드에 대한 first-order 와 second-order difference operator 입니다.&lt;/li&gt;
  &lt;li&gt;두번째 항은 트렌드 디퍼런스 \(\nabla \tau_t\) 가 천천히 변화하지만 종종 갑작스러운 레벨 쉬프트(abrupt level shift)가 있다는 것을 의미합니다.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;세번째 항은 트렌드가 smooth하고 piecewise linear such that \(\nabla^2 x_t = \nabla(\nabla x_t)) = x_t -2 x_{t-1} + x_{t-2}\) are sparse&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;이를 매트릭스 형태로 표현하면 다음과 같습니다.&lt;/li&gt;
&lt;/ul&gt;

\[\Vert P \nabla \tau - q \Vert _1\]

&lt;p&gt;where the matrix P and vector q are&lt;/p&gt;

\[P = \begin{bmatrix}
M_{(N-T) \times (N-1)} \\
\lambda_1 I_{(N-1) \times (N-1)} \\
\lambda_2 D_{(N-2) \times (N-1)} \\
\end{bmatrix}, 
q = \begin{bmatrix}
g_{(N-T) \times 1} \\
0_{(2N-3) \times 1} \\
\end{bmatrix}\]

&lt;p&gt;M and D are Toeplitz matrix (refer to the paper for details)&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;위의 최적화식을 통해서 \(\tau_1\)에 대한 상대적인 트렌드(relative trend, \(\tilde{\tau}_t^r\))를 구할수 있습니다.&lt;/li&gt;
&lt;/ul&gt;

\[\tilde{\tau}_t^r = \tilde{\tau}_t - \tau_1 = 
\begin{cases}
0, &amp;amp; t=1 \\
\sum_{I=2}^t \nabla \tilde{\tau}_i, &amp;amp; t \ge 2
\end{cases}\]

&lt;ul&gt;
  &lt;li&gt;그리고 나서, 디컴포지션 모델은 아래와 같이 업데이트 됩니다. 
\(y_t'' = y_t' - \tilde{\tau}_t^r = s_t + \tau_1 + r_t'' \\
r_t’’ = a_t + (n_t - \hat{n}_t) +  (\tau_t - \tilde{\tau}_t)\)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;seasonality-extraction&quot;&gt;Seasonality Extraction&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;relative trend component를 분리한 후에는, \(y’’_t\)는 시즈널리티로 오염되어 있다고 생각할 수 있습니다.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;기존의 시즈널리티 분해 방법들은 주기가 \(T\)인 \(s_t\)’를 구하기 위해서는 K개의 연속적인 값 \(y_{t-KT}, y_{t-(K-1)T}, …, y_{t-T}\) 만 고려하였습니다. 하지만, 이 방식은 시즈널리 쉬프트 현상을 설명할수 없다는 단점이 있습니다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;여기서는  \(y’’_{t-KT}\)를 중심으로 인접한 값들을 고려합니다.  \(y’’_{t-KT}\)를 계산할때는 그 값을 중심으로 2H+1개의 인접값들 \(y’’_{t-KT-H}, y’’_{t-KT-H+1}, …, y’’_{t-KT}, y’’_{t-KT+1}…, y’’_{t-KT+H}\)를 사용합니다.&lt;/li&gt;
  &lt;li&gt;시즈널 컴포넌트 \(s_t\) 는 아래와 같이 of \(y’’_t\)의 가중합으로 표현됩니다.&lt;/li&gt;
&lt;/ul&gt;

\[\tilde{s}_t = \sum_{(t’, j) \in \Omega} w^t_{(t’,j’)}y’’_j\]

&lt;p&gt;Where the \(w^t_{(t’,j’)}\) and \(\Omega\) are defined as&lt;/p&gt;

\[w^t_{(t’,j’)} = \frac{1}{z}e^{-\frac{\left\vert j- t \right\vert ^2}{2\delta_d^2}} e^{-\frac{\left\vert y’’_j - y’’_{t’} \right\vert ^2}{2\delta_i^2}} \\
\Omega = \{(t’,j) \vert (t’=t-k \times T, j= t’ \pm h )\} \\
k=1, 2, …, K; \ h=0, 1, …, H\]

&lt;ul&gt;
  &lt;li&gt;시즈널티리를 분리한 후에는, 리마이더 시그널은 아래와 같이 표현됩니다. 
\(r’’’_t = y’’_t - \tilde{s}_t = a_t + (n_t - \hat{n}_t) + (\tau_t - \tilde{\tau}_t) + (s_t - \tilde{s}_t)\)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;final-adjustment&quot;&gt;Final Adjustment&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;시즈널리티 컴포넌트의 합계는 0으로 조정되어야합니다.&lt;/li&gt;
&lt;/ul&gt;

\[\sum_{I=j}^{I=j+T-1}s_i = 0\]

&lt;ul&gt;
  &lt;li&gt;따라서 평균값(트렌드 \(\tau_1\)에 대응되는 값)을 빼줌으로서 시즈널리트를 조정합니다.&lt;/li&gt;
&lt;/ul&gt;

\[\hat{\tau}_1 = \frac{1}{T\lfloor N/T \rfloor} \sum_{t=1}^{T\lfloor N/T \rfloor} \tilde{s}_t \\
\hat{s}_t = \tilde{s}_t - \hat{\tau}_1 \\
\hat{\tau}_t = \tilde{\tau}^r_t + \hat{\tau}_1 \\
\hat{r}_t = y_t - \hat{s}_t + \hat{\tau}_t\]

&lt;ul&gt;
  &lt;li&gt;리마인더 시그널 \(\hat{r}_t\) 가 수렴할 때까지 위 과정을 반복합니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2019-02-24/algorithm1.png&quot; width=&quot;550&quot; /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2019-02-24/fig3.png&quot; width=&quot;550&quot; /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2019-02-24/fig4.png&quot; width=&quot;550&quot; /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2019-02-24/table2.png&quot; width=&quot;450&quot; /&gt;&lt;br /&gt;&lt;/p&gt;</content><author><name>yjucho</name></author><category term="Deep Learning paper" /><category term="Time-series" /><summary type="html">Qingsong Wen et al (2018, Alibaba Group)</summary></entry><entry><title type="html">How does batch normalization help optimization?</title><link href="http://localhost:4000/deep%20learning%20paper/batchnorm/" rel="alternate" type="text/html" title="How does batch normalization help optimization?" /><published>2019-02-12T00:00:00+09:00</published><updated>2019-02-12T00:00:00+09:00</updated><id>http://localhost:4000/deep%20learning%20paper/batchnorm</id><content type="html" xml:base="http://localhost:4000/deep%20learning%20paper/batchnorm/">&lt;p&gt;&lt;b&gt;Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas et al. (2018, MIT) &lt;/b&gt;&lt;/p&gt;

&lt;h2 id=&quot;abstract&quot;&gt;Abstract&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;BatchNorm은 딥러닝의 안정적이고 학습속도를 빠르게 하는데 도움을 주는 기법으로 널리 활용되고 있습니다.&lt;/li&gt;
  &lt;li&gt;하지만 그 활용성에 비해서 왜 BatchNorm이 효과적인지에 대한 실질적인 고찰은 거의 없었으며, 대부분은 internal covariance shift를 줄이는 효과를 줄이기 때문이라고 믿고 있습니다.&lt;/li&gt;
  &lt;li&gt;이 논문에서는 internal covariance shift라는 것이 실제로는 BatchNorm과 거의 상관없다는 것을 실험적으로 확인하였습니다. BatchNorm기법이 최적화 함수를 훨씬 smoother하게 만들어주기 때문이라는 것을 이론적, 실험적으로 확인하였으며, 이 영향으로 인해 그래디언트가 더 안정적으로 움직여 빠른 학습이 가능하다는 것을 주장합니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;지난 몇년간 딥러닝이 컴퓨터비전, 스피치 인식 등과 같은 풀기어려운 문제를 해결하는데 성공하였으며, 이러한 성공에는 BatchNorm이 큰 기여를 하고 있습니다.&lt;/li&gt;
  &lt;li&gt;BatchNorm의 실용성은 논란의 여기가 없지만, 그러한 효과가 왜 발생하느지에 대한 명확한 이유는 아직 밝혀지지 않았습니다.&lt;/li&gt;
  &lt;li&gt;BatchNorm이 처음 제안되었을 때는 internal covariance shift(ICS)를 최소화하기 위한 방안으로 설명되었지만, 이 연구에서는 ICS와의 연관성을 말해주는 상세한 증거를 찾지 못하였습니다,&lt;/li&gt;
  &lt;li&gt;Constribution
    &lt;ul&gt;
      &lt;li&gt;BatchNorm과 ICS는 아무런 관련이 없다는 것을 설명하며&lt;/li&gt;
      &lt;li&gt;BatchNorm이 효과적인 이유는 최적화 함수를 더 smooth하게 만들어 learning rate가 더 크더라도 안정적인 그래디언트를 보장하여 더 빠른 학습을 가능하게 하기 때문임을 확인하였습니다.&lt;/li&gt;
      &lt;li&gt;이러한 내용을 실험적인 확인 이외에 loss함수와 그 gradient의 Lipschitzness 를 이용해 이론적으로 설명하였습니다.&lt;/li&gt;
      &lt;li&gt;이 고찰을 통해서 BatchNorm과 동일한 효과를 가져오는 다른 기법들이 존재할수 있는 가능성을 제시하였습니다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;batch-normalization-and-internal-covariance-shift&quot;&gt;Batch normalization and internal covariance shift&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;처음에 Ioffe and Szegedy의 BatchNorm은 모델이 학습될때는 파라미터가 바뀌기때문에 각 레이어의 입력값들의 분포가 달라지는 현상(internal covariate shift)를 줄이기 위해 제안되었습니다.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;BatchNorm이란 각 레이어의 액티베이션값을 평균과 분산을 각 각 0과 1로 정규화시킨 후, 모델의 설명력을 유지하기 위해 다시 scaled and shifted를 해주는 과정으로 이루집니다. 이 과정은 이전 레이어의 non-linearity전에 이루어집니다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Does BatchNorm’s performance stem form controlling internal covariate shift?
    &lt;ul&gt;
      &lt;li&gt;BatchNorm 논문에서는 각 레이어의 인풋 분포의 평균과 분산을 제한하는 것이 학습성능에 직접적인 영향을 준다고 주장하였습니다. 이 것을 입증하기 위해 한가지 실험을 수행하였습니다.&lt;/li&gt;
      &lt;li&gt;배치놈 이후에 랜덤한 노이즈를 일부러 주입하여 네트워크를 학습시켜 보았습니다. 노이즈는 평균이 0이 아니고, 분산도 1이 아닌 분포에서 추출하여 각 레이어의 액티베이션에 더해주었습니다. 이 때, 각 스텝마다 노이즈의 분포가 달라지도록 하여 꽤 심한 covariate shift를 만들어보았습니다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2019-02-12/fig2.png&quot; width=&quot;550&quot; /&gt;&lt;br /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Fig2는 standard, standard + batchnorm, standard + noisy batchnorm 세가지 네트워크의 학습 성능(좌)을 나타냅니다. 또한 시간에 따른 레이어의 액티베이션의 분포들(우)을 함께 표시하였습니다. 그림에서 볼수 있듯이, 학습데이터셋을 기준으로 batchnorm과 noisy batchnorm의 성능차이는 거의 없는 것을 확인하였습니다. 두가지 모두 standard 보다 높은 성능을 보였습니다.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;i&gt;Noisy batchnorm의 액티베이션 분포들은 불안정하지만, 학습 성능은 좋다는 실험 결과는 batchnorm의 효과가 레이어의 인풋 분포를 안정시키기 때문이다는 주장을 반박하는 결과입니다.&lt;/i&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Is BatchNorm reducing internal covariate shift?
    &lt;ul&gt;
      &lt;li&gt;그렇다면 더 광의적인 측면의 internal covariate shift가 batchnorm의 높은 학습성능과 직접적으로 연관된 것은 아닐까?&lt;/li&gt;
      &lt;li&gt;네트워크의 각각 레이어는 주어진 인풋에 대해서 리스크 최적화 문제를 푸는 것으로 생각할수 있고, 파라미터가 업데이트될 때마다 인풋을 바꾸고, 결과적으로 최적화 문제 자체를 바꾸게 됩니다. Ioffe and Szegedy는 이 현상을 internal covariate shift라고 불렀고, 각 레이어의 인풋 분포 관점에서 설명하려고 했습니다. 하지만 이 관점은 batchnorm의 성공적인 성능을 설명해주지 못한다는 것을 앞서 실험을 통해 살펴보았습니다.&lt;/li&gt;
      &lt;li&gt;인풋 분포가 아니라 전체 최적화 관점에서 각 레이어의 그래디어트 변화를 살펴보도록 하겠습니다.&lt;/li&gt;
      &lt;li&gt;이를 위해서 어떤 레이어의 전(before)/후(after) 그래디언트 변화를 다음과 같이 정의하도록 하겠습니다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

\[\mbox{We define internal covariate shift(ICS) of activation i at time t  to be the difference } \lVert G_{t, i} - G’_{t, I} \rVert_2  \mbox{ where} \\
G_{t,i} = \nabla_{W_i^{(t)}} \mathcal{L}(W_1^{(t)}, …, W_k^{(t)} ; x^{(t)}, y^{(t)}) \\
G^\prime_{t,i}  = \nabla_{W_i^{(t)}} \mathcal{L}(W_1^{(t+1)}, …, W_{i-1}^{(t+1)}, W_i^{(t)}, W_{i+1}^{(t)}, …, W_k^{(t)} ; x^{(t)}, y^{(t)})\]

&lt;ul&gt;
  &lt;li&gt;\(G_{t, i}\)는 모든 레이어가 동시에 업데이트되는 가정에서의 그래디언트(as is typical)이고, \(G’_{t, I}\)는 i번째 레이어 이전의 레이어들이 새로운 값으로 업데이트된 후의 그래디언트입니다. 따라서 G와 G’의 차이는 i번째 레이어의 인풋이 변함에 따라 그 파라미터(\(W_i\))의 optimization landscape가 얼마나 변화하는지를 나타냅니다.&lt;/li&gt;
  &lt;li&gt;정의된 지표를 internal covariate shift 정도로 사용하고, batchnorm을 사용했을때와 사용하지 않았을때를 비교했습니다. &lt;i&gt;기존의 Batchnorm 논문의 주장대로라면 batchnorm을 사용하는 경우, G와 G’간의 상관관계가 높아지기 때문에 ICS는 낮아져야합니다.&lt;/i&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2019-02-12/fig3.png&quot; width=&quot;550&quot; /&gt;&lt;br /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;실험결과, BatchNorm을 사용한 네트워크의 ICS가 오히려 증가하는 것으로 나타났습니다. - fig3&lt;/li&gt;
  &lt;li&gt;(Fig3) Standard + batchNorm가 standard 보다 더 빠르게 학습되지만 (첫번째 컬럼, 정확도와 로쓰 차트),  Standard + batchNorm와 standard의 ICS 변화는 거의 비슷하거나, Standard + batchNorm의 ICS가 standard보다 높은 것으로 나타났습니다. (두번째 &amp;amp; 세번째 컬럼)&lt;/li&gt;
  &lt;li&gt;&lt;i&gt;이 실험결과는 batchNorm 사용하더라도 G와 G’가 서로 uncorrelated하다는 것을 의미합니다. 즉 batchnorm을 사용하여 인풋 분포를 조절하는게 internal covariate shift를 줄이지 못한다는 것입니다.&lt;/i&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;why-does-batchnorm-work&quot;&gt;Why does BatchNorm work?&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;The smoothing effect of BatchNorm
    &lt;ul&gt;
      &lt;li&gt;그렇다면 왜 BatchNorm이 효과적일까?&lt;/li&gt;
      &lt;li&gt;결론부터 말하면, BatchNorm이 우리가 풀어야할 최적화문제의 landscape를 smooth하게 만들어주기 때문입니다. loss function의 Lipschitzness를 높여줘, 더 효과적인 \(\beta\)-smoothness를 갖도록 합니다. loss가 작은 비율로 변화하면 gradient의 변화량도 작아집니다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;\(\mbox{f is L-Lipschitz, If} \left\vert f(x_1) - f(x_2) \right\vert \le L \lVert x_1 - x_2 \rVert, \mbox{for all} x_1 \ and \ x_2 \\
\mbox{f is } \beta-smooth \mbox{, If its gradients are } \beta-Lipschitz \mbox{i.e, if} \left\vert \nabla f(x_1) - \nabla f(x_2) \right\vert \le \beta \lVert x_1 - x_2 \rVert, \mbox{for all } x_1 \ and \ x_2\)&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Non-BatchNorm 네트워크의 loss function은 non-convex하고 flat regions 또는 sharp local minima를 갖고 있고 있어 gradient가 갑자기 사라지거나(flat region), 갑자기 폭발하기도(sharp local minima)하죠. 반면 BatchNorm에 의해 smooth된 loss function은 gradient가 이러한 위험에 빠질 가능성이 더 낮아 더 안정적이고 예측가능한 학습을 할수 있게 됩니다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Exploration of the optimization landscape
&lt;img src=&quot;/assets/img/2019-02-12/fig4.png&quot; width=&quot;550&quot; /&gt;&lt;br /&gt;
    &lt;ul&gt;
      &lt;li&gt;우선 Loss function의 Lipschitzness를 실험적으로 살펴보았습니다.&lt;/li&gt;
      &lt;li&gt;(Fig4)(a)는 학습시간에 따라 그래디언트에 따라서 움직였을때, loss값이 얼마나 바뀌었는지를 나타냅니다. BatchNorm을 사용한지 않은 바닐라 네트워크에서는 값의 변동폭이 큰 것을 볼수 있습니다. (b)현재 그래디언트 방향과 이전 그래디언트 방향간의 l2 distance를 나타납니다. 마찬가지로 바닐라 네트워크는 그래디언트 간의 거리가 상대적으로 멀고 이는 predictiveness of the gradient가 낮다는 것을 의미합니다. (c)는 effective \(\beta\)-smoothness를 나타냅니다. effective라는 것은 그래디언트 방향으로 움직였을때 그래디언트가 얼마나 바뀌는지를 나타내며, 낮을수록 effective하다고 생각합니다. 이 결과도 앞서와 마찬가지로 BatchNorm을 사용한 경우가 더 effective하다고 나타납니다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Is BatchNorm the best (only?) way to smoothen the landscape?
    &lt;ul&gt;
      &lt;li&gt;loss function의 landscape을 smooth하게 만드는 것이 BatchNorm방식이 유일한 것일까?&lt;/li&gt;
      &lt;li&gt;실험을 위해서 여기서는 first momentum(평균)은 batchnorm처럼 고정하고 normalizes를 \(l_p\)-norm으로 normalizes해보았습니다. (이렇게 정규화된 값들은 더이상 가우시안 분포가 아니고 안정적인 분포를 보장하진 않지만, Fig5에서 나타나듯 BatchNorm과 동일한 성능을 보입니다.&lt;/li&gt;
      &lt;li&gt;논문에서는 Appendix 결과들을 통해서 \(l_p\)-normalization 기법들이 covariate shift를 더 많이 일으키지만, Batchnorm과 마찬가지로 standard 네트워크보다 성능이 좋고 landscape의 smoothness를 개선시킨다는 결과를 말해주고 있습니다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;theoretical-analysis&quot;&gt;Theoretical Analysis&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;TBU&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;이 논문에서는 batchnorm의 효과성이 어디에서 오는지 근본적인 원인을 살펴보았습니다.&lt;/li&gt;
  &lt;li&gt;batchnorm은 internal covariate shift와 거의 상관이 없으며, batchnorm이 오히려 internal covariate shift를 증가시키는 것으로 나타났습니다.&lt;/li&gt;
  &lt;li&gt;대신에 batchnorm은 최적화문제의 landscape를 부드럽게 해주는 효과를 가져오고, 이로 인해서 그래디언트가 예측가능하고, 잘 움직이게 합니다. 이로 인해서 하이퍼파라미터에 로버스트하고, 그래디언트가 사라지거나 폭발하는 현상이 줄어들게 됩니다. 또한 이러한 효과는 batchnorm이 유일하지 않고, 다른 노말리제이션방법들도 동일한 결과를 얻을수 있음을 확인하였습니다.&lt;/li&gt;
&lt;/ul&gt;</content><author><name>yjucho</name></author><category term="Deep Learning paper" /><summary type="html">Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas et al. (2018, MIT)</summary></entry><entry><title type="html">Self-Supervised Generative Adversarial Networks</title><link href="http://localhost:4000/generative%20adversarial%20network/deep%20learning%20paper/self-supervised-gan/" rel="alternate" type="text/html" title="Self-Supervised Generative Adversarial Networks" /><published>2019-02-08T00:00:00+09:00</published><updated>2019-02-08T00:00:00+09:00</updated><id>http://localhost:4000/generative%20adversarial%20network/deep%20learning%20paper/self-supervised-gan</id><content type="html" xml:base="http://localhost:4000/generative%20adversarial%20network/deep%20learning%20paper/self-supervised-gan/">&lt;p&gt;&lt;b&gt; Ting Chen et al. (Google Brain, 2018)&lt;/b&gt;&lt;/p&gt;

&lt;h2 id=&quot;abstract&quot;&gt;Abstract&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Conditional GAN은 이미지 생성에서 탁월한 성능을 보이지만, 많은 양의 라벨링 데이터를 필요로 한다는 단점이 있습니다.&lt;/li&gt;
  &lt;li&gt;이 논문은 self-supervision learning과 adversarial training 기법을 적용하여 별도의 라벨링 없이도 image representations을 학습하고 좋은 품질의 이미지를 생성할 수 있음을 보였습니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;GAN의 학습은 고차원의 파라미터 공간에서 non-convex 게임의 내쉬 이퀼리브리움을 찾는 것이기때문에 매우 불안정한다는 단점은 잘 알려져있습니다.&lt;/li&gt;
  &lt;li&gt;학습이 불안정한 현상을 보이는 이유는 generator와 discrimator가 non-stationary environment 에서 학습되기 때문입니다. 특히 discriminator는 fake class의 분포가 계속 변하게 되어 학습에 어려움을 겪습니다. Non-stationary 환경에서는 뉴럴넷은 이전에 학습 정보를 잊어버리고, 만약 discriminator가 이전의 분류 바운더리를 잊어버리면 학습 과정이 불안정(unstable)해지거나 주기적(cyclic)인 현상이 나타납니다.&lt;/li&gt;
  &lt;li&gt;이 현상을 극복하기 위해서 이전 연구들은 주로 conditioning 기법을 사용하였습니다. Supervised information(클래스 라벨)을 이용해 discriminator를 학습시키면 학습이 더 안정되고 catastrophic forgetting같은 현상이 경감됩니다.&lt;/li&gt;
  &lt;li&gt;하지만 기존 방식은 많은 양의 라벨링 데이터가 필요합니다. 또한 라벨링데이터가 있다하더라도 굉장히 sparse하기 때문에 고차원의 추상화된 공간을 모두 커버하기에는 한계가 존재합니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;b&gt;Contribution&lt;/b&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;이 논문은 라벨링된 데이터 없이 conditioning기법의 장점을 이용하고자 기존 GAN에 self-supervised loss를 더한 Self supervised GAN(SSGAN)을 제안하였습니다.&lt;/li&gt;
  &lt;li&gt;실험을 통해서 self-supervised GAN(SSGAN)이 동일한 실험 조건에서는 unconditional GAN보다 더 좋은 성능을 보임을 확인하였습니다.&lt;/li&gt;
  &lt;li&gt;SSGAN은 향후  high quality, fully unsupervised, natural image synthesis의 새로운 가능성을 제시하였습니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;a-key-issue--discriminator-forgetting&quot;&gt;A key Issue : discriminator forgetting&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2019-02-08/fig2.png&quot; width=&quot;400&quot; /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Fig2 : 학습이 진행될동안 discriminator의 분류 정확도를 관찰한 결과, unconditional GAN은 500k iterations 이후에는 학습된 정보를 잃어버리고 성능이 낮아지는 현상이 일어났습니다. 반면 SSGAN은 학습이 지속됨에 따라 분류 성능도 점차 향상하는 것을 볼 수 있었습니다. (이미지넷 데이터)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2019-02-08/fig3.png&quot; width=&quot;450&quot; /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Fig3 : cifar10 데이터에 대해서 각 클래스마다 1k iterations을 학습시키고 10개 클래스에 대해서 10k iterations이 지나면, 다시 처음 클래스를 학습하는 실험을 하였습니다. (a)는 바닐라 클래시파이어로 10k 이후에도 클래스가 바뀔때마다 학습성능이 떨어졌다가 올라가는 모습이 나타나지만, (b)self-supervised loss가 추가된 클래시파이어는 이전 정보를 잃어버리는 현상이 완화된 것을 볼 수 있습니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;the-self-supervised-gan&quot;&gt;The Self-Supervised GAN&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2019-02-08/fig1.png&quot; width=&quot;700&quot; /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Fig1 : SSGAN의 구조는 discriminator가 generator의 학습성능과 상관없이 의미있는 representation을 학습하도록 되어있습니다. 이를 위해서 이미지를 회전시킨 후 회전된 각도를 예측하도록 하는 self-supervision task을 사용하였습니다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;회전각도를 예측하는 것을 포함한 loss function은 다음과 같습니다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

\[L_G = -V(G,D) - \alpha \mathbb{E}_{x \sim P_G}  \mathbb{E}_{r \sim R} [log Q_D (R = r \vert x^r )], \\
L_D = V(G,D) - \beta \mathbb{E}_{x \sim P_{data}}  \mathbb{E}_{r \sim R} [log Q_D (R = r \vert x^r )]\]

&lt;ul&gt;
  &lt;li&gt;\(V(G,D)\)은 GAN의 loss function이고, \(r \in R\)은 회전각입니다. 이 논문에서는 \(R={0도, 90도, 180도, 270도}\)을 사용하였습니다. 이미지 x가 r degree만큼 회전한 것을 \(x^r\)이라고 나타냈으며, \(Q(R \vert x^r)\)은 주어진 샘플에 대해서 discriminator의 회전각 예측 분포를 의미합니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;b&gt; Collaborative Adversarial Training&lt;/b&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;SSGAN에서는 기존 GAN과 마찬가지로 true vs. fake prediction에서는 적대적인 학습을 합니다. 하지만 rotation task에서는 discriminator와 generator가 서로 협력적인 (collaborative) 학습을 하게 됩니다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;먼저 generator가 실제 이미지와 유사하게 이미지를 생성하여 회전시킨 후 discriminator에게 전달하면, discriminator는 회전된 각도를 감지하게 됩니다. 여기서 generator는 조건부 정보(rotation)을 사용하지 않으므로 항상 회전되지 않은 unright 이미지를 생성합니다.&lt;/li&gt;
  &lt;li&gt;discriminator는 실제 데이터에 대해서만 rotation 을 얼마나 정확하게 예측했는지를 기준으로 학습됩니다. 즉 실제 데이터에 대한 rotation loss만 반영하여 파라미터가 업데이트 됩니다.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;generator는 회전을 쉽게 감지할수 있도록 이미지를 생성하여 discriminator가 회전을 잘 감지할수있도록 도와줍니다. (discriminator는 실제 이미지의 로테이션을 잘 감지하도록 학습되었기때문에)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Fig1은 학습과정의 파이프라인을 나타냅니다.
    &lt;ul&gt;
      &lt;li&gt;discriminator의 목적은 1) non-rotated img에 대해서 true or fake를 잘 맞추는 것 2) rotated real img에 대해서 rotation angle을 잘 찾는 것 입니다.&lt;/li&gt;
      &lt;li&gt;generator의 목적은 실제 데이터와 유사하게 이미지를 생성하는 것인데, discrimator가 실제 데이터의 로테이션을 잘 감지되도록 학습했기때문에 generator도 회전을 쉽게 감지할수 있는 이미지를 생성하게 됩니다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;실험을 통해서 1) self-supervision이 baseline GAN과 비교하여 representation의 품질을 향상시킴 2) 동일한 학습 조건에서 conditional GAN과 비교가능 수준으로 conditional generation을 향상시킴 을 보였습니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;experimental-settings&quot;&gt;Experimental settings&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;dataset : IMAGENET, CIFAR10, LSUN-BEDROOM, CELEBA-HQ&lt;/li&gt;
  &lt;li&gt;Models :
    &lt;ul&gt;
      &lt;li&gt;baseline models
        &lt;ul&gt;
          &lt;li&gt;unconditional GAN with spectral normalization (Uncond-GAN)&lt;/li&gt;
          &lt;li&gt;conditional GAN using the label-conditioning strategy (Cond-GAN)&lt;/li&gt;
          &lt;li&gt;label-conditional batch normalization in Cond-GAN&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;ResNet architectures for the genertor and discriminator&lt;/li&gt;
      &lt;li&gt;self-modulated batch normalization in SS-GAN(sBN)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;results&quot;&gt;Results&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2019-02-08/fig4.png&quot; width=&quot;700&quot; /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2019-02-08/fig5.png&quot; width=&quot;500&quot; /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2019-02-08/tab1.png&quot; width=&quot;500&quot; /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2019-02-08/tab2.png&quot; width=&quot;500&quot; /&gt;&lt;br /&gt;&lt;/p&gt;</content><author><name>yjucho</name></author><category term="Generative Adversarial Network" /><category term="Deep Learning paper" /><summary type="html">Ting Chen et al. (Google Brain, 2018)</summary></entry></feed>